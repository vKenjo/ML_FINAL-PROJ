{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49273b9b",
   "metadata": {},
   "source": [
    "# YOLOv8 Microplastics Detection\n",
    "\n",
    "This notebook will guide you through training a YOLOv8 object detection model to detect microplastics using your dataset.\n",
    "\n",
    "> **Troubleshooting DataLoader Errors**: If you encounter `DataLoader worker exited unexpectedly` errors, this notebook has been updated to fix these issues by:\n",
    "> 1. Setting workers=0 to avoid multiprocessing issues\n",
    "> 2. Reducing batch size to prevent memory overflows\n",
    "> 3. Using CPU instead of GPU for more stable processing\n",
    "> 4. Disabling caching to prevent file access conflicts\n",
    "\n",
    "> **Important Update**: Your dataset is in detection format (bounding boxes), not segmentation format. The notebook has been updated to use YOLOv8 detection instead of segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a783f3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ultralytics in c:\\users\\blasi\\anaconda3\\lib\\site-packages (8.3.133)\n",
      "Requirement already satisfied: numpy>=1.23.0 in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from ultralytics) (1.26.4)\n",
      "Requirement already satisfied: matplotlib>=3.3.0 in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from ultralytics) (3.10.3)\n",
      "Requirement already satisfied: opencv-python>=4.6.0 in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from ultralytics) (4.11.0.86)\n",
      "Requirement already satisfied: pillow>=7.1.2 in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from ultralytics) (10.2.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from ultralytics) (6.0.2)\n",
      "Requirement already satisfied: requests>=2.23.0 in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from ultralytics) (2.32.3)\n",
      "Requirement already satisfied: scipy>=1.4.1 in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from ultralytics) (1.11.4)\n",
      "Requirement already satisfied: torch>=1.8.0 in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from ultralytics) (2.5.1+cu121)\n",
      "Requirement already satisfied: torchvision>=0.9.0 in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from ultralytics) (0.20.1+cu121)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from ultralytics) (4.65.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from ultralytics) (5.9.0)\n",
      "Requirement already satisfied: py-cpuinfo in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from ultralytics) (9.0.0)\n",
      "Requirement already satisfied: pandas>=1.1.4 in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from ultralytics) (2.2.3)\n",
      "Requirement already satisfied: seaborn>=0.11.0 in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from ultralytics) (0.13.2)\n",
      "Requirement already satisfied: ultralytics-thop>=2.0.0 in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from ultralytics) (2.0.14)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (24.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from pandas>=1.1.4->ultralytics) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from pandas>=1.1.4->ultralytics) (2023.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from requests>=2.23.0->ultralytics) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from requests>=2.23.0->ultralytics) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from requests>=2.23.0->ultralytics) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from requests>=2.23.0->ultralytics) (2024.8.30)\n",
      "Requirement already satisfied: filelock in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from torch>=1.8.0->ultralytics) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from torch>=1.8.0->ultralytics) (2023.10.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from tqdm>=4.64.0->ultralytics) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# 1. Install Required Libraries\n",
    "%pip install ultralytics\n",
    "\n",
    "# The following line will download the YOLOv8 detection model if it doesn't exist\n",
    "# Uncomment if you need to download it\n",
    "# !python -c \"from ultralytics import YOLO; YOLO('yolov8n.pt')\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337cec39",
   "metadata": {},
   "source": [
    "## 2. Dataset Structure and Format\n",
    "\n",
    "Your dataset should be structured as:\n",
    "- data/train/images/, data/train/labels/\n",
    "- data/valid/images/, data/valid/labels/\n",
    "- data/test/images/, data/test/labels/\n",
    "\n",
    "### Label Format for Object Detection\n",
    "YOLOv8 detection labels contain normalized bounding box coordinates for each object:\n",
    "```\n",
    "<class-id> <x_center> <y_center> <width> <height>\n",
    "```\n",
    "Where:\n",
    "- `class-id`: The object class (0 for microplastics)\n",
    "- `x_center, y_center`: Normalized center coordinates of the bounding box (0-1)\n",
    "- `width, height`: Normalized width and height of the bounding box (0-1)\n",
    "\n",
    "Each object in an image has its own line in the label file. The dataset already contains these labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616bbf68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: NVIDIA GeForce RTX 3050 Ti Laptop GPU\n",
      "Verifying dataset paths...\n",
      "Base path: C:\\Users\\blasi\\CS-ML\\FINAL_PROJ\n",
      "Train path: C:\\Users\\blasi\\CS-ML\\FINAL_PROJ\\data\\train\\images\n",
      "  Found 3226 images in train folder\n",
      "Val path: C:\\Users\\blasi\\CS-ML\\FINAL_PROJ\\data\\valid\\images\n",
      "  Found 928 images in val folder\n",
      "Test path: C:\\Users\\blasi\\CS-ML\\FINAL_PROJ\\data\\test\\images\n",
      "  Found 453 images in test folder\n",
      "Starting training...\n",
      "Starting training...\n",
      "New https://pypi.org/project/ultralytics/8.3.134 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.133  Python-3.11.7 torch-2.5.1+cu121 CUDA:0 (NVIDIA GeForce RTX 3050 Ti Laptop GPU, 4096MiB)\n",
      "New https://pypi.org/project/ultralytics/8.3.134 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.133  Python-3.11.7 torch-2.5.1+cu121 CUDA:0 (NVIDIA GeForce RTX 3050 Ti Laptop GPU, 4096MiB)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=8, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=data.yaml, degrees=0.0, deterministic=True, device=0, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=10, erasing=0.4, exist_ok=True, fliplr=0.5, flipud=0.5, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=train, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=50, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=runs/detect, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=runs\\detect\\train, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=0, workspace=None\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=8, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=data.yaml, degrees=0.0, deterministic=True, device=0, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=10, erasing=0.4, exist_ok=True, fliplr=0.5, flipud=0.5, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=train, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=50, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=runs/detect, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=runs\\detect\\train, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=0, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    751507  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \n",
      " 22        [15, 18, 21]  1    751507  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \n",
      "Model summary: 129 layers, 3,011,043 parameters, 3,011,027 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n",
      "Model summary: 129 layers, 3,011,043 parameters, 3,011,027 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed \n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed \n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 334.262.5 MB/s, size: 34.1 KB)\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 334.262.5 MB/s, size: 34.1 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\blasi\\CS-ML\\FINAL_PROJ\\data\\train\\labels.cache... 3226 images, 0 backgrounds, 0 corrupt: 100%|██████████| 3226/3226 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 329.971.8 MB/s, size: 32.7 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\blasi\\CS-ML\\FINAL_PROJ\\data\\valid\\labels.cache... 928 images, 0 backgrounds, 0 corrupt: 100%|██████████| 928/928 [00:00<?, ?it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs\\detect\\train\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns\\detect\\train\u001b[0m\n",
      "Starting training for 10 epochs...\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns\\detect\\train\u001b[0m\n",
      "Starting training for 10 epochs...\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/10      1.03G      2.146      3.641      1.584         47        640:   1%|          | 5/404 [00:01<02:39,  2.50it/s]"
     ]
    }
   ],
   "source": [
    "# 3. Training YOLOv8 Detection Model\n",
    "import os\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "import yaml\n",
    "\n",
    "DATASET_YAML = 'data.yaml'\n",
    "\n",
    "def verify_dataset(yaml_path):\n",
    "    if not os.path.exists(yaml_path):\n",
    "        print(f\"ERROR: Dataset YAML file not found: {yaml_path}\")\n",
    "        return False\n",
    "    try:\n",
    "        with open(yaml_path, 'r') as f:\n",
    "            data_cfg = yaml.safe_load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Failed to load YAML: {e}\")\n",
    "        return False\n",
    "    if 'path' not in data_cfg:\n",
    "        print(\"ERROR: 'path' key missing in YAML file.\")\n",
    "        return False\n",
    "    base_path = Path(data_cfg['path'])\n",
    "    print(f\"Base path: {base_path}\")\n",
    "    all_ok = True\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        if split in data_cfg:\n",
    "            split_path = base_path / data_cfg[split]\n",
    "            print(f\"{split.capitalize()} path: {split_path}\")\n",
    "            if not split_path.exists():\n",
    "                print(f\"WARNING: {split} path does not exist: {split_path}\")\n",
    "                try:\n",
    "                    os.makedirs(split_path, exist_ok=True)\n",
    "                    print(f\"Created directory: {split_path}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"ERROR: Could not create directory {split_path}: {e}\")\n",
    "                    all_ok = False\n",
    "            else:\n",
    "                img_count = len(list(split_path.glob('*.jpg'))) + len(list(split_path.glob('*.png')))\n",
    "                print(f\"  Found {img_count} images in {split} folder\")\n",
    "                if img_count == 0:\n",
    "                    print(f\"WARNING: No images found in {split_path}\")\n",
    "    return all_ok\n",
    "\n",
    "# Device selection\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "    print(f\"CUDA available: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"No GPU available, using CPU instead.\")\n",
    "\n",
    "print(\"Verifying dataset paths...\")\n",
    "dataset_ok = verify_dataset(DATASET_YAML)\n",
    "if not dataset_ok:\n",
    "    print(\"Dataset verification failed. Please check your dataset structure and YAML file.\")\n",
    "else:\n",
    "    # Load YOLOv8 detection model\n",
    "    try:\n",
    "        model = YOLO('yolov8n.pt')\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Could not load YOLOv8 model: {e}\")\n",
    "        model = None\n",
    "    if model is not None:\n",
    "        try:\n",
    "            print(\"Starting training...\")\n",
    "            results = model.train(\n",
    "                data=DATASET_YAML,\n",
    "                epochs=10,             # Increased epochs for better learning\n",
    "                imgsz=640,             # Input image size\n",
    "                batch=8,               # Reduced batch size to prevent memory issues\n",
    "                workers=0,             # Set workers to 0 to avoid DataLoader multiprocessing issues\n",
    "                mosaic=1.0,            # Mosaic augmentation\n",
    "                scale=0.5,             # Scale augmentation\n",
    "                perspective=0.0,       # No perspective augmentation for small objects\n",
    "                flipud=0.5,            # Flip up-down augmentation\n",
    "                fliplr=0.5,            # Flip left-right augmentation\n",
    "                hsv_h=0.015,           # HSV hue augmentation (reduced for consistency)\n",
    "                hsv_s=0.7,             # HSV saturation augmentation\n",
    "                hsv_v=0.4,             # HSV value augmentation\n",
    "                patience=50,           # Early stopping patience\n",
    "                device=str(device),    # Use GPU if available, otherwise CPU\n",
    "                project='runs/detect', # Project directory\n",
    "                name='train',          # Run name\n",
    "                exist_ok=True,         # Overwrite existing directory\n",
    "                cache=False            # Disable cache to prevent potential issues\n",
    "            )\n",
    "            print(\"Training completed successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR during training: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3783eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 Training Visualization\n",
    "from IPython.display import display, Image\n",
    "from pathlib import Path\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Function to display training progress during or after training\n",
    "def show_training_plots():\n",
    "    results_path = Path('runs/detect/train')  # Changed from segment to detect\n",
    "    \n",
    "    # Check if the directory exists first\n",
    "    if not results_path.exists():\n",
    "        print(f\"Warning: Results directory not found at {results_path}\")\n",
    "        return\n",
    "    \n",
    "    # Results plots\n",
    "    plots = {\n",
    "        'Training Loss': results_path / 'results.png',\n",
    "        'Validation Confusion Matrix': results_path / 'val_confusion_matrix_normalized.png',\n",
    "        'PR Curve': results_path / 'PR_curve.png'\n",
    "    }\n",
    "    \n",
    "    found_plots = False\n",
    "    for title, plot_path in plots.items():\n",
    "        if plot_path.exists():\n",
    "            found_plots = True\n",
    "            print(f\"\\n{title}:\")\n",
    "            try:\n",
    "                display(Image(str(plot_path)))\n",
    "            except Exception as e:\n",
    "                print(f\"Error displaying {title}: {str(e)}\")\n",
    "        else:\n",
    "            print(f\"\\n{title} plot not found at {plot_path}\")\n",
    "    \n",
    "    if not found_plots:\n",
    "        print(\"No training plots found. Training may not have completed successfully.\")\n",
    "\n",
    "# Call this function after training completes\n",
    "# Uncomment to run after training\n",
    "# show_training_plots()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3e096f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Evaluate on Test Set\n",
    "\n",
    "# Function to safely perform validation\n",
    "def safe_validation(model, yaml_path, split='test'):\n",
    "    try:\n",
    "        # Evaluate the model with error handling\n",
    "        metrics = model.val(\n",
    "            data=yaml_path,\n",
    "            split=split,\n",
    "            conf=0.25,           # Confidence threshold\n",
    "            iou=0.5,             # IoU threshold\n",
    "            max_det=300,         # Maximum detections per image\n",
    "            verbose=True         # Print detailed metrics\n",
    "        )\n",
    "        return metrics\n",
    "    except Exception as e:\n",
    "        print(f\"Error during validation: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Run validation\n",
    "metrics = safe_validation(model, DATASET_YAML)\n",
    "\n",
    "# Print summary metrics if validation was successful\n",
    "if metrics is not None:\n",
    "    try:\n",
    "        print(f\"Precision: {metrics.box.maps.mean():.4f}\")\n",
    "        print(f\"mAP@0.5: {metrics.box.map50:.4f}\")\n",
    "        print(f\"mAP@0.5:0.95: {metrics.box.map:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error displaying metrics: {str(e)}\")\n",
    "else:\n",
    "    print(\"Validation failed. Cannot display metrics.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37d0961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 Visualize Evaluation Results\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# Get confusion matrix plot if available\n",
    "conf_matrix = Path('runs/detect/train/confusion_matrix.png')  # Changed from segment to detect\n",
    "if conf_matrix.exists():\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    img = plt.imread(conf_matrix)\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "# Get PR curve if available\n",
    "pr_curve = Path('runs/detect/train/PR_curve.png')  # Changed from segment to detect\n",
    "if pr_curve.exists():\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    img = plt.imread(pr_curve)\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.title('Precision-Recall Curve')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fda8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Inference Example\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Function to safely load and run inference on an image\n",
    "def safe_inference(model, image_path, conf=0.25):\n",
    "    try:\n",
    "        if not os.path.exists(image_path):\n",
    "            print(f\"Warning: Image not found: {image_path}\")\n",
    "            return None\n",
    "            \n",
    "        # Run inference with error handling\n",
    "        results = model(image_path, conf=conf)\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(f\"Error during inference on {image_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Run inference on test images\n",
    "test_image = 'data/test/images/1_jpg.rf.cde0320b040f0984f45350362147d2b2.jpg'\n",
    "results = safe_inference(model, test_image)\n",
    "\n",
    "# Only proceed if inference was successful\n",
    "if results is not None:\n",
    "    # Plot results\n",
    "    try:\n",
    "        fig, ax = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "        # Original image with detections\n",
    "        ax[0].imshow(plt.imread(test_image))\n",
    "        ax[0].set_title('Original Image')\n",
    "        ax[0].axis('off')\n",
    "\n",
    "        # Image with bounding boxes\n",
    "        results[0].plot(boxes=True, labels=True)  # Removed masks=True since we're using detection\n",
    "        ax[1].imshow(results[0].orig_img)\n",
    "        ax[1].set_title('Detected Microplastics')\n",
    "        ax[1].axis('off')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Print detection statistics\n",
    "        print(f\"Detected {len(results[0])} microplastics with confidence >0.25\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error plotting results: {str(e)}\")\n",
    "\n",
    "# Run inference on a few more images if available\n",
    "try:\n",
    "    # Get list of test images\n",
    "    import glob\n",
    "    test_images = glob.glob('data/test/images/*.jpg')[:3]  # Get first 3 test images to avoid memory issues\n",
    "    \n",
    "    # Run inference on multiple images\n",
    "    if len(test_images) > 1:\n",
    "        for img_path in test_images[1:]:  # Skip the first one as we already processed it\n",
    "            img_results = safe_inference(model, img_path)\n",
    "            if img_results is not None:\n",
    "                try:\n",
    "                    img_results[0].show()\n",
    "                    print(f\"Detected {len(img_results[0])} microplastics in {os.path.basename(img_path)}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error showing results for {img_path}: {str(e)}\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"Error processing additional images: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f7b26b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "- You can adjust the number of epochs, image size, or model variant (e.g., yolov8m.pt) as needed.\n",
    "- Run the notebook cells to train and evaluate your model.\n",
    "\n",
    "## Understanding the Results\n",
    "\n",
    "### Metrics Explanation\n",
    "- **mAP (mean Average Precision)**: The primary metric for object detection performance\n",
    "- **Precision**: How accurate the positive detections are\n",
    "- **Recall**: The ability of the model to find all microplastics in the image\n",
    "- **IoU (Intersection over Union)**: Measures how well the predicted bounding boxes overlap with the ground truth\n",
    "\n",
    "### Model Improvements\n",
    "\n",
    "To improve model performance:\n",
    "\n",
    "1. **Try larger models**: Replace `yolov8n.pt` with:\n",
    "   - `yolov8s.pt` (small) \n",
    "   - `yolov8m.pt` (medium)\n",
    "   - `yolov8l.pt` (large)\n",
    "   - `yolov8x.pt` (extra large)\n",
    "\n",
    "2. **Data augmentation**: Add more augmentations to prevent overfitting:\n",
    "   ```python\n",
    "   model.train(\n",
    "       # Other parameters\n",
    "       augment=True,\n",
    "       mixup=0.1,\n",
    "       copy_paste=0.1\n",
    "   )\n",
    "   ```\n",
    "\n",
    "3. **Optimization**: Try different optimizers:\n",
    "   ```python\n",
    "   model.train(\n",
    "       # Other parameters\n",
    "       optimizer=\"AdamW\",\n",
    "       lr0=0.001\n",
    "   )\n",
    "   ```\n",
    "\n",
    "4. **Export model**: Save the model for deployment:\n",
    "   ```python\n",
    "   model.export(format=\"onnx\") # or \"torchscript\", \"openvino\", etc.\n",
    "   ```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
