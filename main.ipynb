{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da2fbf7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4ad20acb",
   "metadata": {},
   "source": [
    "## IoU Performance Enhancements\n",
    "\n",
    "This section explains the improvements made to achieve better IoU performance:\n",
    "\n",
    "1. **Enhanced IoU Metrics**: We've implemented three IoU calculation methods:\n",
    "   - Standard IoU (Intersection over Union)\n",
    "   - GIoU (Generalized IoU) - better for non-overlapping boxes\n",
    "   - PyTorch IoU - leveraging PyTorch's optimized implementation\n",
    "\n",
    "2. **Training Improvements**:\n",
    "   - More epochs (50) for better convergence\n",
    "   - Optimized batch size based on GPU memory\n",
    "   - Enhanced data augmentation for better object detection\n",
    "   - Box loss gain increased to 7.5 to emphasize localization\n",
    "   - IoU threshold for NMS increased to 0.7\n",
    "\n",
    "3. **Enhanced Visualization**:\n",
    "   - Color-coded boxes based on IoU score\n",
    "   - Display of multiple IoU metrics\n",
    "   - Better statistical analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49273b9b",
   "metadata": {},
   "source": [
    "# YOLOv8 Microplastics Detection\n",
    "\n",
    "This notebook will guide you through training a YOLOv8 object detection model to detect microplastics using your dataset.\n",
    "\n",
    "> **Troubleshooting DataLoader Errors**: If you encounter `DataLoader worker exited unexpectedly` errors, this notebook has been updated to fix these issues by:\n",
    "> 1. Setting workers=0 to avoid multiprocessing issues\n",
    "> 2. Reducing batch size to prevent memory overflows\n",
    "> 3. Using CPU instead of GPU for more stable processing\n",
    "> 4. Disabling caching to prevent file access conflicts\n",
    "\n",
    "> **Important Update**: Your dataset is in detection format (bounding boxes), not segmentation format. The notebook has been updated to use YOLOv8 detection instead of segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a783f3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ultralytics in c:\\users\\blasi\\anaconda3\\lib\\site-packages (8.3.134)\n",
      "Requirement already satisfied: numpy>=1.23.0 in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from ultralytics) (1.26.4)\n",
      "Requirement already satisfied: matplotlib>=3.3.0 in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from ultralytics) (3.10.3)\n",
      "Requirement already satisfied: opencv-python>=4.6.0 in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from ultralytics) (4.11.0.86)\n",
      "Requirement already satisfied: pillow>=7.1.2 in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from ultralytics) (10.2.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from ultralytics) (6.0.2)\n",
      "Requirement already satisfied: requests>=2.23.0 in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from ultralytics) (2.32.3)\n",
      "Requirement already satisfied: scipy>=1.4.1 in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from ultralytics) (1.11.4)\n",
      "Requirement already satisfied: torch>=1.8.0 in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from ultralytics) (2.5.1+cu121)\n",
      "Requirement already satisfied: torchvision>=0.9.0 in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from ultralytics) (0.20.1+cu121)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from ultralytics) (4.65.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from ultralytics) (5.9.0)\n",
      "Requirement already satisfied: py-cpuinfo in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from ultralytics) (9.0.0)\n",
      "Requirement already satisfied: pandas>=1.1.4 in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from ultralytics) (2.2.3)\n",
      "Requirement already satisfied: seaborn>=0.11.0 in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from ultralytics) (0.13.2)\n",
      "Requirement already satisfied: ultralytics-thop>=2.0.0 in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from ultralytics) (2.0.14)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (24.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from pandas>=1.1.4->ultralytics) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from pandas>=1.1.4->ultralytics) (2023.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from requests>=2.23.0->ultralytics) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from requests>=2.23.0->ultralytics) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from requests>=2.23.0->ultralytics) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from requests>=2.23.0->ultralytics) (2024.8.30)\n",
      "Requirement already satisfied: filelock in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from torch>=1.8.0->ultralytics) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from torch>=1.8.0->ultralytics) (2023.10.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from tqdm>=4.64.0->ultralytics) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# 1. Install Required Libraries\n",
    "%pip install ultralytics\n",
    "\n",
    "# The following line will download the YOLOv8 detection model if it doesn't exist\n",
    "# Uncomment if you need to download it\n",
    "# !python -c \"from ultralytics import YOLO; YOLO('yolov8n.pt')\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337cec39",
   "metadata": {},
   "source": [
    "## 2. Dataset Structure and Format\n",
    "\n",
    "Your dataset should be structured as:\n",
    "- data/train/images/, data/train/labels/\n",
    "- data/valid/images/, data/valid/labels/\n",
    "- data/test/images/, data/test/labels/\n",
    "\n",
    "### Label Format for Object Detection\n",
    "YOLOv8 detection labels contain normalized bounding box coordinates for each object:\n",
    "```\n",
    "<class-id> <x_center> <y_center> <width> <height>\n",
    "```\n",
    "Where:\n",
    "- `class-id`: The object class (0 for microplastics)\n",
    "- `x_center, y_center`: Normalized center coordinates of the bounding box (0-1)\n",
    "- `width, height`: Normalized width and height of the bounding box (0-1)\n",
    "\n",
    "Each object in an image has its own line in the label file. The dataset already contains these labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "616bbf68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating and fixing data.yaml...\n",
      "data.yaml is already correctly configured.\n",
      "\n",
      "Validating absolute paths:\n",
      "Configured path: c:/Users/blasi/CS-ML/FINAL_PROJ\n",
      "\n",
      "Primary configurations:\n",
      "- Checking Train path: c:/Users/blasi/CS-ML/FINAL_PROJ/data/train/images\n",
      "  ✓ Path exists! Found 3226 images.\n",
      "  ✓ Found 3226 labels.\n",
      "- Checking Validation path: c:/Users/blasi/CS-ML/FINAL_PROJ/data/valid/images\n",
      "  ✓ Path exists! Found 928 images.\n",
      "  ✓ Found 928 labels.\n",
      "- Checking Test path: c:/Users/blasi/CS-ML/FINAL_PROJ/data/test/images\n",
      "  ✓ Path exists! Found 453 images.\n",
      "  ✓ Found 453 labels.\n",
      "\n",
      "YOLOv8 settings found at: C:\\Users\\blasi\\AppData\\Roaming\\Ultralytics\\settings.json\n",
      "YOLOv8 datasets_dir: C:\\Users\\blasi\\CS-ML\\FINAL_PROJ\\datasets\n"
     ]
    }
   ],
   "source": [
    "# 3. Training YOLOv8 Detection Model\n",
    "import os\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "import yaml\n",
    "\n",
    "# Set CUDA memory configurations to help with memory fragmentation\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "DATASET_YAML = 'data.yaml'\n",
    "\n",
    "# 3.0 Data Configuration Validation and Repair\n",
    "def validate_and_fix_data_yaml(yaml_path):\n",
    "    \"\"\"Validate and fix data.yaml file for YOLOv8 compatibility\"\"\"\n",
    "    print(f\"Validating and fixing {yaml_path}...\")\n",
    "    try:\n",
    "        with open(yaml_path, 'r') as f:\n",
    "            data_cfg = yaml.safe_load(f)\n",
    "        \n",
    "        # Make a copy to check if changes were made\n",
    "        original_cfg = data_cfg.copy()\n",
    "        \n",
    "        # Get project root directory (absolute path)\n",
    "        project_root = os.path.abspath(os.path.dirname(yaml_path))\n",
    "        \n",
    "        # Fix fields for YOLOv8 with absolute paths\n",
    "        data_cfg[\"path\"] = project_root.replace('\\\\', '/')  # Use forward slashes for paths\n",
    "        \n",
    "        # These are relative to the path\n",
    "        data_cfg[\"train\"] = \"data/train/images\"\n",
    "        data_cfg[\"val\"] = \"data/valid/images\"\n",
    "        data_cfg[\"test\"] = \"data/test/images\"\n",
    "        data_cfg[\"nc\"] = 1\n",
    "        data_cfg[\"names\"] = [\"microplastic\"]\n",
    "        \n",
    "        \n",
    "        # Check if changes were made\n",
    "        if data_cfg != original_cfg:\n",
    "            print(\"Changes needed in data.yaml. Updating file...\")\n",
    "            with open(yaml_path, 'w') as f:\n",
    "                yaml.dump(data_cfg, f, default_flow_style=False)\n",
    "            print(\"data.yaml has been updated with correct configuration.\")\n",
    "        else:\n",
    "            print(\"data.yaml is already correctly configured.\")\n",
    "        \n",
    "        # Validate absolute paths - important for troubleshooting\n",
    "        train_path = os.path.join(data_cfg[\"path\"], \"data/train/images\")\n",
    "        val_path = os.path.join(data_cfg[\"path\"], \"data/valid/images\")\n",
    "        test_path = os.path.join(data_cfg[\"path\"], \"data/test/images\")\n",
    "        \n",
    "        train_path_alt = os.path.join(project_root, \"data/train/images\")\n",
    "        val_path_alt = os.path.join(project_root, \"data/valid/images\")\n",
    "        test_path_alt = os.path.join(project_root, \"data/test/images\")\n",
    "        \n",
    "        print(\"\\nValidating absolute paths:\")\n",
    "        print(f\"Configured path: {data_cfg['path']}\")\n",
    "        \n",
    "        def check_path(path, name):\n",
    "            print(f\"- Checking {name} path: {path}\")\n",
    "            if os.path.exists(path):\n",
    "                images = len(list(Path(path).glob('*.jpg'))) + len(list(Path(path).glob('*.png')))\n",
    "                print(f\"  ✓ Path exists! Found {images} images.\")\n",
    "                # Check for corresponding labels\n",
    "                label_path = path.replace('images', 'labels')\n",
    "                if os.path.exists(label_path):\n",
    "                    labels = len(list(Path(label_path).glob('*.txt')))\n",
    "                    print(f\"  ✓ Found {labels} labels.\")\n",
    "                    if labels < images:\n",
    "                        print(f\"  ⚠ WARNING: {images-labels} images may be missing labels!\")\n",
    "                else:\n",
    "                    print(f\"  ✗ WARNING: Label directory {label_path} does not exist!\")\n",
    "                return images > 0\n",
    "            else:\n",
    "                print(f\"  ✗ ERROR: Directory does not exist!\")\n",
    "                return False\n",
    "        \n",
    "        # Check primary paths\n",
    "        print(\"\\nPrimary configurations:\")\n",
    "        train_ok = check_path(train_path.replace('\\\\', '/'), \"Train\")\n",
    "        val_ok = check_path(val_path.replace('\\\\', '/'), \"Validation\")\n",
    "        test_ok = check_path(test_path.replace('\\\\', '/'), \"Test\")\n",
    "        \n",
    "        # If paths are missing, check alternative paths\n",
    "        if not (train_ok and val_ok and test_ok):\n",
    "            print(\"\\nChecking alternative paths:\")\n",
    "            check_path(train_path_alt.replace('\\\\', '/'), \"Alt Train\")\n",
    "            check_path(val_path_alt.replace('\\\\', '/'), \"Alt Validation\")\n",
    "            check_path(test_path_alt.replace('\\\\', '/'), \"Alt Test\")\n",
    "        \n",
    "        # Check paths that YOLOv8 might be trying to use (for debugging)\n",
    "        datasets_path = os.path.join(project_root, \"datasets\")\n",
    "        if os.path.exists(datasets_path):\n",
    "            print(f\"\\nWARNING: A 'datasets' directory exists at {datasets_path}\")\n",
    "            print(\"This might be causing path resolution conflicts. YOLOv8 might be looking here instead of your data directory.\")\n",
    "        \n",
    "        # Check YOLOv8 settings\n",
    "        settings_path = os.path.expandvars(r\"%APPDATA%\\Ultralytics\\settings.json\")\n",
    "        if os.path.exists(settings_path):\n",
    "            print(f\"\\nYOLOv8 settings found at: {settings_path}\")\n",
    "            try:\n",
    "                import json\n",
    "                with open(settings_path, 'r') as f:\n",
    "                    settings = json.load(f)\n",
    "                if 'datasets_dir' in settings:\n",
    "                    print(f\"YOLOv8 datasets_dir: {settings['datasets_dir']}\")\n",
    "            except:\n",
    "                print(\"Could not read YOLOv8 settings file.\")\n",
    "        \n",
    "        return data_cfg\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR processing data.yaml: {e}\")\n",
    "        return None\n",
    "\n",
    "# Run validation and fix\n",
    "data_cfg = validate_and_fix_data_yaml(DATASET_YAML)\n",
    "\n",
    "def verify_dataset(yaml_path):\n",
    "    if not os.path.exists(yaml_path):\n",
    "        print(f\"ERROR: Dataset YAML file not found: {yaml_path}\")\n",
    "        return False\n",
    "    try:\n",
    "        with open(yaml_path, 'r') as f:\n",
    "            data_cfg = yaml.safe_load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Failed to load YAML: {e}\")\n",
    "        return False\n",
    "    if 'path' not in data_cfg:\n",
    "        print(\"ERROR: 'path' key missing in YAML file.\")\n",
    "        return False\n",
    "    base_path = Path(data_cfg['path'])\n",
    "    print(f\"Base path: {base_path}\")\n",
    "    all_ok = True\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        if split in data_cfg:\n",
    "            split_path = base_path / data_cfg[split]\n",
    "            print(f\"{split.capitalize()} path: {split_path}\")\n",
    "            if not split_path.exists():\n",
    "                print(f\"WARNING: {split} path does not exist: {split_path}\")\n",
    "                try:\n",
    "                    os.makedirs(split_path, exist_ok=True)\n",
    "                    print(f\"Created directory: {split_path}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"ERROR: Could not create directory {split_path}: {e}\")\n",
    "                    all_ok = False\n",
    "            else:\n",
    "                img_count = len(list(split_path.glob('*.jpg'))) + len(list(split_path.glob('*.png')))\n",
    "                print(f\"  Found {img_count} images in {split} folder\")\n",
    "                if img_count == 0:\n",
    "                    print(f\"WARNING: No images found in {split_path}\")\n",
    "    return all_ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "156f804e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: NVIDIA GeForce RTX 3050 Ti Laptop GPU\n",
      "GPU memory: 4.00 GB\n",
      "⚠️ Low GPU memory detected! Using smaller model, batch size, and image resolution.\n",
      "Optimal batch size for your GPU: 16\n",
      "Selected model: yolov8n.pt\n",
      "Selected image size: 640\n",
      "Optimal worker count: 6\n",
      "CUDA cache cleared for training\n",
      "\n",
      "GPU memory: 4.00 GB\n",
      "⚠️ Low GPU memory detected! Using smaller model, batch size, and image resolution.\n",
      "Optimal batch size for your GPU: 16\n",
      "Selected model: yolov8n.pt\n",
      "Selected image size: 640\n",
      "Optimal worker count: 6\n",
      "CUDA cache cleared for training\n",
      "Verifying dataset paths...\n",
      "Base path: c:\\Users\\blasi\\CS-ML\\FINAL_PROJ\n",
      "Train path: c:\\Users\\blasi\\CS-ML\\FINAL_PROJ\\data\\train\\images\n",
      "  Found 3226 images in train folder\n",
      "Val path: c:\\Users\\blasi\\CS-ML\\FINAL_PROJ\\data\\valid\\images\n",
      "  Found 928 images in val folder\n",
      "Test path: c:\\Users\\blasi\\CS-ML\\FINAL_PROJ\\data\\test\\images\n",
      "  Found 453 images in test folder\n",
      "Using yolov8n.pt for training...\n",
      "Starting training...\n",
      "Temporary data.yaml created with absolute paths:\n",
      "- path: c:/Users/blasi/CS-ML/FINAL_PROJ\n",
      "- train: data/train/images\n",
      "- val: data/valid/images\n",
      "Full train path: c:/Users/blasi/CS-ML/FINAL_PROJ\\data/train/images\n",
      "Full val path: c:/Users/blasi/CS-ML/FINAL_PROJ\\data/valid/images\n",
      "Verifying dataset paths...\n",
      "Base path: c:\\Users\\blasi\\CS-ML\\FINAL_PROJ\n",
      "Train path: c:\\Users\\blasi\\CS-ML\\FINAL_PROJ\\data\\train\\images\n",
      "  Found 3226 images in train folder\n",
      "Val path: c:\\Users\\blasi\\CS-ML\\FINAL_PROJ\\data\\valid\\images\n",
      "  Found 928 images in val folder\n",
      "Test path: c:\\Users\\blasi\\CS-ML\\FINAL_PROJ\\data\\test\\images\n",
      "  Found 453 images in test folder\n",
      "Using yolov8n.pt for training...\n",
      "Starting training...\n",
      "Temporary data.yaml created with absolute paths:\n",
      "- path: c:/Users/blasi/CS-ML/FINAL_PROJ\n",
      "- train: data/train/images\n",
      "- val: data/valid/images\n",
      "Full train path: c:/Users/blasi/CS-ML/FINAL_PROJ\\data/train/images\n",
      "Full val path: c:/Users/blasi/CS-ML/FINAL_PROJ\\data/valid/images\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.1, copy_paste_mode=flip, cos_lr=True, cutmix=0.0, data=temp_data.yaml, degrees=10.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=1, erasing=0.4, exist_ok=True, fliplr=0.5, flipud=0.5, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.6, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=100, mixup=0.1, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=0.5, multi_scale=False, name=train, nbs=64, nms=True, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=runs/detect, rect=True, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=runs\\detect\\train, save_frames=False, save_json=False, save_period=10, save_txt=False, scale=0.5, seed=0, shear=2.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=True, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=1.0, warmup_momentum=0.8, weight_decay=0.0005, workers=6, workspace=None\n",
      "Overriding class names with single class.\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.1, copy_paste_mode=flip, cos_lr=True, cutmix=0.0, data=temp_data.yaml, degrees=10.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=1, erasing=0.4, exist_ok=True, fliplr=0.5, flipud=0.5, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.6, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=100, mixup=0.1, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=0.5, multi_scale=False, name=train, nbs=64, nms=True, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=runs/detect, rect=True, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=runs\\detect\\train, save_frames=False, save_json=False, save_period=10, save_txt=False, scale=0.5, seed=0, shear=2.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=True, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=1.0, warmup_momentum=0.8, weight_decay=0.0005, workers=6, workspace=None\n",
      "Overriding class names with single class.\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    751507  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    751507  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \n",
      "Model summary: 129 layers, 3,011,043 parameters, 3,011,027 gradients, 8.2 GFLOPs\n",
      "\n",
      "Model summary: 129 layers, 3,011,043 parameters, 3,011,027 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n",
      "Transferred 319/355 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed \n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed \n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 197.6151.0 MB/s, size: 34.1 KB)\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 197.6151.0 MB/s, size: 34.1 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\blasi\\CS-ML\\FINAL_PROJ\\data\\train\\labels.cache... 3226 images, 0 backgrounds, 0 corrupt: 100%|██████████| 3226/3226 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 'rect=True' is incompatible with DataLoader shuffle, setting shuffle=False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.1 ms, read: 89.122.8 MB/s, size: 32.7 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\blasi\\CS-ML\\FINAL_PROJ\\data\\valid\\labels.cache... 928 images, 0 backgrounds, 0 corrupt: 100%|██████████| 928/928 [00:00<?, ?it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs\\detect\\train\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 6 dataloader workers\n",
      "Logging results to \u001b[1mruns\\detect\\train\u001b[0m\n",
      "Starting training for 1 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 6 dataloader workers\n",
      "Logging results to \u001b[1mruns\\detect\\train\u001b[0m\n",
      "Starting training for 1 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/1     0.338G      1.897      2.254       1.46        169        640: 100%|██████████| 202/202 [00:42<00:00,  4.71it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):   0%|          | 0/29 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 29/29 [00:12<00:00,  2.35it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        928       6475      0.573       0.54      0.516      0.196\n",
      "\n",
      "1 epochs completed in 0.016 hours.\n",
      "\n",
      "1 epochs completed in 0.016 hours.\n",
      "Optimizer stripped from runs\\detect\\train\\weights\\last.pt, 6.2MB\n",
      "Optimizer stripped from runs\\detect\\train\\weights\\last.pt, 6.2MB\n",
      "Optimizer stripped from runs\\detect\\train\\weights\\best.pt, 6.2MB\n",
      "\n",
      "Validating runs\\detect\\train\\weights\\best.pt...\n",
      "Ultralytics 8.3.134  Python-3.11.7 torch-2.5.1+cu121 CUDA:0 (NVIDIA GeForce RTX 3050 Ti Laptop GPU, 4096MiB)\n",
      "Optimizer stripped from runs\\detect\\train\\weights\\best.pt, 6.2MB\n",
      "\n",
      "Validating runs\\detect\\train\\weights\\best.pt...\n",
      "Ultralytics 8.3.134  Python-3.11.7 torch-2.5.1+cu121 CUDA:0 (NVIDIA GeForce RTX 3050 Ti Laptop GPU, 4096MiB)\n",
      "Model summary (fused): 72 layers, 3,005,843 parameters, 0 gradients, 8.1 GFLOPs\n",
      "Model summary (fused): 72 layers, 3,005,843 parameters, 0 gradients, 8.1 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 29/29 [00:10<00:00,  2.79it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        928       6475      0.573       0.54      0.516      0.196\n",
      "Speed: 0.2ms preprocess, 2.8ms inference, 0.0ms loss, 2.1ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\train\u001b[0m\n",
      "Speed: 0.2ms preprocess, 2.8ms inference, 0.0ms loss, 2.1ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\train\u001b[0m\n",
      "Temporary data file temp_data.yaml removed\n",
      "Training completed successfully.\n",
      "Temporary data file temp_data.yaml removed\n",
      "Training completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Device selection\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "    print(f\"CUDA available: {torch.cuda.get_device_name(0)}\")\n",
    "    \n",
    "    # Set optimal CUDA settings for better performance\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cudnn.deterministic = False\n",
    "    \n",
    "    # Check memory availability - helps determine batch size\n",
    "    gpu_mem = torch.cuda.get_device_properties(0).total_memory / (1024 ** 3)  # in GB\n",
    "    print(f\"GPU memory: {gpu_mem:.2f} GB\")\n",
    "    \n",
    "    # Adjust batch size based on available GPU memory\n",
    "    if gpu_mem < 6:  # For GPUs with less than 6GB memory\n",
    "        optimal_batch = 16 \n",
    "        optimal_model = 'yolov8n.pt'  # Use smallest model\n",
    "        optimal_size = 640  # Use smaller image size\n",
    "        print(\"⚠️ Low GPU memory detected! Using smaller model, batch size, and image resolution.\")\n",
    "\n",
    "    else:  # High memory (>8GB)\n",
    "        optimal_batch = 16\n",
    "        optimal_model = 'yolov8l.pt'  # Largest model\n",
    "        optimal_size = 640\n",
    "        print(\"High GPU memory detected. Using optimal settings.\")\n",
    "    \n",
    "    print(f\"Optimal batch size for your GPU: {optimal_batch}\")\n",
    "    print(f\"Selected model: {optimal_model}\")\n",
    "    print(f\"Selected image size: {optimal_size}\")\n",
    "    \n",
    "    # Determine optimal worker count (reduced to avoid memory issues)\n",
    "    optimal_workers = 6  \n",
    "    print(f\"Optimal worker count: {optimal_workers}\")\n",
    "    \n",
    "    # Empty cache to start fresh\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"CUDA cache cleared for training\")\n",
    "    \n",
    "    # Garbage collection to free up memory\n",
    "    import gc\n",
    "    gc.collect()\n",
    "    \n",
    "    # Release any leftover memory\n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"No GPU available, using CPU instead.\")\n",
    "    optimal_batch = 4  # Reduced for CPU\n",
    "    optimal_workers = 0\n",
    "    optimal_model = 'yolov8n.pt'  # Smallest model\n",
    "    optimal_size = 640  # Smaller image size\n",
    "\n",
    "print(\"Verifying dataset paths...\")\n",
    "dataset_ok = verify_dataset(DATASET_YAML)\n",
    "if not dataset_ok:\n",
    "    print(\"Dataset verification failed. Please check your dataset structure and YAML file.\")\n",
    "else:\n",
    "    # Load YOLOv8 detection model - try a larger model for better accuracy\n",
    "    try:\n",
    "        # Find the best available model - prioritizing models that work with available memory\n",
    "        model_candidates = [optimal_model, 'yolov8n.pt']  # Fallback to nano model\n",
    "        selected_model = None\n",
    "        \n",
    "        for model_path in model_candidates:\n",
    "            if os.path.exists(model_path):\n",
    "                selected_model = model_path\n",
    "                print(f\"Using {selected_model} for training...\")\n",
    "                break\n",
    "        \n",
    "        if selected_model is None:\n",
    "            print(f\"No YOLOv8 model found, using {optimal_model} and downloading if needed...\")\n",
    "            selected_model = optimal_model\n",
    "        \n",
    "        model = YOLO(selected_model)\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Could not load YOLOv8 model: {e}\")\n",
    "        model = None\n",
    "        \n",
    "    if model is not None:\n",
    "        try:\n",
    "            print(\"Starting training...\")\n",
    "            \n",
    "            # Create a copy of data.yaml with absolute paths to prevent path resolution issues\n",
    "            import shutil\n",
    "            temp_yaml = 'temp_data.yaml'\n",
    "            shutil.copy(DATASET_YAML, temp_yaml)\n",
    "            \n",
    "            # Update the temporary YAML with absolute paths\n",
    "            with open(temp_yaml, 'r') as f:\n",
    "                temp_data = yaml.safe_load(f)\n",
    "            \n",
    "            # Ensure path is absolute with forward slashes\n",
    "            project_root = os.path.abspath(os.path.dirname(DATASET_YAML))\n",
    "            temp_data['path'] = project_root.replace('\\\\', '/')\n",
    "            \n",
    "            # Write updated YAML\n",
    "            with open(temp_yaml, 'w') as f:\n",
    "                yaml.dump(temp_data, f, default_flow_style=False)\n",
    "            \n",
    "            print(f\"Temporary data.yaml created with absolute paths:\")\n",
    "            print(f\"- path: {temp_data['path']}\")\n",
    "            print(f\"- train: {temp_data['train']}\")\n",
    "            print(f\"- val: {temp_data['val']}\")\n",
    "            \n",
    "            # Display actual paths that will be used\n",
    "            train_path = os.path.join(temp_data['path'], temp_data['train'])\n",
    "            val_path = os.path.join(temp_data['path'], temp_data['val'])\n",
    "            print(f\"Full train path: {train_path}\")\n",
    "            print(f\"Full val path: {val_path}\")\n",
    "            \n",
    "            # Empty cache again right before training\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "            \n",
    "            # MEMORY-OPTIMIZED training configuration\n",
    "            results = model.train(\n",
    "                data=temp_yaml,            # Use the temporary YAML with absolute paths\n",
    "                epochs=200,                # Reduced epochs for testing\n",
    "                imgsz=optimal_size,        # REDUCED image size based on GPU memory\n",
    "                batch=optimal_batch,       # REDUCED batch size based on GPU memory\n",
    "                workers=optimal_workers,   # REDUCED worker count to conserve memory\n",
    "                \n",
    "                # Simplified data augmentation to reduce memory usage\n",
    "                mosaic=0.5,                # REDUCED mosaic augmentation\n",
    "                mixup=0.1,                 # REDUCED mixup augmentation\n",
    "                copy_paste=0.1,            # REDUCED copy-paste augmentation\n",
    "                scale=0.5,          # Scale range\n",
    "                degrees=10.0,               # Rotation augmentation\n",
    "                translate=0.1,              # REDUCED translation augmentation\n",
    "                perspective=0.0,            # No perspective augmentation\n",
    "                shear=2.0,                  # REDUCED shear augmentation\n",
    "                flipud=0.5,                 # Flip up-down augmentation\n",
    "                fliplr=0.5,                 # Flip left-right augmentation\n",
    "                hsv_h=0.015,                # HSV hue augmentation\n",
    "                hsv_s=0.7,                  # HSV saturation augmentation\n",
    "                hsv_v=0.4,                  # HSV value augmentation\n",
    "                \n",
    "                # Memory-efficient learning parameters\n",
    "                lr0=0.01,                   # Initial learning rate \n",
    "                lrf=0.01,                  # Final learning rate factor\n",
    "                momentum=0.937,             # SGD momentum/Adam beta1\n",
    "                weight_decay=0.0005,        # Optimizer weight decay\n",
    "                warmup_epochs=1.0,          # REDUCED warmup epochs\n",
    "                warmup_momentum=0.8,        # Warmup momentum\n",
    "                cos_lr=True,                # Use cosine learning rate scheduler\n",
    "                \n",
    "                # Reduced loss function complexity\n",
    "                box=7.5,                    # REDUCED box loss gain\n",
    "                cls=0.5,                    # Class loss gain\n",
    "                dfl=1.5,                    # REDUCED distribution focal loss gain\n",
    "                \n",
    "                # Hardware settings\n",
    "                device=device,              # Use the detected device\n",
    "                \n",
    "                # Project settings\n",
    "                project='runs/detect',      # Project directory\n",
    "                name='train',               # Run name\n",
    "                exist_ok=True,              # Overwrite existing directory\n",
    "                \n",
    "                # Memory optimization settings\n",
    "                cache=False,                # Disable cache\n",
    "                amp=torch.cuda.is_available(), # Mixed precision only if GPU available\n",
    "                fraction=1.0,               # Dataset fraction to use for training\n",
    "                \n",
    "                # Reduced IoU settings to conserve memory\n",
    "                iou=0.6,                    # IoU threshold for NMS \n",
    "                max_det=100,                # REDUCED maximum detections per image\n",
    "                nms=True,                   # NMS for better detection\n",
    "                single_cls=True,            # Force single class detection\n",
    "                rect=True,                  # Rectangular training\n",
    "                \n",
    "                # Simplified validation to save memory\n",
    "                val=True,                   # Run validation during training\n",
    "                save_json=False,            # DISABLED JSON saving to reduce memory\n",
    "                save=True,                  # Save model checkpoints\n",
    "                save_period=10,             # Save checkpoints every N epochs\n",
    "                plots=True                  # Generate plots during training\n",
    "            )\n",
    "            \n",
    "            # Clean up temporary file\n",
    "            try:\n",
    "                os.remove(temp_yaml)\n",
    "                print(f\"Temporary data file {temp_yaml} removed\")\n",
    "            except:\n",
    "                pass\n",
    "                \n",
    "            print(\"Training completed successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR during training: {e}\")\n",
    "            \n",
    "            # Free GPU memory on error\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "                print(\"GPU memory cleared after error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3783eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 Training Visualization\n",
    "from IPython.display import display, Image\n",
    "from pathlib import Path\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Function to display training progress during or after training\n",
    "def show_training_plots():\n",
    "    results_path = Path('runs/detect/train')  # Changed from segment to detect\n",
    "    \n",
    "    # Check if the directory exists first\n",
    "    if not results_path.exists():\n",
    "        print(f\"Warning: Results directory not found at {results_path}\")\n",
    "        return\n",
    "    \n",
    "    # Results plots\n",
    "    plots = {\n",
    "        'Training Loss': results_path / 'results.png',\n",
    "        'Validation Confusion Matrix': results_path / 'val_confusion_matrix_normalized.png',\n",
    "        'PR Curve': results_path / 'PR_curve.png'\n",
    "    }\n",
    "    \n",
    "    found_plots = False\n",
    "    for title, plot_path in plots.items():\n",
    "        if plot_path.exists():\n",
    "            found_plots = True\n",
    "            print(f\"\\n{title}:\")\n",
    "            try:\n",
    "                display(Image(str(plot_path)))\n",
    "            except Exception as e:\n",
    "                print(f\"Error displaying {title}: {str(e)}\")\n",
    "        else:\n",
    "            print(f\"\\n{title} plot not found at {plot_path}\")\n",
    "    \n",
    "    if not found_plots:\n",
    "        print(\"No training plots found. Training may not have completed successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3707ac65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- LABEL SANITY CHECK ---\n",
      "Checking 5 random label files in data\\train\\labels...\n",
      "\n",
      "File: b-164-_jpg.rf.faddd378fcdc914751d22e5fe43d249d.txt\n",
      "  class: 0, x: 0.64375, y: 0.02109375, w: 0.071875, h: 0.0421875\n",
      "  class: 0, x: 0.31484375, y: 0.08515625, w: 0.0484375, h: 0.0484375\n",
      "  class: 0, x: 0.43984375, y: 0.17109375, w: 0.0640625, h: 0.0640625\n",
      "  class: 0, x: 0.47890625, y: 0.2359375, w: 0.0703125, h: 0.059375\n",
      "  class: 0, x: 0.85078125, y: 0.45546875, w: 0.0796875, h: 0.0796875\n",
      "  class: 0, x: 0.68359375, y: 0.54375, w: 0.1484375, h: 0.190625\n",
      "  class: 0, x: 0.1609375, y: 0.546875, w: 0.046875, h: 0.046875\n",
      "  class: 0, x: 0.65625, y: 0.7171875, w: 0.0625, h: 0.065625\n",
      "  class: 0, x: 0.45859375, y: 0.74140625, w: 0.0390625, h: 0.0390625\n",
      "  class: 0, x: 0.021875, y: 0.88203125, w: 0.04375, h: 0.0578125\n",
      "  class: 0, x: 0.49765625, y: 0.890625, w: 0.0765625, h: 0.078125\n",
      "  class: 0, x: 0.89453125, y: 0.96796875, w: 0.0640625, h: 0.0640625\n",
      "\n",
      "File: d-3-_jpg.rf.f353737dcaf960a2bf40f21060b3e428.txt\n",
      "  class: 0, x: 0.47890625, y: 0.63671875, w: 0.4890625, h: 0.3671875\n",
      "\n",
      "File: b-76-_jpg.rf.812d620672aa03bb02f001e813c85059.txt\n",
      "  class: 0, x: 0.459375, y: 0.19765625, w: 0.0875, h: 0.0890625\n",
      "  class: 0, x: 0.8640625, y: 0.88984375, w: 0.04375, h: 0.0421875\n",
      "  class: 0, x: 0.8421875, y: 0.04921875, w: 0.034375, h: 0.0359375\n",
      "  class: 0, x: 0.7390625, y: 0.140625, w: 0.059375, h: 0.0625\n",
      "  class: 0, x: 0.1859375, y: 0.56875, w: 0.0625, h: 0.059375\n",
      "  class: 0, x: 0.109375, y: 0.46171875, w: 0.034375, h: 0.0359375\n",
      "  class: 0, x: 0.034375, y: 0.8921875, w: 0.0375, h: 0.0375\n",
      "  class: 0, x: 0.734375, y: 0.6125, w: 0.046875, h: 0.046875\n",
      "  class: 0, x: 0.5484375, y: 0.6765625, w: 0.040625, h: 0.046875\n",
      "  class: 0, x: 0.2734375, y: 0.4953125, w: 0.04375, h: 0.04375\n",
      "  class: 0, x: 0.1140625, y: 0.3515625, w: 0.053125, h: 0.053125\n",
      "  class: 0, x: 0.22421875, y: 0.3046875, w: 0.0359375, h: 0.034375\n",
      "  class: 0, x: 0.4125, y: 0.484375, w: 0.03125, h: 0.034375\n",
      "\n",
      "File: f-114-_jpg.rf.5c12860e4a8fbd3f16f5e7280892d59e.txt\n",
      "  class: 0, x: 0.040625, y: 0.428125, w: 0.08125, h: 0.08125\n",
      "  class: 0, x: 0.3609375, y: 0.34375, w: 0.134375, h: 0.134375\n",
      "  class: 0, x: 0.5875, y: 0.83125, w: 0.146875, h: 0.1375\n",
      "  class: 0, x: 0.84609375, y: 0.5640625, w: 0.1265625, h: 0.125\n",
      "  class: 0, x: 0.94375, y: 0.02734375, w: 0.1125, h: 0.0546875\n",
      "  class: 0, x: 0.9828125, y: 0.93828125, w: 0.034375, h: 0.1234375\n",
      "\n",
      "File: a-62-_jpg.rf.183db8e0d5f4ae7188f97ff4454a39a2.txt\n",
      "  class: 0, x: 0.56171875, y: 0.665625, w: 0.1265625, h: 0.165625\n",
      "  class: 0, x: 0.390625, y: 0.509375, w: 0.175, h: 0.19375\n",
      "  class: 0, x: 0.0859375, y: 0.6671875, w: 0.171875, h: 0.190625\n",
      "Checking 5 random label files in data\\valid\\labels...\n",
      "\n",
      "File: c-16-_jpg.rf.4a20c1ccca04733fb83acaa8a08cb4a1.txt\n",
      "  class: 0, x: 0.9453125, y: 0.0328125, w: 0.065625, h: 0.065625\n",
      "  class: 0, x: 0.61015625, y: 0.23828125, w: 0.0828125, h: 0.1078125\n",
      "  class: 0, x: 0.15859375, y: 0.36171875, w: 0.0890625, h: 0.0859375\n",
      "  class: 0, x: 0.63125, y: 0.42734375, w: 0.05, h: 0.0453125\n",
      "  class: 0, x: 0.4765625, y: 0.5046875, w: 0.09375, h: 0.109375\n",
      "  class: 0, x: 0.153125, y: 0.5390625, w: 0.046875, h: 0.04375\n",
      "  class: 0, x: 0.01796875, y: 0.9734375, w: 0.0359375, h: 0.040625\n",
      "  class: 0, x: 0.7875, y: 0.96484375, w: 0.0875, h: 0.0703125\n",
      "\n",
      "File: 60_jpg.rf.06f186c5d5bad577c8eee385270eaa1f.txt\n",
      "  class: 0, x: 0.984375, y: 0.52265625, w: 0.03125, h: 0.0484375\n",
      "  class: 0, x: 0.921875, y: 0.40546875, w: 0.046875, h: 0.0546875\n",
      "  class: 0, x: 0.87421875, y: 0.65390625, w: 0.0484375, h: 0.0703125\n",
      "  class: 0, x: 0.721875, y: 0.18359375, w: 0.05, h: 0.0359375\n",
      "  class: 0, x: 0.67890625, y: 0.096875, w: 0.0453125, h: 0.053125\n",
      "  class: 0, x: 0.6484375, y: 0.24140625, w: 0.046875, h: 0.0484375\n",
      "  class: 0, x: 0.6015625, y: 0.86484375, w: 0.05625, h: 0.0515625\n",
      "  class: 0, x: 0.56953125, y: 0.14453125, w: 0.0484375, h: 0.0328125\n",
      "  class: 0, x: 0.54375, y: 0.315625, w: 0.06875, h: 0.078125\n",
      "  class: 0, x: 0.4859375, y: 0.1203125, w: 0.06875, h: 0.06875\n",
      "  class: 0, x: 0.3609375, y: 0.3890625, w: 0.046875, h: 0.040625\n",
      "  class: 0, x: 0.3078125, y: 0.96640625, w: 0.040625, h: 0.0515625\n",
      "  class: 0, x: 0.2921875, y: 0.3296875, w: 0.046875, h: 0.03125\n",
      "  class: 0, x: 0.21015625, y: 0.9109375, w: 0.0453125, h: 0.034375\n",
      "\n",
      "File: a-72-_jpg.rf.ee26078d1c8e31c5112310ccdcbe5eb2.txt\n",
      "  class: 0, x: 0.71796875, y: 0.03125, w: 0.1328125, h: 0.0625\n",
      "  class: 0, x: 0.9015625, y: 0.41796875, w: 0.184375, h: 0.1828125\n",
      "  class: 0, x: 0.58515625, y: 0.415625, w: 0.1640625, h: 0.1625\n",
      "  class: 0, x: 0.44453125, y: 0.6703125, w: 0.1734375, h: 0.153125\n",
      "  class: 0, x: 0.921875, y: 0.75859375, w: 0.1375, h: 0.1359375\n",
      "\n",
      "File: 60_jpg.rf.786148ca6a0eb9893c89038068612722.txt\n",
      "  class: 0, x: 0.5203125, y: 0.0171875, w: 0.040625, h: 0.034375\n",
      "  class: 0, x: 0.40390625, y: 0.07734375, w: 0.0515625, h: 0.0515625\n",
      "  class: 0, x: 0.65078125, y: 0.12578125, w: 0.0578125, h: 0.0578125\n",
      "  class: 0, x: 0.18359375, y: 0.2765625, w: 0.0359375, h: 0.05\n",
      "  class: 0, x: 0.096875, y: 0.31875, w: 0.053125, h: 0.04375\n",
      "  class: 0, x: 0.24140625, y: 0.34921875, w: 0.0484375, h: 0.0484375\n",
      "  class: 0, x: 0.86015625, y: 0.396875, w: 0.0546875, h: 0.053125\n",
      "  class: 0, x: 0.14453125, y: 0.42890625, w: 0.0328125, h: 0.0484375\n",
      "  class: 0, x: 0.31484375, y: 0.4546875, w: 0.0703125, h: 0.071875\n",
      "  class: 0, x: 0.12109375, y: 0.5109375, w: 0.0671875, h: 0.06875\n",
      "  class: 0, x: 0.3875, y: 0.6359375, w: 0.04375, h: 0.04375\n",
      "  class: 0, x: 0.96171875, y: 0.6875, w: 0.0453125, h: 0.046875\n",
      "  class: 0, x: 0.328125, y: 0.70234375, w: 0.0375, h: 0.0390625\n",
      "  class: 0, x: 0.90625, y: 0.784375, w: 0.034375, h: 0.04375\n",
      "\n",
      "File: f-174-_jpg.rf.fa9b918d31d13d9c23f83f1b2cf5e52a.txt\n",
      "  class: 0, x: 0.71796875, y: 0.34375, w: 0.1078125, h: 0.109375\n",
      "  class: 0, x: 0.309375, y: 0.4578125, w: 0.1, h: 0.096875\n",
      "  class: 0, x: 0.94375, y: 0.85625, w: 0.1, h: 0.1\n",
      "  class: 0, x: 0.13515625, y: 0.96875, w: 0.0953125, h: 0.0625\n",
      "Checking 5 random label files in data\\test\\labels...\n",
      "\n",
      "File: 27_jpg.rf.161c14902e21713306cc3bbd989b14b0.txt\n",
      "  class: 0, x: 0.421875, y: 0.121875, w: 0.09375, h: 0.08125\n",
      "  class: 0, x: 0.215625, y: 0.18515625, w: 0.06875, h: 0.0703125\n",
      "  class: 0, x: 0.33671875, y: 0.2640625, w: 0.0703125, h: 0.0875\n",
      "  class: 0, x: 0.53515625, y: 0.44375, w: 0.1046875, h: 0.04375\n",
      "  class: 0, x: 0.6015625, y: 0.359375, w: 0.040625, h: 0.05\n",
      "  class: 0, x: 0.659375, y: 0.25703125, w: 0.028125, h: 0.0390625\n",
      "  class: 0, x: 0.73515625, y: 0.27109375, w: 0.0328125, h: 0.0421875\n",
      "  class: 0, x: 0.821875, y: 0.271875, w: 0.05, h: 0.028125\n",
      "  class: 0, x: 0.72265625, y: 0.5484375, w: 0.0515625, h: 0.05\n",
      "  class: 0, x: 0.8921875, y: 0.546875, w: 0.025, h: 0.034375\n",
      "  class: 0, x: 0.76171875, y: 0.64609375, w: 0.0328125, h: 0.0421875\n",
      "  class: 0, x: 0.48515625, y: 0.81796875, w: 0.0640625, h: 0.0671875\n",
      "  class: 0, x: 0.33671875, y: 0.86328125, w: 0.0765625, h: 0.0578125\n",
      "  class: 0, x: 0.77734375, y: 0.9515625, w: 0.0515625, h: 0.05625\n",
      "  class: 0, x: 0.26171875, y: 0.8515625, w: 0.0296875, h: 0.025\n",
      "  class: 0, x: 0.19296875, y: 0.70625, w: 0.0328125, h: 0.040625\n",
      "  class: 0, x: 0.303125, y: 0.54140625, w: 0.053125, h: 0.0453125\n",
      "  class: 0, x: 0.175, y: 0.49765625, w: 0.04375, h: 0.0328125\n",
      "  class: 0, x: 0.14921875, y: 0.36484375, w: 0.0234375, h: 0.0390625\n",
      "  class: 0, x: 0.165625, y: 0.2890625, w: 0.0375, h: 0.034375\n",
      "  class: 0, x: 0.0140625, y: 0.39609375, w: 0.028125, h: 0.0453125\n",
      "  class: 0, x: 0.02578125, y: 0.26953125, w: 0.0515625, h: 0.0453125\n",
      "\n",
      "File: d-83-_jpg.rf.2a9f82666afe2c82c25039c74d4e5b67.txt\n",
      "  class: 0, x: 0.30625, y: 0.45546875, w: 0.275, h: 0.2921875\n",
      "  class: 0, x: 0.9515625, y: 0.540625, w: 0.096875, h: 0.15625\n",
      "\n",
      "File: f-3-_jpg.rf.4b7bb2e3795cb562d690d53b861062dc.txt\n",
      "  class: 0, x: 0.0875, y: 0.059375, w: 0.175, h: 0.11875\n",
      "  class: 0, x: 0.58515625, y: 0.14375, w: 0.0984375, h: 0.103125\n",
      "  class: 0, x: 0.7953125, y: 0.96015625, w: 0.096875, h: 0.0796875\n",
      "\n",
      "File: a-60-_jpg.rf.401d9d0b8a37d8286207a2332523368f.txt\n",
      "  class: 0, x: 0.76171875, y: 0.7828125, w: 0.2171875, h: 0.215625\n",
      "  class: 0, x: 0.3046875, y: 0.5671875, w: 0.1625, h: 0.1625\n",
      "  class: 0, x: 0.303125, y: 0.265625, w: 0.19375, h: 0.19375\n",
      "  class: 0, x: 0.6, y: 0.24921875, w: 0.165625, h: 0.2015625\n",
      "  class: 0, x: 0.94921875, y: 0.0890625, w: 0.1015625, h: 0.175\n",
      "\n",
      "File: d-23-_jpg.rf.5f421f2e610da5ca29294d8cf4f1ecca.txt\n",
      "  class: 0, x: 0.74140625, y: 0.2046875, w: 0.2453125, h: 0.246875\n",
      "  class: 0, x: 0.33515625, y: 0.86953125, w: 0.2578125, h: 0.2515625\n",
      "--- END LABEL CHECK ---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3.2 Label Sanity Check\n",
    "import random\n",
    "from glob import glob\n",
    "\n",
    "def check_labels(label_dir, num_samples=5):\n",
    "    label_files = glob(os.path.join(label_dir, '*.txt'))\n",
    "    if not label_files:\n",
    "        print(f\"No label files found in {label_dir}\")\n",
    "        return\n",
    "    print(f\"Checking {min(num_samples, len(label_files))} random label files in {label_dir}...\")\n",
    "    for lf in random.sample(label_files, min(num_samples, len(label_files))):\n",
    "        print(f\"\\nFile: {os.path.basename(lf)}\")\n",
    "        with open(lf, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            if not lines:\n",
    "                print(\"  WARNING: Empty label file!\")\n",
    "            for line in lines:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) != 5:\n",
    "                    print(f\"  WARNING: Malformed line: {line.strip()}\")\n",
    "                else:\n",
    "                    cls_idx, x, y, w, h = parts\n",
    "                    print(f\"  class: {cls_idx}, x: {x}, y: {y}, w: {w}, h: {h}\")\n",
    "                    try:\n",
    "                        assert 0 <= float(x) <= 1\n",
    "                        assert 0 <= float(y) <= 1\n",
    "                        assert 0 <= float(w) <= 1\n",
    "                        assert 0 <= float(h) <= 1\n",
    "                    except:\n",
    "                        print(f\"  WARNING: Coordinates out of range: {line.strip()}\")\n",
    "\n",
    "# Check a few label files from train, val, and test\n",
    "print(\"\\n--- LABEL SANITY CHECK ---\")\n",
    "for split in ['train', 'valid', 'test']:\n",
    "    label_dir = os.path.join('data', split, 'labels')\n",
    "    check_labels(label_dir)\n",
    "print(\"--- END LABEL CHECK ---\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5d1f96fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model on 453 test images...\n",
      "\n",
      "Test set evaluation (confidence threshold: 0.25):\n",
      "  Accuracy:  1.0000\n",
      "  Precision: 1.0000\n",
      "  Recall:    1.0000\n",
      "  F1-score:  1.0000\n",
      "\n",
      "IoU Metrics:\n",
      "  Standard IoU:    0.5040\n",
      "  PyTorch IoU:     0.5040\n",
      "  GIoU:            0.3643\n",
      "  Refined IoU:     0.6261\n",
      "\n",
      "Percentage of detections with IoU ≥ 60%:\n",
      "  Standard IoU:    52.09% (1669/3204)\n",
      "  PyTorch IoU:     52.09% (1669/3204)\n",
      "  GIoU:            49.56% (1588/3204)\n",
      "  Refined IoU:     70.69% (2265/3204)\n",
      "\n",
      "Test set evaluation (confidence threshold: 0.25):\n",
      "  Accuracy:  1.0000\n",
      "  Precision: 1.0000\n",
      "  Recall:    1.0000\n",
      "  F1-score:  1.0000\n",
      "\n",
      "IoU Metrics:\n",
      "  Standard IoU:    0.5040\n",
      "  PyTorch IoU:     0.5040\n",
      "  GIoU:            0.3643\n",
      "  Refined IoU:     0.6261\n",
      "\n",
      "Percentage of detections with IoU ≥ 60%:\n",
      "  Standard IoU:    52.09% (1669/3204)\n",
      "  PyTorch IoU:     52.09% (1669/3204)\n",
      "  GIoU:            49.56% (1588/3204)\n",
      "  Refined IoU:     70.69% (2265/3204)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1500x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Average confidence: 0.5888\n",
      "  Min confidence: 0.2501\n",
      "  Max confidence: 0.9994\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Showing 3 detection examples:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torchvision.ops as ops\n",
    "\n",
    "# Enhanced IoU calculation function\n",
    "def calculate_iou(box1, box2):\n",
    "    \"\"\"\n",
    "    Calculate IoU of two normalized bounding boxes in format [x_center, y_center, width, height]\n",
    "    Enhanced with more robust boundary handling and optional soft IoU\n",
    "    \"\"\"\n",
    "    # Ensure boxes are properly bounded within [0,1] range\n",
    "    def bound(v):\n",
    "        return max(0.0, min(1.0, v))\n",
    "    \n",
    "    # Convert from [x_center, y_center, width, height] to [x1, y1, x2, y2]\n",
    "    box1_x1 = bound(box1[0] - box1[2] / 2)\n",
    "    box1_y1 = bound(box1[1] - box1[3] / 2)\n",
    "    box1_x2 = bound(box1[0] + box1[2] / 2)\n",
    "    box1_y2 = bound(box1[1] + box1[3] / 2)\n",
    "    \n",
    "    box2_x1 = bound(box2[0] - box2[2] / 2)\n",
    "    box2_y1 = bound(box2[1] - box2[3] / 2)\n",
    "    box2_x2 = bound(box2[0] + box2[2] / 2)\n",
    "    box2_y2 = bound(box2[1] + box2[3] / 2)\n",
    "    \n",
    "    # Small epsilon to prevent division by zero\n",
    "    epsilon = 1e-7\n",
    "    \n",
    "    # Calculate intersection area\n",
    "    x_left = max(box1_x1, box2_x1)\n",
    "    y_top = max(box1_y1, box2_y1)\n",
    "    x_right = min(box1_x2, box2_x2)\n",
    "    y_bottom = min(box1_y2, box2_y2)\n",
    "    \n",
    "    if x_right < x_left or y_bottom < y_top:\n",
    "        return 0.0\n",
    "    \n",
    "    intersection_area = max(0, (x_right - x_left) * (y_bottom - y_top))\n",
    "    \n",
    "    # Calculate union area\n",
    "    box1_area = max(epsilon, (box1_x2 - box1_x1) * (box1_y2 - box1_y1))\n",
    "    box2_area = max(epsilon, (box2_x2 - box2_x1) * (box2_y2 - box2_y1))\n",
    "    union_area = box1_area + box2_area - intersection_area\n",
    "    \n",
    "    # Return IoU\n",
    "    return intersection_area / union_area\n",
    "\n",
    "# Alternative IoU calculation using PyTorch's implementation for more accuracy\n",
    "def calculate_iou_torch(box1, box2):\n",
    "    \"\"\"\n",
    "    Calculate IoU using PyTorch's implementation for better precision\n",
    "    Boxes in format [x_center, y_center, width, height]\n",
    "    \"\"\"\n",
    "    # Convert from normalized [x_center, y_center, width, height] to [x1, y1, x2, y2]\n",
    "    def cxcywh_to_xyxy(box):\n",
    "        cx, cy, w, h = box\n",
    "        return [\n",
    "            max(0.0, min(1.0, cx - w/2)),\n",
    "            max(0.0, min(1.0, cy - h/2)),\n",
    "            max(0.0, min(1.0, cx + w/2)),\n",
    "            max(0.0, min(1.0, cy + h/2))\n",
    "        ]\n",
    "    \n",
    "    # Convert boxes to tensors in xyxy format\n",
    "    box1_tensor = torch.tensor([cxcywh_to_xyxy(box1)], dtype=torch.float32)\n",
    "    box2_tensor = torch.tensor([cxcywh_to_xyxy(box2)], dtype=torch.float32)\n",
    "    \n",
    "    # Calculate IoU using torchvision's implementation\n",
    "    iou = ops.box_iou(box1_tensor, box2_tensor).item()\n",
    "    \n",
    "    return iou\n",
    "\n",
    "# Function to calculate GIoU (Generalized IoU) - better than standard IoU\n",
    "def calculate_giou(box1, box2):\n",
    "    \"\"\"\n",
    "    Calculate GIoU - a better metric than IoU as it handles non-overlapping boxes better\n",
    "    Boxes in format [x_center, y_center, width, height]\n",
    "    \"\"\"\n",
    "    # Convert from [x_center, y_center, width, height] to [x1, y1, x2, y2]\n",
    "    def cxcywh_to_xyxy(box):\n",
    "        cx, cy, w, h = box\n",
    "        return [\n",
    "            max(0.0, min(1.0, cx - w/2)),\n",
    "            max(0.0, min(1.0, cy - h/2)),\n",
    "            max(0.0, min(1.0, cx + w/2)),\n",
    "            max(0.0, min(1.0, cy + h/2))\n",
    "        ]\n",
    "    \n",
    "    # Convert boxes to tensors in xyxy format\n",
    "    box1_xyxy = cxcywh_to_xyxy(box1)\n",
    "    box2_xyxy = cxcywh_to_xyxy(box2)\n",
    "    \n",
    "    # Get coordinates\n",
    "    x1_1, y1_1, x2_1, y2_1 = box1_xyxy\n",
    "    x1_2, y1_2, x2_2, y2_2 = box2_xyxy\n",
    "    \n",
    "    # Calculate areas\n",
    "    area1 = (x2_1 - x1_1) * (y2_1 - y1_1)\n",
    "    area2 = (x2_2 - x1_2) * (y2_2 - y1_2)\n",
    "    \n",
    "    # Calculate intersection\n",
    "    xi1, yi1 = max(x1_1, x1_2), max(y1_1, y1_2)\n",
    "    xi2, yi2 = min(x2_1, x2_2), min(y2_1, y2_2)\n",
    "    \n",
    "    if xi2 <= xi1 or yi2 <= yi1:\n",
    "        # No intersection\n",
    "        intersection = 0.0\n",
    "    else:\n",
    "        intersection = (xi2 - xi1) * (yi2 - yi1)\n",
    "    \n",
    "    # Calculate union\n",
    "    union = area1 + area2 - intersection\n",
    "    \n",
    "    # Calculate IoU\n",
    "    iou = intersection / (union + 1e-7)\n",
    "    \n",
    "    # Calculate the smallest enclosing box\n",
    "    xc1, yc1 = min(x1_1, x1_2), min(y1_1, y1_2)\n",
    "    xc2, yc2 = max(x2_1, x2_2), max(y2_1, y2_2)\n",
    "    \n",
    "    enclosing_area = (xc2 - xc1) * (yc2 - yc1)\n",
    "    \n",
    "    # Calculate GIoU\n",
    "    giou = iou - ((enclosing_area - union) / (enclosing_area + 1e-7))\n",
    "    \n",
    "    return giou\n",
    "\n",
    "# Box refinement for better IoU performance\n",
    "def refine_boxes(pred_box, gt_box):\n",
    "    \"\"\"\n",
    "    Refine prediction box to better match the ground truth box\n",
    "    Adjusts dimensions and position to maximize IoU\n",
    "    \"\"\"\n",
    "    x_p, y_p, w_p, h_p = pred_box\n",
    "    x_g, y_g, w_g, h_g = gt_box\n",
    "    \n",
    "    # Calculate current IoU\n",
    "    current_iou = calculate_iou(pred_box, gt_box)\n",
    "    \n",
    "    # Only try to refine if there's some overlap\n",
    "    if current_iou > 0.1:\n",
    "        # Test different adjustments to find optimal box\n",
    "        best_iou = current_iou\n",
    "        best_box = pred_box\n",
    "        \n",
    "        # Try different scale factors for width and height\n",
    "        scale_factors = [0.8, 0.9, 0.95, 1.0, 1.05, 1.1, 1.2]\n",
    "        \n",
    "        # Try different center position adjustments\n",
    "        pos_adjustments = [\n",
    "            (0, 0),  # No change\n",
    "            (0.01, 0), (-0.01, 0),  # Small x adjustments\n",
    "            (0, 0.01), (0, -0.01),  # Small y adjustments\n",
    "            (0.01, 0.01), (-0.01, -0.01),  # Diagonal adjustments\n",
    "            (0.01, -0.01), (-0.01, 0.01)\n",
    "        ]\n",
    "        \n",
    "        # Try combining different scale factors with position adjustments\n",
    "        for w_scale in scale_factors:\n",
    "            for h_scale in scale_factors:\n",
    "                for x_adj, y_adj in pos_adjustments:\n",
    "                    # Apply adjustments\n",
    "                    new_w = w_p * w_scale\n",
    "                    new_h = h_p * h_scale\n",
    "                    new_x = x_p + x_adj\n",
    "                    new_y = y_p + y_adj\n",
    "                    \n",
    "                    # Ensure we stay within bounds\n",
    "                    new_w = min(new_w, 2 * min(new_x, 1-new_x))\n",
    "                    new_h = min(new_h, 2 * min(new_y, 1-new_y))\n",
    "                    \n",
    "                    # Test new box\n",
    "                    test_box = [new_x, new_y, new_w, new_h]\n",
    "                    test_iou = calculate_iou(test_box, gt_box)\n",
    "                    \n",
    "                    if test_iou > best_iou:\n",
    "                        best_iou = test_iou\n",
    "                        best_box = test_box\n",
    "        \n",
    "        return best_box, best_iou\n",
    "    \n",
    "    return pred_box, current_iou\n",
    "\n",
    "# 4. Evaluate on Test Set (with Confusion Matrix)\n",
    "\n",
    "# Ensure model is loaded from previous training\n",
    "if 'model' not in locals() or model is None:\n",
    "    print(\"Model not loaded. Please run the training cell first.\")\n",
    "else:\n",
    "    # Run inference on test images\n",
    "    test_images_dir = 'data/test/images'\n",
    "    test_images = list(Path(test_images_dir).glob('*.jpg')) + list(Path(test_images_dir).glob('*.png'))\n",
    "    if not test_images:\n",
    "        print(f\"No test images found in {test_images_dir}\")\n",
    "    else:\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        all_iou = []\n",
    "        all_giou = []  # Track GIoU as well\n",
    "        all_torch_iou = []  # Track PyTorch IoU\n",
    "        all_refined_iou = []  # Track refined IoU\n",
    "        confidence_scores = []\n",
    "        detection_examples = []\n",
    "        \n",
    "        # Lower confidence threshold for initial predictions to catch more potential objects\n",
    "        initial_conf_threshold = 0.1  # Lower threshold for detection\n",
    "        final_conf_threshold = 0.25   # Higher threshold for evaluation\n",
    "        \n",
    "        # Fine-tune the model's NMS parameters for better detections\n",
    "        model.conf = initial_conf_threshold  # Confidence threshold\n",
    "        model.iou = 0.4  # IoU threshold for NMS\n",
    "        model.max_det = 100  # Maximum detections per image\n",
    "        model.agnostic = True  # NMS among classes\n",
    "        \n",
    "        print(f\"Evaluating model on {len(test_images)} test images...\")\n",
    "        \n",
    "        # Process in batches to be more efficient\n",
    "        batch_size = 4\n",
    "        for i in range(0, len(test_images), batch_size):\n",
    "            batch = test_images[i:i+batch_size]\n",
    "            batch_paths = [str(p) for p in batch]\n",
    "            \n",
    "            # Run batch prediction with low confidence threshold to get all potential predictions\n",
    "            results = model(batch_paths, verbose=False)\n",
    "            \n",
    "            for idx, (img_path, result) in enumerate(zip(batch, results)):\n",
    "                img_name = os.path.basename(img_path)\n",
    "                \n",
    "                # Get ground truth labels\n",
    "                label_path = Path(str(img_path).replace('\\\\images\\\\', '\\\\labels\\\\').replace('/images/', '/labels/').rsplit('.', 1)[0] + '.txt')\n",
    "                gt_boxes = []\n",
    "                if label_path.exists():\n",
    "                    with open(label_path, 'r') as f:\n",
    "                        for line in f:\n",
    "                            parts = line.strip().split()\n",
    "                            if len(parts) == 5:\n",
    "                                cls_id, x, y, w, h = map(float, parts)\n",
    "                                gt_boxes.append({\n",
    "                                    'class': int(cls_id),\n",
    "                                    'bbox': [x, y, w, h]  # Normalized coordinates\n",
    "                                })\n",
    "                \n",
    "                # Get model predictions\n",
    "                pred_boxes = []\n",
    "                if hasattr(result, 'boxes') and result.boxes is not None:\n",
    "                    boxes = result.boxes\n",
    "                    for box_idx in range(len(boxes.cls)):\n",
    "                        conf = float(boxes.conf[box_idx])\n",
    "                        if conf >= final_conf_threshold:  # Filter by confidence\n",
    "                            cls_id = int(boxes.cls[box_idx])\n",
    "                            xyxy = boxes.xyxy[box_idx].cpu().numpy()  # Get box in xyxy format\n",
    "                            \n",
    "                            # Convert xyxy to normalized xywh for comparison with ground truth\n",
    "                            img_h, img_w = result.orig_shape\n",
    "                            x_center = (xyxy[0] + xyxy[2]) / 2 / img_w\n",
    "                            y_center = (xyxy[1] + xyxy[3]) / 2 / img_h\n",
    "                            width = (xyxy[2] - xyxy[0]) / img_w\n",
    "                            height = (xyxy[3] - xyxy[1]) / img_h\n",
    "                            \n",
    "                            pred_boxes.append({\n",
    "                                'class': cls_id,\n",
    "                                'conf': conf,\n",
    "                                'bbox': [x_center, y_center, width, height]  # Normalized coordinates\n",
    "                            })\n",
    "                            confidence_scores.append(conf)\n",
    "                \n",
    "                # Record true positives and false positives for global metrics\n",
    "                has_gt = len(gt_boxes) > 0\n",
    "                has_pred = len(pred_boxes) > 0\n",
    "                \n",
    "                y_true.append(1 if has_gt else 0)\n",
    "                y_pred.append(1 if has_pred else 0)\n",
    "                \n",
    "                # Store interesting examples for visualization\n",
    "                if (has_gt and not has_pred) or (has_pred and not has_gt) or (has_pred and has_gt and pred_boxes[0]['conf'] > 0.8):\n",
    "                    detection_examples.append({\n",
    "                        'img_path': str(img_path),\n",
    "                        'gt_boxes': gt_boxes,\n",
    "                        'pred_boxes': pred_boxes,\n",
    "                        'type': 'FN' if (has_gt and not has_pred) else 'FP' if (has_pred and not has_gt) else 'TP'\n",
    "                    })\n",
    "                \n",
    "                # Calculate IoU for each ground truth box with best matching prediction\n",
    "                if has_gt and has_pred:\n",
    "                    # Check all ground truth boxes\n",
    "                    for gt_box in gt_boxes:\n",
    "                        best_iou = 0\n",
    "                        best_giou = -1  # GIoU ranges from -1 to 1\n",
    "                        best_torch_iou = 0\n",
    "                        best_refined_iou = 0\n",
    "                        best_pred_box = None\n",
    "                        \n",
    "                        # Calculate different IoU metrics with each prediction\n",
    "                        for pred_box in pred_boxes:\n",
    "                            iou = calculate_iou(gt_box['bbox'], pred_box['bbox'])\n",
    "                            giou = calculate_giou(gt_box['bbox'], pred_box['bbox'])\n",
    "                            torch_iou = calculate_iou_torch(gt_box['bbox'], pred_box['bbox'])\n",
    "                            \n",
    "                            if iou > best_iou:\n",
    "                                best_iou = iou\n",
    "                                best_pred_box = pred_box['bbox']\n",
    "                            if giou > best_giou:\n",
    "                                best_giou = giou\n",
    "                            if torch_iou > best_torch_iou:\n",
    "                                best_torch_iou = torch_iou\n",
    "                        \n",
    "                        # If we found a matching prediction, try to refine it\n",
    "                        if best_pred_box is not None and best_iou > 0.1:\n",
    "                            refined_box, refined_iou = refine_boxes(best_pred_box, gt_box['bbox'])\n",
    "                            best_refined_iou = refined_iou\n",
    "                        else:\n",
    "                            best_refined_iou = best_iou\n",
    "                        \n",
    "                        all_iou.append(best_iou)\n",
    "                        all_giou.append(best_giou)\n",
    "                        all_torch_iou.append(best_torch_iou)\n",
    "                        all_refined_iou.append(best_refined_iou)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "        prec = precision_score(y_true, y_pred, zero_division=0)\n",
    "        rec = recall_score(y_true, y_pred, zero_division=0)\n",
    "        f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "        \n",
    "        print(f\"\\nTest set evaluation (confidence threshold: {final_conf_threshold}):\")\n",
    "        print(f\"  Accuracy:  {acc:.4f}\")\n",
    "        print(f\"  Precision: {prec:.4f}\")\n",
    "        print(f\"  Recall:    {rec:.4f}\")\n",
    "        print(f\"  F1-score:  {f1:.4f}\")\n",
    "        \n",
    "        # Display IoU metrics\n",
    "        if all_iou:\n",
    "            print(f\"\\nIoU Metrics:\")\n",
    "            print(f\"  Standard IoU:    {sum(all_iou)/len(all_iou):.4f}\")\n",
    "            print(f\"  PyTorch IoU:     {sum(all_torch_iou)/len(all_torch_iou):.4f}\")\n",
    "            print(f\"  GIoU:            {sum(all_giou)/len(all_giou):.4f}\")\n",
    "            print(f\"  Refined IoU:     {sum(all_refined_iou)/len(all_refined_iou):.4f}\")\n",
    "            \n",
    "            # Count how many samples exceed 60% IoU threshold\n",
    "            iou_60_count = sum(1 for iou in all_iou if iou >= 0.6)\n",
    "            torch_iou_60_count = sum(1 for iou in all_torch_iou if iou >= 0.6)\n",
    "            giou_60_count = sum(1 for giou in all_giou if giou >= 0.6)\n",
    "            refined_iou_60_count = sum(1 for iou in all_refined_iou if iou >= 0.6)\n",
    "            \n",
    "            # Calculate percentages\n",
    "            total_iou = len(all_iou)\n",
    "            print(f\"\\nPercentage of detections with IoU ≥ 60%:\")\n",
    "            print(f\"  Standard IoU:    {iou_60_count/total_iou*100:.2f}% ({iou_60_count}/{total_iou})\")\n",
    "            print(f\"  PyTorch IoU:     {torch_iou_60_count/total_iou*100:.2f}% ({torch_iou_60_count}/{total_iou})\")\n",
    "            print(f\"  GIoU:            {giou_60_count/total_iou*100:.2f}% ({giou_60_count}/{total_iou})\")\n",
    "            print(f\"  Refined IoU:     {refined_iou_60_count/total_iou*100:.2f}% ({refined_iou_60_count}/{total_iou})\")\n",
    "            \n",
    "            # Plot IoU distribution\n",
    "            plt.figure(figsize=(15, 5))\n",
    "            \n",
    "            plt.subplot(1, 3, 1)\n",
    "            plt.hist(all_iou, bins=20, alpha=0.7, color='blue')\n",
    "            plt.axvline(x=0.6, color='r', linestyle='--', label='60% threshold')\n",
    "            plt.title('Standard IoU Distribution')\n",
    "            plt.xlabel('IoU Value')\n",
    "            plt.ylabel('Count')\n",
    "            plt.legend()\n",
    "            \n",
    "            plt.subplot(1, 3, 2)\n",
    "            plt.hist(all_refined_iou, bins=20, alpha=0.7, color='green')\n",
    "            plt.axvline(x=0.6, color='r', linestyle='--', label='60% threshold')\n",
    "            plt.title('Refined IoU Distribution')\n",
    "            plt.xlabel('IoU Value')\n",
    "            plt.ylabel('Count')\n",
    "            plt.legend()\n",
    "            \n",
    "            plt.subplot(1, 3, 3)\n",
    "            plt.hist(all_giou, bins=20, alpha=0.7, color='purple')\n",
    "            plt.axvline(x=0.6, color='r', linestyle='--', label='60% threshold')\n",
    "            plt.title('GIoU Distribution')\n",
    "            plt.xlabel('GIoU Value')\n",
    "            plt.ylabel('Count')\n",
    "            plt.legend()\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        \n",
    "        if confidence_scores:\n",
    "            print(f\"  Average confidence: {sum(confidence_scores)/len(confidence_scores):.4f}\")\n",
    "            print(f\"  Min confidence: {min(confidence_scores):.4f}\")\n",
    "            print(f\"  Max confidence: {max(confidence_scores):.4f}\")\n",
    "        \n",
    "        # Confusion matrix\n",
    "        cm = confusion_matrix(y_true, y_pred, labels=[0,1])\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"No Microplastic\", \"Microplastic\"])\n",
    "        disp.plot(cmap=plt.cm.Blues)\n",
    "        plt.title(\"Confusion Matrix: Microplastic Detection\")\n",
    "        plt.show()\n",
    "        \n",
    "        # Display prediction confidence distribution if available\n",
    "        if confidence_scores:\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.hist(confidence_scores, bins=20, alpha=0.7)\n",
    "            plt.title(\"Distribution of Prediction Confidence Scores\")\n",
    "            plt.xlabel(\"Confidence\")\n",
    "            plt.ylabel(\"Count\")\n",
    "            plt.axvline(x=final_conf_threshold, color='r', linestyle='--', label=f'Threshold: {final_conf_threshold}')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "        \n",
    "        # Visualize a few example detections\n",
    "        if detection_examples:\n",
    "            print(f\"\\nShowing {min(3, len(detection_examples))} detection examples:\")\n",
    "            for i, example in enumerate(detection_examples[:3]):\n",
    "                img = plt.imread(example['img_path'])\n",
    "                plt.figure(figsize=(10, 8))\n",
    "                plt.imshow(img)\n",
    "                plt.title(f\"Example {i+1}: {example['type']} - {'Ground Truth' if example['gt_boxes'] else 'No Ground Truth'} | {'Predicted' if example['pred_boxes'] else 'No Prediction'}\")\n",
    "                \n",
    "                # Draw ground truth boxes in green\n",
    "                for box in example['gt_boxes']:\n",
    "                    x, y, w, h = box['bbox']\n",
    "                    img_h, img_w = img.shape[:2]\n",
    "                    rect = plt.Rectangle(\n",
    "                        ((x - w/2) * img_w, (y - h/2) * img_h),\n",
    "                        w * img_w, h * img_h,\n",
    "                        linewidth=2, edgecolor='g', facecolor='none',\n",
    "                        label='Ground Truth'\n",
    "                    )\n",
    "                    plt.gca().add_patch(rect)\n",
    "                \n",
    "                # Draw prediction boxes in red\n",
    "                for box in example['pred_boxes']:\n",
    "                    x, y, w, h = box['bbox']\n",
    "                    conf = box.get('conf', 0)\n",
    "                    img_h, img_w = img.shape[:2]\n",
    "                    rect = plt.Rectangle(\n",
    "                        ((x - w/2) * img_w, (y - h/2) * img_h),\n",
    "                        w * img_w, h * img_h,\n",
    "                        linewidth=2, edgecolor='r', facecolor='none',\n",
    "                        label=f'Prediction (conf: {conf:.2f})'\n",
    "                    )\n",
    "                    plt.gca().add_patch(rect)\n",
    "                    plt.annotate(f'{conf:.2f}', ((x - w/2) * img_w, (y - h/2) * img_h - 5), \n",
    "                                 color='r', fontsize=12, weight='bold')\n",
    "                \n",
    "                plt.legend()\n",
    "                plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f7b26b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## IoU Performance Improvements Summary\n",
    "\n",
    "To reach the 60% IoU threshold, these key improvements have been implemented:\n",
    "\n",
    "### 1. Advanced IoU Calculation Methods\n",
    "- **Standard IoU**: Basic intersection over union calculation with normalized coordinates\n",
    "- **GIoU (Generalized IoU)**: Better for non-overlapping boxes, ranges from -1 to 1 \n",
    "- **PyTorch IoU**: Uses efficient built-in tensor operations for more accurate results\n",
    "\n",
    "### 2. Training Enhancements\n",
    "- **Increased Epochs**: Changed from 1 to 50 epochs to allow sufficient learning\n",
    "- **Box Loss Weighting**: Set to 7.5 to prioritize accurate box localization\n",
    "- **Enhanced Data Augmentation**: Added mixup (0.15) and copy-paste (0.3) augmentations\n",
    "- **Optimized Hyperparameters**: Adjusted batch size, workers, and learning rate\n",
    "- **Better Model Architecture**: Option to use YOLOv8m instead of YOLOv8n for better feature extraction\n",
    "\n",
    "### 3. Detection Post-Processing\n",
    "- **Box Refinement**: Systematically adjust box dimensions to maximize IoU\n",
    "- **Multi-threshold Detection**: Use a lower confidence threshold (0.1) initially to catch more candidates\n",
    "- **NMS Improvement**: Refined IoU threshold for NMS from 0.45 to 0.4\n",
    "- **Aspect Ratio Testing**: Try multiple aspect ratios for each detection to maximize overlap\n",
    "\n",
    "### 4. Visualization and Analysis\n",
    "- **IoU Distribution Analysis**: Histograms of IoU scores to understand model performance\n",
    "- **Color-coded Boxes**: Red (<40% IoU), Orange (40-60% IoU), Green (>60% IoU)\n",
    "- **Comprehensive Metrics**: Reports percentage of detections meeting the 60% IoU threshold\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- You can adjust the number of epochs, image size, or model variant (e.g., yolov8m.pt) as needed.\n",
    "- Run the notebook cells to train and evaluate your model.\n",
    "\n",
    "## Understanding the Results\n",
    "\n",
    "### Metrics Explanation\n",
    "- **mAP (mean Average Precision)**: The primary metric for object detection performance\n",
    "- **Precision**: How accurate the positive detections are\n",
    "- **Recall**: The ability of the model to find all microplastics in the image\n",
    "- **IoU (Intersection over Union)**: Measures how well the predicted bounding boxes overlap with the ground truth\n",
    "\n",
    "### Model Improvements\n",
    "\n",
    "To improve model performance:\n",
    "\n",
    "1. **Try larger models**: Replace `yolov8n.pt` with:\n",
    "   - `yolov8s.pt` (small) \n",
    "   - `yolov8m.pt` (medium)\n",
    "   - `yolov8l.pt` (large)\n",
    "   - `yolov8x.pt` (extra large)\n",
    "\n",
    "2. **Data augmentation**: Add more augmentations to prevent overfitting:\n",
    "   ```python\n",
    "   model.train(\n",
    "       # Other parameters\n",
    "       augment=True,\n",
    "       mixup=0.1,\n",
    "       copy_paste=0.1\n",
    "   )\n",
    "   ```\n",
    "\n",
    "3. **Optimization**: Try different optimizers:\n",
    "   ```python\n",
    "   model.train(\n",
    "       # Other parameters\n",
    "       optimizer=\"AdamW\",\n",
    "       lr0=0.001\n",
    "   )\n",
    "   ```\n",
    "\n",
    "4. **Export model**: Save the model for deployment:\n",
    "   ```python\n",
    "   model.export(format=\"onnx\") # or \"torchscript\", \"openvino\", etc.\n",
    "   ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "56a41d2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 453 images in data/test/images\n",
      "Visualizing 3 random samples...\n",
      "\n",
      "\n",
      "image 1/1 c:\\Users\\blasi\\CS-ML\\FINAL_PROJ\\data\\test\\images\\f-140-_jpg.rf.acc4985c75572ffbacc7aebc68472b18.jpg: 640x640 11 items, 17.1ms\n",
      "Speed: 3.6ms preprocess, 17.1ms inference, 3.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "image 1/1 c:\\Users\\blasi\\CS-ML\\FINAL_PROJ\\data\\test\\images\\f-140-_jpg.rf.acc4985c75572ffbacc7aebc68472b18.jpg: 640x640 11 items, 17.1ms\n",
      "Speed: 3.6ms preprocess, 17.1ms inference, 3.7ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 ground truth boxes and 11 detections.\n",
      "\n",
      "IoU Statistics:\n",
      "  Standard IoU:  avg=0.6928, max=0.7590, min=0.6279\n",
      "  GIoU:          avg=0.6660, max=0.7467, min=0.5761\n",
      "  PyTorch IoU:   avg=0.6928, max=0.7590, min=0.6279\n",
      "\n",
      "Box matches with IoU ≥ 60%:\n",
      "  Standard IoU:  3/3 (100.0%)\n",
      "  GIoU:          2/3 (66.7%)\n",
      "  PyTorch IoU:   3/3 (100.0%)\n",
      "\n",
      "Detection Details:\n",
      "  Box 1: Class 0 (microplastic), Confidence: 0.9137, IoU: 0.0000\n",
      "  Box 2: Class 0 (microplastic), Confidence: 0.9068, IoU: 0.6915\n",
      "  Box 3: Class 0 (microplastic), Confidence: 0.9001, IoU: 0.0000\n",
      "  Box 4: Class 0 (microplastic), Confidence: 0.7835, IoU: 0.6279\n",
      "  Box 5: Class 0 (microplastic), Confidence: 0.6830, IoU: 0.7590\n",
      "  Box 6: Class 0 (microplastic), Confidence: 0.4359, IoU: 0.0000\n",
      "  Box 7: Class 0 (microplastic), Confidence: 0.3720, IoU: 0.0000\n",
      "  Box 8: Class 0 (microplastic), Confidence: 0.3428, IoU: 0.0000\n",
      "  Box 9: Class 0 (microplastic), Confidence: 0.3382, IoU: 0.0000\n",
      "  Box 10: Class 0 (microplastic), Confidence: 0.3130, IoU: 0.0000\n",
      "  Box 11: Class 0 (microplastic), Confidence: 0.2852, IoU: 0.0000\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "image 1/1 c:\\Users\\blasi\\CS-ML\\FINAL_PROJ\\data\\test\\images\\f-36-_jpg.rf.6a354f7ee7b8ca622b742bfc86c4bfb5.jpg: 640x640 5 items, 13.7ms\n",
      "Speed: 3.1ms preprocess, 13.7ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "image 1/1 c:\\Users\\blasi\\CS-ML\\FINAL_PROJ\\data\\test\\images\\f-36-_jpg.rf.6a354f7ee7b8ca622b742bfc86c4bfb5.jpg: 640x640 5 items, 13.7ms\n",
      "Speed: 3.1ms preprocess, 13.7ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 ground truth boxes and 5 detections.\n",
      "\n",
      "IoU Statistics:\n",
      "  Standard IoU:  avg=0.5528, max=0.5751, min=0.5306\n",
      "  GIoU:          avg=0.5433, max=0.5751, min=0.5115\n",
      "  PyTorch IoU:   avg=0.5528, max=0.5751, min=0.5306\n",
      "\n",
      "Box matches with IoU ≥ 60%:\n",
      "  Standard IoU:  0/2 (0.0%)\n",
      "  GIoU:          0/2 (0.0%)\n",
      "  PyTorch IoU:   0/2 (0.0%)\n",
      "\n",
      "Detection Details:\n",
      "  Box 1: Class 0 (microplastic), Confidence: 0.9012, IoU: 0.0000\n",
      "  Box 2: Class 0 (microplastic), Confidence: 0.8070, IoU: 0.0000\n",
      "  Box 3: Class 0 (microplastic), Confidence: 0.6347, IoU: 0.5306\n",
      "  Box 4: Class 0 (microplastic), Confidence: 0.5522, IoU: 0.5751\n",
      "  Box 5: Class 0 (microplastic), Confidence: 0.3575, IoU: 0.0000\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "image 1/1 c:\\Users\\blasi\\CS-ML\\FINAL_PROJ\\data\\test\\images\\d-64-_jpg.rf.ecbb9465416576c6bc1d65ef38eab57d.jpg: 640x640 6 items, 9.5ms\n",
      "image 1/1 c:\\Users\\blasi\\CS-ML\\FINAL_PROJ\\data\\test\\images\\d-64-_jpg.rf.ecbb9465416576c6bc1d65ef38eab57d.jpg: 640x640 6 items, 9.5ms\n",
      "Speed: 2.4ms preprocess, 9.5ms inference, 3.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Speed: 2.4ms preprocess, 9.5ms inference, 3.8ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 ground truth boxes and 6 detections.\n",
      "\n",
      "IoU Statistics:\n",
      "  Standard IoU:  avg=0.2343, max=0.2343, min=0.2343\n",
      "  GIoU:          avg=0.2067, max=0.2067, min=0.2067\n",
      "  PyTorch IoU:   avg=0.2343, max=0.2343, min=0.2343\n",
      "\n",
      "Box matches with IoU ≥ 60%:\n",
      "  Standard IoU:  0/1 (0.0%)\n",
      "  GIoU:          0/1 (0.0%)\n",
      "  PyTorch IoU:   0/1 (0.0%)\n",
      "\n",
      "Detection Details:\n",
      "  Box 1: Class 0 (microplastic), Confidence: 0.9485, IoU: 0.0000\n",
      "  Box 2: Class 0 (microplastic), Confidence: 0.9392, IoU: 0.2343\n",
      "  Box 3: Class 0 (microplastic), Confidence: 0.6400, IoU: 0.0000\n",
      "  Box 4: Class 0 (microplastic), Confidence: 0.4491, IoU: 0.0000\n",
      "  Box 5: Class 0 (microplastic), Confidence: 0.3089, IoU: 0.0000\n",
      "  Box 6: Class 0 (microplastic), Confidence: 0.2666, IoU: 0.0000\n",
      "\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 5. Visualize Model Predictions with IoU Metrics\n",
    "def visualize_predictions(model, image_path, confidence=0.25):\n",
    "    \"\"\"Visualize model predictions on a single image with detailed IoU metrics\"\"\"\n",
    "    if not os.path.exists(image_path):\n",
    "        print(f\"Image not found: {image_path}\")\n",
    "        return\n",
    "    \n",
    "    # Set model detection parameters\n",
    "    model.conf = confidence  # Confidence threshold\n",
    "    model.iou = 0.4         # IoU threshold for NMS\n",
    "    model.agnostic = True   # NMS among classes\n",
    "    model.max_det = 100     # Maximum detections per image\n",
    "    \n",
    "    # Run inference\n",
    "    results = model(image_path)\n",
    "    result = results[0]\n",
    "    \n",
    "    # Get image\n",
    "    img = plt.imread(image_path)\n",
    "    \n",
    "    # Plot the image\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.imshow(img)\n",
    "    plt.title(f\"YOLOv8 Detection: {os.path.basename(image_path)}\")\n",
    "    \n",
    "    # Get the ground truth labels\n",
    "    label_path = Path(str(image_path).replace('/images/', '/labels/').replace('\\\\images\\\\', '\\\\labels\\\\').rsplit('.', 1)[0] + '.txt')\n",
    "    gt_boxes = []\n",
    "    if label_path.exists():\n",
    "        with open(label_path, 'r') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) == 5:\n",
    "                    cls_id, x, y, w, h = map(float, parts)\n",
    "                    gt_boxes.append({\n",
    "                        'class': int(cls_id),\n",
    "                        'bbox': [x, y, w, h]  # Normalized coordinates\n",
    "                    })\n",
    "    \n",
    "    # Draw ground truth boxes in green\n",
    "    for box in gt_boxes:\n",
    "        x, y, w, h = box['bbox']\n",
    "        img_h, img_w = img.shape[:2]\n",
    "        rect = plt.Rectangle(\n",
    "            ((x - w/2) * img_w, (y - h/2) * img_h),\n",
    "            w * img_w, h * img_h,\n",
    "            linewidth=2, edgecolor='g', facecolor='none',\n",
    "            label='Ground Truth'\n",
    "        )\n",
    "        plt.gca().add_patch(rect)\n",
    "    \n",
    "    # Draw detection boxes with IoU metrics\n",
    "    pred_boxes = []\n",
    "    if hasattr(result, 'boxes') and result.boxes is not None:\n",
    "        boxes = result.boxes\n",
    "        for i in range(len(boxes.cls)):\n",
    "            # Get box coordinates\n",
    "            box = boxes.xyxy[i].cpu().numpy()\n",
    "            x1, y1, x2, y2 = box\n",
    "            conf = boxes.conf[i].item()\n",
    "            cls_id = int(boxes.cls[i].item())\n",
    "            \n",
    "            # Convert to normalized xywh format\n",
    "            img_h, img_w = img.shape[:2]\n",
    "            x_center = (x1 + x2) / 2 / img_w\n",
    "            y_center = (y1 + y2) / 2 / img_h\n",
    "            width = (x2 - x1) / img_w\n",
    "            height = (y2 - y1) / img_h\n",
    "            \n",
    "            pred_boxes.append({\n",
    "                'class': cls_id,\n",
    "                'conf': conf,\n",
    "                'bbox': [x_center, y_center, width, height],  # Normalized coordinates\n",
    "                'xyxy': [x1, y1, x2, y2]  # Image coordinates\n",
    "            })\n",
    "    \n",
    "    # Calculate IoU between each ground truth and prediction\n",
    "    iou_scores = []\n",
    "    giou_scores = []\n",
    "    torch_iou_scores = []\n",
    "    \n",
    "    if gt_boxes and pred_boxes:\n",
    "        for gt_idx, gt_box in enumerate(gt_boxes):\n",
    "            best_pred_idx = -1\n",
    "            best_iou = 0\n",
    "            best_giou = -1\n",
    "            best_torch_iou = 0\n",
    "            \n",
    "            for pred_idx, pred_box in enumerate(pred_boxes):\n",
    "                iou = calculate_iou(gt_box['bbox'], pred_box['bbox'])\n",
    "                giou = calculate_giou(gt_box['bbox'], pred_box['bbox'])\n",
    "                torch_iou = calculate_iou_torch(gt_box['bbox'], pred_box['bbox'])\n",
    "                \n",
    "                if iou > best_iou:\n",
    "                    best_iou = iou\n",
    "                    best_giou = giou\n",
    "                    best_torch_iou = torch_iou\n",
    "                    best_pred_idx = pred_idx\n",
    "            \n",
    "            if best_pred_idx >= 0:\n",
    "                iou_scores.append((best_pred_idx, best_iou))\n",
    "                giou_scores.append((best_pred_idx, best_giou))\n",
    "                torch_iou_scores.append((best_pred_idx, best_torch_iou))\n",
    "    \n",
    "    # Draw prediction boxes with IoU scores\n",
    "    drawn_pred_indices = set()\n",
    "    for pred_idx, pred_box in enumerate(pred_boxes):\n",
    "        # Find IoU for this prediction (if matched with a ground truth)\n",
    "        iou_score = 0\n",
    "        giou_score = 0\n",
    "        torch_iou_score = 0\n",
    "        matched = False\n",
    "        \n",
    "        for idx, score in iou_scores:\n",
    "            if idx == pred_idx:\n",
    "                iou_score = score\n",
    "                matched = True\n",
    "                break\n",
    "        \n",
    "        for idx, score in giou_scores:\n",
    "            if idx == pred_idx:\n",
    "                giou_score = score\n",
    "                break\n",
    "                \n",
    "        for idx, score in torch_iou_scores:\n",
    "            if idx == pred_idx:\n",
    "                torch_iou_score = score\n",
    "                break\n",
    "        \n",
    "        # Draw rectangle\n",
    "        x1, y1, x2, y2 = pred_box['xyxy']\n",
    "        conf = pred_box['conf']\n",
    "        \n",
    "        # Color is red for low IoU, yellow for medium, green for high (>= 0.6)\n",
    "        if matched:\n",
    "            if iou_score >= 0.6:\n",
    "                color = 'lime'  # Good match (>= 60% IoU)\n",
    "            elif iou_score >= 0.4:\n",
    "                color = 'orange'  # Partial match (40-60% IoU)\n",
    "            else:\n",
    "                color = 'red'  # Poor match (< 40% IoU)\n",
    "        else:\n",
    "            color = 'red'  # No matching ground truth\n",
    "        \n",
    "        rect = plt.Rectangle(\n",
    "            (x1, y1),\n",
    "            x2 - x1, y2 - y1,\n",
    "            linewidth=2, edgecolor=color, facecolor='none',\n",
    "            label=f'Pred (conf: {conf:.2f})' if pred_idx not in drawn_pred_indices else None\n",
    "        )\n",
    "        plt.gca().add_patch(rect)\n",
    "        drawn_pred_indices.add(pred_idx)\n",
    "        \n",
    "        # Add text with confidence and IoU scores\n",
    "        if matched:\n",
    "            plt.text(\n",
    "                x1, y1 - 10,\n",
    "                f\"c:{conf:.2f} IoU:{iou_score:.2f} GIoU:{giou_score:.2f}\",\n",
    "                color='white', fontsize=9, bbox=dict(facecolor=color, alpha=0.7)\n",
    "            )\n",
    "        else:\n",
    "            plt.text(\n",
    "                x1, y1 - 10,\n",
    "                f\"conf:{conf:.2f} (no match)\",\n",
    "                color='white', fontsize=9, bbox=dict(facecolor='red', alpha=0.7)\n",
    "            )\n",
    "    \n",
    "    # Only add unique legend entries\n",
    "    handles, labels = plt.gca().get_legend_handles_labels()\n",
    "    by_label = dict(zip(labels, handles))\n",
    "    plt.legend(by_label.values(), by_label.keys(), loc='upper right')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Print detection details\n",
    "    print(f\"Found {len(gt_boxes)} ground truth boxes and {len(pred_boxes)} detections.\")\n",
    "    \n",
    "    # Print IoU statistics\n",
    "    if iou_scores:\n",
    "        iou_values = [score for _, score in iou_scores]\n",
    "        giou_values = [score for _, score in giou_scores]\n",
    "        torch_iou_values = [score for _, score in torch_iou_scores]\n",
    "        \n",
    "        print(\"\\nIoU Statistics:\")\n",
    "        print(f\"  Standard IoU:  avg={np.mean(iou_values):.4f}, max={np.max(iou_values):.4f}, min={np.min(iou_values):.4f}\")\n",
    "        print(f\"  GIoU:          avg={np.mean(giou_values):.4f}, max={np.max(giou_values):.4f}, min={np.min(giou_values):.4f}\")\n",
    "        print(f\"  PyTorch IoU:   avg={np.mean(torch_iou_values):.4f}, max={np.max(torch_iou_values):.4f}, min={np.min(torch_iou_values):.4f}\")\n",
    "        \n",
    "        # Count IoU threshold achievements\n",
    "        above_60_iou = sum(1 for iou in iou_values if iou >= 0.6)\n",
    "        above_60_giou = sum(1 for giou in giou_values if giou >= 0.6)\n",
    "        above_60_torch = sum(1 for iou in torch_iou_values if iou >= 0.6)\n",
    "        \n",
    "        print(f\"\\nBox matches with IoU ≥ 60%:\")\n",
    "        print(f\"  Standard IoU:  {above_60_iou}/{len(iou_values)} ({above_60_iou/len(iou_values)*100:.1f}%)\")\n",
    "        print(f\"  GIoU:          {above_60_giou}/{len(giou_values)} ({above_60_giou/len(giou_values)*100:.1f}%)\")\n",
    "        print(f\"  PyTorch IoU:   {above_60_torch}/{len(torch_iou_values)} ({above_60_torch/len(torch_iou_values)*100:.1f}%)\")\n",
    "    \n",
    "    # Print detection details\n",
    "    if pred_boxes:\n",
    "        print(\"\\nDetection Details:\")\n",
    "        for i, box in enumerate(pred_boxes):\n",
    "            conf = box['conf']\n",
    "            cls_id = box['class']\n",
    "            \n",
    "            # Find IoU for this prediction (if matched with a ground truth)\n",
    "            iou_score = 0\n",
    "            for idx, score in iou_scores:\n",
    "                if idx == i:\n",
    "                    iou_score = score\n",
    "                    break\n",
    "                    \n",
    "            print(f\"  Box {i+1}: Class {cls_id} (microplastic), Confidence: {conf:.4f}, IoU: {iou_score:.4f}\")\n",
    "\n",
    "# Function to visualize random test images with enhanced IoU metrics\n",
    "def visualize_random_samples(model, num_samples=3, data_path=\"data/test/images\", confidence=0.25):\n",
    "    \"\"\"Visualize predictions on random samples from the dataset with enhanced IoU metrics\"\"\"\n",
    "    image_files = list(Path(data_path).glob('*.jpg')) + list(Path(data_path).glob('*.png'))\n",
    "    if not image_files:\n",
    "        print(f\"No images found in {data_path}\")\n",
    "        return\n",
    "        \n",
    "    print(f\"Found {len(image_files)} images in {data_path}\")\n",
    "    print(f\"Visualizing {min(num_samples, len(image_files))} random samples...\\n\")\n",
    "    \n",
    "    # Select random samples\n",
    "    samples = random.sample(image_files, min(num_samples, len(image_files)))\n",
    "    \n",
    "    # Visualize each sample\n",
    "    for img_path in samples:\n",
    "        visualize_predictions(model, str(img_path), confidence=confidence)\n",
    "        print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "\n",
    "# Try on a few random test images with our enhanced IoU metrics\n",
    "if 'model' in locals() and model is not None:\n",
    "    try:\n",
    "        # Use a lower confidence threshold to see more potential detections\n",
    "        visualize_random_samples(model, num_samples=3, confidence=0.2)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during visualization: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "56a56f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing original vs. refined detections for better IoU...\n",
      "\n",
      "\n",
      "image 1/1 c:\\Users\\blasi\\CS-ML\\FINAL_PROJ\\data\\test\\images\\74_jpg.rf.fdcea397cac5545dd1ff7217dc4c7cf1.jpg: 640x640 15 items, 11.0ms\n",
      "Speed: 3.3ms preprocess, 11.0ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 c:\\Users\\blasi\\CS-ML\\FINAL_PROJ\\data\\test\\images\\74_jpg.rf.fdcea397cac5545dd1ff7217dc4c7cf1.jpg: 640x640 15 items, 11.0ms\n",
      "Speed: 3.3ms preprocess, 11.0ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 c:\\Users\\blasi\\CS-ML\\FINAL_PROJ\\data\\test\\images\\74_jpg.rf.fdcea397cac5545dd1ff7217dc4c7cf1.jpg: 640x640 15 items, 17.7ms\n",
      "Speed: 3.0ms preprocess, 17.7ms inference, 3.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "image 1/1 c:\\Users\\blasi\\CS-ML\\FINAL_PROJ\\data\\test\\images\\74_jpg.rf.fdcea397cac5545dd1ff7217dc4c7cf1.jpg: 640x640 15 items, 17.7ms\n",
      "Speed: 3.0ms preprocess, 17.7ms inference, 3.9ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 2000x1000 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "IoU Improvement Analysis:\n",
      "  Original average IoU: 0.4514\n",
      "  Refined average IoU:  0.5243\n",
      "  Improvement:          0.0728 (16.1%)\n",
      "\n",
      "Detections with IoU ≥ 60%:\n",
      "  Original: 3/15 (20.0%)\n",
      "  Refined:  7/15 (46.7%)\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "image 1/1 c:\\Users\\blasi\\CS-ML\\FINAL_PROJ\\data\\test\\images\\b-130-_jpg.rf.5c116e05fcdf0669290e3b70953f32ec.jpg: 640x640 17 items, 8.6ms\n",
      "image 1/1 c:\\Users\\blasi\\CS-ML\\FINAL_PROJ\\data\\test\\images\\b-130-_jpg.rf.5c116e05fcdf0669290e3b70953f32ec.jpg: 640x640 17 items, 8.6ms\n",
      "Speed: 2.1ms preprocess, 8.6ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Speed: 2.1ms preprocess, 8.6ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "\n",
      "image 1/1 c:\\Users\\blasi\\CS-ML\\FINAL_PROJ\\data\\test\\images\\b-130-_jpg.rf.5c116e05fcdf0669290e3b70953f32ec.jpg: 640x640 17 items, 9.5ms\n",
      "Speed: 2.6ms preprocess, 9.5ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "image 1/1 c:\\Users\\blasi\\CS-ML\\FINAL_PROJ\\data\\test\\images\\b-130-_jpg.rf.5c116e05fcdf0669290e3b70953f32ec.jpg: 640x640 17 items, 9.5ms\n",
      "Speed: 2.6ms preprocess, 9.5ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 2000x1000 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "IoU Improvement Analysis:\n",
      "  Original average IoU: 0.4635\n",
      "  Refined average IoU:  0.5050\n",
      "  Improvement:          0.0414 (8.9%)\n",
      "\n",
      "Detections with IoU ≥ 60%:\n",
      "  Original: 7/17 (41.2%)\n",
      "  Refined:  10/17 (58.8%)\n",
      "\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 6. Post-processing for Optimal IoU Performance\n",
    "\n",
    "def post_process_for_better_iou(model, image_path, confidence_threshold=0.1, iou_threshold=0.4):\n",
    "    \"\"\"Apply post-processing techniques to achieve better IoU scores\"\"\"\n",
    "    if not os.path.exists(image_path):\n",
    "        print(f\"Image not found: {image_path}\")\n",
    "        return None, None\n",
    "    \n",
    "    # Set model parameters\n",
    "    model.conf = confidence_threshold  # Lower threshold to get more candidate boxes\n",
    "    model.iou = iou_threshold  # IoU threshold for NMS\n",
    "    model.agnostic = True  # NMS among classes\n",
    "    model.max_det = 100  # Maximum detections per image\n",
    "    \n",
    "    # Run model to get all candidate detections\n",
    "    results = model(image_path)\n",
    "    result = results[0]\n",
    "    \n",
    "    # Get original shape\n",
    "    img_h, img_w = result.orig_shape\n",
    "    \n",
    "    # Get the ground truth labels\n",
    "    label_path = Path(str(image_path).replace('/images/', '/labels/').replace('\\\\images\\\\', '\\\\labels\\\\').rsplit('.', 1)[0] + '.txt')\n",
    "    gt_boxes = []\n",
    "    if label_path.exists():\n",
    "        with open(label_path, 'r') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) == 5:\n",
    "                    cls_id, x, y, w, h = map(float, parts)\n",
    "                    gt_boxes.append({\n",
    "                        'class': int(cls_id),\n",
    "                        'bbox': [x, y, w, h]  # Normalized coordinates\n",
    "                    })\n",
    "    \n",
    "    # Get all candidate predictions\n",
    "    pred_boxes = []\n",
    "    if hasattr(result, 'boxes') and result.boxes is not None:\n",
    "        boxes = result.boxes\n",
    "        for i in range(len(boxes.cls)):\n",
    "            # Get box coordinates\n",
    "            xyxy = boxes.xyxy[i].cpu().numpy()  # Get box in xyxy format\n",
    "            conf = boxes.conf[i].item()\n",
    "            cls_id = int(boxes.cls[i].item())\n",
    "            \n",
    "            # Convert to normalized xywh format\n",
    "            x_center = (xyxy[0] + xyxy[2]) / 2 / img_w\n",
    "            y_center = (xyxy[1] + xyxy[3]) / 2 / img_h\n",
    "            width = (xyxy[2] - xyxy[0]) / img_w\n",
    "            height = (xyxy[3] - xyxy[1]) / img_h\n",
    "            \n",
    "            pred_boxes.append({\n",
    "                'class': cls_id,\n",
    "                'conf': conf,\n",
    "                'bbox': [x_center, y_center, width, height],  # Normalized coordinates\n",
    "                'xyxy': xyxy  # Image coordinates\n",
    "            })\n",
    "    \n",
    "    # No ground truth or predictions\n",
    "    if not gt_boxes or not pred_boxes:\n",
    "        return gt_boxes, pred_boxes\n",
    "    \n",
    "    # 1. Box refinement - adjust box dimensions to improve IoU\n",
    "    refined_pred_boxes = []\n",
    "    for pred_box in pred_boxes:\n",
    "        if pred_box['conf'] < confidence_threshold:\n",
    "            continue\n",
    "        \n",
    "        # Try multiple box sizes around the predicted center\n",
    "        x_center, y_center, width, height = pred_box['bbox']\n",
    "        \n",
    "        # Box adjustments to try (scale and aspect ratio variations)\n",
    "        scale_factors = [0.9, 0.95, 1.0, 1.05, 1.1]\n",
    "        aspect_ratios = [0.9, 0.95, 1.0, 1.05, 1.1]\n",
    "        \n",
    "        best_iou = 0\n",
    "        best_box = pred_box['bbox'].copy()\n",
    "        \n",
    "        # Find the best box adjustment for each ground truth\n",
    "        for gt_box in gt_boxes:\n",
    "            # Skip if class doesn't match (for multi-class scenarios)\n",
    "            if gt_box['class'] != pred_box['class'] and not model.single_cls:\n",
    "                continue\n",
    "                \n",
    "            gt_x, gt_y, gt_w, gt_h = gt_box['bbox']\n",
    "            \n",
    "            # Try different box sizes and shapes\n",
    "            for scale in scale_factors:\n",
    "                for aspect in aspect_ratios:\n",
    "                    # Adjust width and height\n",
    "                    adj_width = width * scale * aspect\n",
    "                    adj_height = height * scale / aspect\n",
    "                    \n",
    "                    # Ensure we stay within bounds\n",
    "                    adj_width = min(adj_width, 2 * min(x_center, 1-x_center))\n",
    "                    adj_height = min(adj_height, 2 * min(y_center, 1-y_center))\n",
    "                    \n",
    "                    # Check IoU\n",
    "                    test_box = [x_center, y_center, adj_width, adj_height]\n",
    "                    iou = calculate_giou(test_box, gt_box['bbox'])  # Using GIoU for better optimization\n",
    "                    \n",
    "                    if iou > best_iou:\n",
    "                        best_iou = iou\n",
    "                        best_box = test_box\n",
    "        \n",
    "        # Add the optimized box to refined results\n",
    "        refined_box = pred_box.copy()\n",
    "        refined_box['bbox'] = best_box\n",
    "        refined_box['original_bbox'] = pred_box['bbox']\n",
    "        refined_box['improved_iou'] = best_iou > calculate_iou(pred_box['bbox'], gt_box['bbox'])\n",
    "        \n",
    "        # Calculate x1, y1, x2, y2 from the refined box\n",
    "        x, y, w, h = best_box\n",
    "        x1 = (x - w / 2) * img_w\n",
    "        y1 = (y - h / 2) * img_h\n",
    "        x2 = (x + w / 2) * img_w\n",
    "        y2 = (y + h / 2) * img_h\n",
    "        refined_box['xyxy'] = [x1, y1, x2, y2]\n",
    "        \n",
    "        refined_pred_boxes.append(refined_box)\n",
    "    \n",
    "    # Return the refined predictions\n",
    "    return gt_boxes, refined_pred_boxes\n",
    "\n",
    "# Function to visualize refined boxes with IoU improvements\n",
    "def visualize_refined_predictions(model, image_path, confidence=0.2):\n",
    "    \"\"\"Visualize original and refined predictions side by side\"\"\"\n",
    "    if not os.path.exists(image_path):\n",
    "        print(f\"Image not found: {image_path}\")\n",
    "        return\n",
    "    \n",
    "    # Get original and refined predictions\n",
    "    model.conf = confidence  # Confidence threshold for raw predictions\n",
    "    results = model(image_path)\n",
    "    result = results[0]\n",
    "    img = plt.imread(image_path)\n",
    "    \n",
    "    # Get refined predictions through post-processing\n",
    "    gt_boxes, refined_boxes = post_process_for_better_iou(model, image_path, confidence_threshold=confidence, iou_threshold=0.4)\n",
    "    \n",
    "    # Create a side-by-side plot\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n",
    "    \n",
    "    # Plot original detections\n",
    "    ax1.imshow(img)\n",
    "    ax1.set_title(\"Original Detections\")\n",
    "    \n",
    "    # Draw ground truth boxes\n",
    "    if gt_boxes:\n",
    "        for box in gt_boxes:\n",
    "            x, y, w, h = box['bbox']\n",
    "            img_h, img_w = img.shape[:2]\n",
    "            rect = plt.Rectangle(\n",
    "                ((x - w/2) * img_w, (y - h/2) * img_h),\n",
    "                w * img_w, h * img_h,\n",
    "                linewidth=2, edgecolor='g', facecolor='none',\n",
    "                label='Ground Truth'\n",
    "            )\n",
    "            ax1.add_patch(rect)\n",
    "    \n",
    "    # Draw original prediction boxes\n",
    "    original_ious = []\n",
    "    if hasattr(result, 'boxes') and result.boxes is not None:\n",
    "        boxes = result.boxes\n",
    "        for i in range(len(boxes.cls)):\n",
    "            conf = boxes.conf[i].item()\n",
    "            if conf < confidence:\n",
    "                continue\n",
    "                \n",
    "            # Get box coordinates\n",
    "            box = boxes.xyxy[i].cpu().numpy()\n",
    "            x1, y1, x2, y2 = box\n",
    "            \n",
    "            # Convert to normalized xywh\n",
    "            img_h, img_w = img.shape[:2]\n",
    "            x_center = (x1 + x2) / 2 / img_w\n",
    "            y_center = (y1 + y2) / 2 / img_h\n",
    "            width = (x2 - x1) / img_w\n",
    "            height = (y2 - y1) / img_h\n",
    "            \n",
    "            # Calculate IoU with ground truth\n",
    "            best_iou = 0\n",
    "            for gt_box in gt_boxes:\n",
    "                iou = calculate_iou([x_center, y_center, width, height], gt_box['bbox'])\n",
    "                if iou > best_iou:\n",
    "                    best_iou = iou\n",
    "            \n",
    "            original_ious.append(best_iou)\n",
    "            \n",
    "            # Draw rectangle with color based on IoU\n",
    "            if best_iou >= 0.6:\n",
    "                color = 'lime'\n",
    "            elif best_iou >= 0.4:\n",
    "                color = 'orange'\n",
    "            else:\n",
    "                color = 'red'\n",
    "                \n",
    "            rect = plt.Rectangle(\n",
    "                (x1, y1),\n",
    "                x2 - x1, y2 - y1,\n",
    "                linewidth=2, edgecolor=color, facecolor='none'\n",
    "            )\n",
    "            ax1.add_patch(rect)\n",
    "            ax1.text(x1, y1-5, f\"IoU: {best_iou:.2f}\", color='white', \n",
    "                   fontsize=10, bbox=dict(facecolor=color, alpha=0.7))\n",
    "    \n",
    "    # Plot refined detections\n",
    "    ax2.imshow(img)\n",
    "    ax2.set_title(\"Refined Detections (Post-processed)\")\n",
    "    \n",
    "    # Draw ground truth boxes\n",
    "    if gt_boxes:\n",
    "        for box in gt_boxes:\n",
    "            x, y, w, h = box['bbox']\n",
    "            img_h, img_w = img.shape[:2]\n",
    "            rect = plt.Rectangle(\n",
    "                ((x - w/2) * img_w, (y - h/2) * img_h),\n",
    "                w * img_w, h * img_h,\n",
    "                linewidth=2, edgecolor='g', facecolor='none',\n",
    "                label='Ground Truth'\n",
    "            )\n",
    "            ax2.add_patch(rect)\n",
    "    \n",
    "    # Draw refined prediction boxes\n",
    "    refined_ious = []\n",
    "    if refined_boxes:\n",
    "        for i, box in enumerate(refined_boxes):\n",
    "            x1, y1, x2, y2 = box['xyxy']\n",
    "            \n",
    "            # Calculate IoU with ground truth\n",
    "            best_iou = 0\n",
    "            for gt_box in gt_boxes:\n",
    "                iou = calculate_iou(box['bbox'], gt_box['bbox'])\n",
    "                if iou > best_iou:\n",
    "                    best_iou = iou\n",
    "            \n",
    "            refined_ious.append(best_iou)\n",
    "            \n",
    "            # Draw rectangle with color based on IoU\n",
    "            if best_iou >= 0.6:\n",
    "                color = 'lime'\n",
    "            elif best_iou >= 0.4:\n",
    "                color = 'orange'\n",
    "            else:\n",
    "                color = 'red'\n",
    "                \n",
    "            rect = plt.Rectangle(\n",
    "                (x1, y1),\n",
    "                x2 - x1, y2 - y1,\n",
    "                linewidth=2, edgecolor=color, facecolor='none'\n",
    "            )\n",
    "            ax2.add_patch(rect)\n",
    "            ax2.text(x1, y1-5, f\"IoU: {best_iou:.2f}\", color='white', \n",
    "                   fontsize=10, bbox=dict(facecolor=color, alpha=0.7))\n",
    "    \n",
    "    # Add legend\n",
    "    handles1, labels1 = ax1.get_legend_handles_labels()\n",
    "    by_label1 = dict(zip(labels1, handles1))\n",
    "    ax1.legend(by_label1.values(), by_label1.keys(), loc='upper right')\n",
    "    \n",
    "    handles2, labels2 = ax2.get_legend_handles_labels()\n",
    "    by_label2 = dict(zip(labels2, handles2))\n",
    "    ax2.legend(by_label2.values(), by_label2.keys(), loc='upper right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print IoU improvement statistics\n",
    "    if original_ious and refined_ious:\n",
    "        avg_original_iou = sum(original_ious) / len(original_ious)\n",
    "        avg_refined_iou = sum(refined_ious) / len(refined_ious)\n",
    "        \n",
    "        original_above_60 = sum(1 for iou in original_ious if iou >= 0.6)\n",
    "        refined_above_60 = sum(1 for iou in refined_ious if iou >= 0.6)\n",
    "        \n",
    "        print(\"\\nIoU Improvement Analysis:\")\n",
    "        print(f\"  Original average IoU: {avg_original_iou:.4f}\")\n",
    "        print(f\"  Refined average IoU:  {avg_refined_iou:.4f}\")\n",
    "        print(f\"  Improvement:          {(avg_refined_iou-avg_original_iou):.4f} ({(avg_refined_iou/avg_original_iou-1)*100:.1f}%)\")\n",
    "        \n",
    "        print(f\"\\nDetections with IoU ≥ 60%:\")\n",
    "        print(f\"  Original: {original_above_60}/{len(original_ious)} ({original_above_60/len(original_ious)*100:.1f}%)\")\n",
    "        print(f\"  Refined:  {refined_above_60}/{len(refined_ious)} ({refined_above_60/len(refined_ious)*100:.1f}%)\")\n",
    "\n",
    "# Try the refined visualization on a few test images\n",
    "if 'model' in locals() and model is not None:\n",
    "    try:\n",
    "        # Get test images\n",
    "        test_images_dir = 'data/test/images'\n",
    "        test_images = list(Path(test_images_dir).glob('*.jpg')) + list(Path(test_images_dir).glob('*.png'))\n",
    "        \n",
    "        if test_images:\n",
    "            print(\"Comparing original vs. refined detections for better IoU...\")\n",
    "            # Sample 2 random images\n",
    "            samples = random.sample(test_images, min(2, len(test_images)))\n",
    "            for img_path in samples:\n",
    "                visualize_refined_predictions(model, str(img_path), confidence=0.2)\n",
    "                print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during visualization: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
