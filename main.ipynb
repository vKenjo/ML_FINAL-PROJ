{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49273b9b",
   "metadata": {},
   "source": [
    "# YOLOv8 Microplastics Detection\n",
    "\n",
    "This notebook will guide you through training a YOLOv8 object detection model to detect microplastics using your dataset.\n",
    "\n",
    "> **Troubleshooting DataLoader Errors**: If you encounter `DataLoader worker exited unexpectedly` errors, this notebook has been updated to fix these issues by:\n",
    "> 1. Setting workers=0 to avoid multiprocessing issues\n",
    "> 2. Reducing batch size to prevent memory overflows\n",
    "> 3. Using CPU instead of GPU for more stable processing\n",
    "> 4. Disabling caching to prevent file access conflicts\n",
    "\n",
    "> **Important Update**: Your dataset is in detection format (bounding boxes), not segmentation format. The notebook has been updated to use YOLOv8 detection instead of segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a783f3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ultralytics in c:\\users\\blasi\\anaconda3\\lib\\site-packages (8.3.133)\n",
      "Requirement already satisfied: numpy>=1.23.0 in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from ultralytics) (1.26.4)\n",
      "Requirement already satisfied: matplotlib>=3.3.0 in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from ultralytics) (3.10.3)\n",
      "Requirement already satisfied: opencv-python>=4.6.0 in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from ultralytics) (4.11.0.86)\n",
      "Requirement already satisfied: pillow>=7.1.2 in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from ultralytics) (10.2.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from ultralytics) (6.0.2)\n",
      "Requirement already satisfied: requests>=2.23.0 in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from ultralytics) (2.32.3)\n",
      "Requirement already satisfied: scipy>=1.4.1 in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from ultralytics) (1.11.4)\n",
      "Requirement already satisfied: torch>=1.8.0 in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from ultralytics) (2.5.1+cu121)\n",
      "Requirement already satisfied: torchvision>=0.9.0 in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from ultralytics) (0.20.1+cu121)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from ultralytics) (4.65.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from ultralytics) (5.9.0)\n",
      "Requirement already satisfied: py-cpuinfo in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from ultralytics) (9.0.0)\n",
      "Requirement already satisfied: pandas>=1.1.4 in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from ultralytics) (2.2.3)\n",
      "Requirement already satisfied: seaborn>=0.11.0 in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from ultralytics) (0.13.2)\n",
      "Requirement already satisfied: ultralytics-thop>=2.0.0 in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from ultralytics) (2.0.14)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (24.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from pandas>=1.1.4->ultralytics) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from pandas>=1.1.4->ultralytics) (2023.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from requests>=2.23.0->ultralytics) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from requests>=2.23.0->ultralytics) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from requests>=2.23.0->ultralytics) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from requests>=2.23.0->ultralytics) (2024.8.30)\n",
      "Requirement already satisfied: filelock in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from torch>=1.8.0->ultralytics) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from torch>=1.8.0->ultralytics) (2023.10.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from tqdm>=4.64.0->ultralytics) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# 1. Install Required Libraries\n",
    "%pip install ultralytics\n",
    "\n",
    "# The following line will download the YOLOv8 detection model if it doesn't exist\n",
    "# Uncomment if you need to download it\n",
    "# !python -c \"from ultralytics import YOLO; YOLO('yolov8n.pt')\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337cec39",
   "metadata": {},
   "source": [
    "## 2. Dataset Structure and Format\n",
    "\n",
    "Your dataset should be structured as:\n",
    "- data/train/images/, data/train/labels/\n",
    "- data/valid/images/, data/valid/labels/\n",
    "- data/test/images/, data/test/labels/\n",
    "\n",
    "### Label Format for Object Detection\n",
    "YOLOv8 detection labels contain normalized bounding box coordinates for each object:\n",
    "```\n",
    "<class-id> <x_center> <y_center> <width> <height>\n",
    "```\n",
    "Where:\n",
    "- `class-id`: The object class (0 for microplastics)\n",
    "- `x_center, y_center`: Normalized center coordinates of the bounding box (0-1)\n",
    "- `width, height`: Normalized width and height of the bounding box (0-1)\n",
    "\n",
    "Each object in an image has its own line in the label file. The dataset already contains these labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616bbf68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating and fixing data.yaml...\n",
      "data.yaml is already correctly configured.\n",
      "\n",
      "Validating absolute paths:\n",
      "Configured path: c:/Users/blasi/CS-ML/FINAL_PROJ\n",
      "\n",
      "Primary configurations:\n",
      "- Checking Train path: c:/Users/blasi/CS-ML/FINAL_PROJ/data/train/images\n",
      "  ✓ Path exists! Found 3226 images.\n",
      "  ✓ Found 3226 labels.\n",
      "- Checking Validation path: c:/Users/blasi/CS-ML/FINAL_PROJ/data/valid/images\n",
      "  ✓ Path exists! Found 928 images.\n",
      "  ✓ Found 928 labels.\n",
      "- Checking Test path: c:/Users/blasi/CS-ML/FINAL_PROJ/data/test/images\n",
      "  ✓ Path exists! Found 453 images.\n",
      "  ✓ Found 453 labels.\n",
      "\n",
      "YOLOv8 settings found at: C:\\Users\\blasi\\AppData\\Roaming\\Ultralytics\\settings.json\n",
      "YOLOv8 datasets_dir: C:\\Users\\blasi\\CS-ML\\FINAL_PROJ\\datasets\n",
      "CUDA available: NVIDIA GeForce RTX 3050 Ti Laptop GPU\n",
      "Verifying dataset paths...\n",
      "Base path: c:\\Users\\blasi\\CS-ML\\FINAL_PROJ\n",
      "Train path: c:\\Users\\blasi\\CS-ML\\FINAL_PROJ\\data\\train\\images\n",
      "  Found 3226 images in train folder\n",
      "Val path: c:\\Users\\blasi\\CS-ML\\FINAL_PROJ\\data\\valid\\images\n",
      "  Found 928 images in val folder\n",
      "Test path: c:\\Users\\blasi\\CS-ML\\FINAL_PROJ\\data\\test\\images\n",
      "  Found 453 images in test folder\n",
      "  ✓ Path exists! Found 453 images.\n",
      "  ✓ Found 453 labels.\n",
      "\n",
      "YOLOv8 settings found at: C:\\Users\\blasi\\AppData\\Roaming\\Ultralytics\\settings.json\n",
      "YOLOv8 datasets_dir: C:\\Users\\blasi\\CS-ML\\FINAL_PROJ\\datasets\n",
      "CUDA available: NVIDIA GeForce RTX 3050 Ti Laptop GPU\n",
      "Verifying dataset paths...\n",
      "Base path: c:\\Users\\blasi\\CS-ML\\FINAL_PROJ\n",
      "Train path: c:\\Users\\blasi\\CS-ML\\FINAL_PROJ\\data\\train\\images\n",
      "  Found 3226 images in train folder\n",
      "Val path: c:\\Users\\blasi\\CS-ML\\FINAL_PROJ\\data\\valid\\images\n",
      "  Found 928 images in val folder\n",
      "Test path: c:\\Users\\blasi\\CS-ML\\FINAL_PROJ\\data\\test\\images\n",
      "  Found 453 images in test folder\n",
      "Starting training...\n",
      "Temporary data.yaml created with absolute paths:\n",
      "- path: c:/Users/blasi/CS-ML/FINAL_PROJ\n",
      "- train: data/train/images\n",
      "- val: data/valid/images\n",
      "Full train path: c:/Users/blasi/CS-ML/FINAL_PROJ\\data/train/images\n",
      "Full val path: c:/Users/blasi/CS-ML/FINAL_PROJ\\data/valid/images\n",
      "Starting training...\n",
      "Temporary data.yaml created with absolute paths:\n",
      "- path: c:/Users/blasi/CS-ML/FINAL_PROJ\n",
      "- train: data/train/images\n",
      "- val: data/valid/images\n",
      "Full train path: c:/Users/blasi/CS-ML/FINAL_PROJ\\data/train/images\n",
      "Full val path: c:/Users/blasi/CS-ML/FINAL_PROJ\\data/valid/images\n",
      "New https://pypi.org/project/ultralytics/8.3.134 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.133  Python-3.11.7 torch-2.5.1+cu121 CPU (AMD Ryzen 7 6800H with Radeon Graphics)\n",
      "New https://pypi.org/project/ultralytics/8.3.134 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.133  Python-3.11.7 torch-2.5.1+cu121 CPU (AMD Ryzen 7 6800H with Radeon Graphics)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0magnostic_nms=False, amp=False, augment=False, auto_augment=randaugment, batch=8, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=temp_data.yaml, degrees=0.0, deterministic=True, device=cpu, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=1, erasing=0.4, exist_ok=True, fliplr=0.5, flipud=0.5, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=train, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=50, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=runs/detect, rect=True, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=runs\\detect\\train, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=True, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=0, workspace=None\n",
      "Overriding class names with single class.\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0magnostic_nms=False, amp=False, augment=False, auto_augment=randaugment, batch=8, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=temp_data.yaml, degrees=0.0, deterministic=True, device=cpu, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=1, erasing=0.4, exist_ok=True, fliplr=0.5, flipud=0.5, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=train, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=50, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=runs/detect, rect=True, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=runs\\detect\\train, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=True, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=0, workspace=None\n",
      "Overriding class names with single class.\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    751507  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    751507  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \n",
      "Model summary: 129 layers, 3,011,043 parameters, 3,011,027 gradients, 8.2 GFLOPs\n",
      "\n",
      "Model summary: 129 layers, 3,011,043 parameters, 3,011,027 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "Transferred 319/355 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 309.662.7 MB/s, size: 34.1 KB)\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 309.662.7 MB/s, size: 34.1 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\blasi\\CS-ML\\FINAL_PROJ\\data\\train\\labels.cache... 3226 images, 0 backgrounds, 0 corrupt: 100%|██████████| 3226/3226 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 'rect=True' is incompatible with DataLoader shuffle, setting shuffle=False\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 366.863.7 MB/s, size: 32.7 KB)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 366.863.7 MB/s, size: 32.7 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\blasi\\CS-ML\\FINAL_PROJ\\data\\valid\\labels.cache... 928 images, 0 backgrounds, 0 corrupt: 100%|██████████| 928/928 [00:00<?, ?it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs\\detect\\train\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns\\detect\\train\u001b[0m\n",
      "Starting training for 1 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns\\detect\\train\u001b[0m\n",
      "Starting training for 1 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/1         0G      1.891      2.166      1.429          8        640: 100%|██████████| 404/404 [10:38<00:00,  1.58s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):   0%|          | 0/58 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 58/58 [01:25<00:00,  1.47s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        928       6475      0.719      0.596      0.654      0.297\n",
      "\n",
      "1 epochs completed in 0.202 hours.\n",
      "\n",
      "1 epochs completed in 0.202 hours.\n",
      "Optimizer stripped from runs\\detect\\train\\weights\\last.pt, 6.2MB\n",
      "Optimizer stripped from runs\\detect\\train\\weights\\last.pt, 6.2MB\n",
      "Optimizer stripped from runs\\detect\\train\\weights\\best.pt, 6.2MB\n",
      "\n",
      "Validating runs\\detect\\train\\weights\\best.pt...\n",
      "Ultralytics 8.3.133  Python-3.11.7 torch-2.5.1+cu121 CPU (AMD Ryzen 7 6800H with Radeon Graphics)\n",
      "Optimizer stripped from runs\\detect\\train\\weights\\best.pt, 6.2MB\n",
      "\n",
      "Validating runs\\detect\\train\\weights\\best.pt...\n",
      "Ultralytics 8.3.133  Python-3.11.7 torch-2.5.1+cu121 CPU (AMD Ryzen 7 6800H with Radeon Graphics)\n",
      "Model summary (fused): 72 layers, 3,005,843 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\n",
      "Model summary (fused): 72 layers, 3,005,843 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\n",
      "ERROR during training: Dataset 'temp_data.yaml' images not found, missing path 'C:\\Users\\blasi\\CS-ML\\FINAL_PROJ\\datasets\\data\\valid\\images'\n",
      "Note dataset download directory is 'C:\\Users\\blasi\\CS-ML\\FINAL_PROJ\\datasets'. You can update this in 'C:\\Users\\blasi\\AppData\\Roaming\\Ultralytics\\settings.json'\n",
      "\n",
      "TROUBLESHOOTING TIPS:\n",
      "1. Check that your data directories exist and contain images/labels\n",
      "2. If you have a 'datasets' directory, it might be conflicting with YOLOv8's path resolution\n",
      "3. Try running with absolute paths specified in data.yaml\n",
      "4. Check your Ultralytics settings.json file for a custom datasets_dir setting\n",
      "ERROR during training: Dataset 'temp_data.yaml' images not found, missing path 'C:\\Users\\blasi\\CS-ML\\FINAL_PROJ\\datasets\\data\\valid\\images'\n",
      "Note dataset download directory is 'C:\\Users\\blasi\\CS-ML\\FINAL_PROJ\\datasets'. You can update this in 'C:\\Users\\blasi\\AppData\\Roaming\\Ultralytics\\settings.json'\n",
      "\n",
      "TROUBLESHOOTING TIPS:\n",
      "1. Check that your data directories exist and contain images/labels\n",
      "2. If you have a 'datasets' directory, it might be conflicting with YOLOv8's path resolution\n",
      "3. Try running with absolute paths specified in data.yaml\n",
      "4. Check your Ultralytics settings.json file for a custom datasets_dir setting\n"
     ]
    }
   ],
   "source": [
    "# 3. Training YOLOv8 Detection Model\n",
    "import os\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "import yaml\n",
    "\n",
    "DATASET_YAML = 'data.yaml'\n",
    "\n",
    "# 3.0 Data Configuration Validation and Repair\n",
    "def validate_and_fix_data_yaml(yaml_path):\n",
    "    \"\"\"Validate and fix data.yaml file for YOLOv8 compatibility\"\"\"\n",
    "    print(f\"Validating and fixing {yaml_path}...\")\n",
    "    try:\n",
    "        with open(yaml_path, 'r') as f:\n",
    "            data_cfg = yaml.safe_load(f)\n",
    "        \n",
    "        # Make a copy to check if changes were made\n",
    "        original_cfg = data_cfg.copy()\n",
    "        \n",
    "        # Get project root directory (absolute path)\n",
    "        project_root = os.path.abspath(os.path.dirname(yaml_path))\n",
    "        \n",
    "        # Fix fields for YOLOv8 with absolute paths\n",
    "        data_cfg[\"path\"] = project_root.replace('\\\\', '/')  # Use forward slashes for paths\n",
    "        \n",
    "        # These are relative to the path\n",
    "        data_cfg[\"train\"] = \"data/train/images\"\n",
    "        data_cfg[\"val\"] = \"data/valid/images\"\n",
    "        data_cfg[\"test\"] = \"data/test/images\"\n",
    "        data_cfg[\"nc\"] = 1\n",
    "        data_cfg[\"names\"] = [\"microplastic\"]\n",
    "        \n",
    "        # Remove problematic or redundant fields\n",
    "        keys_to_remove = [\"batch\", \"cache\", \"workers\"]\n",
    "        for key in keys_to_remove:\n",
    "            if key in data_cfg:\n",
    "                del data_cfg[key]\n",
    "        \n",
    "        # Check if changes were made\n",
    "        if data_cfg != original_cfg:\n",
    "            print(\"Changes needed in data.yaml. Updating file...\")\n",
    "            with open(yaml_path, 'w') as f:\n",
    "                yaml.dump(data_cfg, f, default_flow_style=False)\n",
    "            print(\"data.yaml has been updated with correct configuration.\")\n",
    "        else:\n",
    "            print(\"data.yaml is already correctly configured.\")\n",
    "        \n",
    "        # Validate absolute paths - important for troubleshooting\n",
    "        train_path = os.path.join(data_cfg[\"path\"], \"data/train/images\")\n",
    "        val_path = os.path.join(data_cfg[\"path\"], \"data/valid/images\")\n",
    "        test_path = os.path.join(data_cfg[\"path\"], \"data/test/images\")\n",
    "        \n",
    "        train_path_alt = os.path.join(project_root, \"data/train/images\")\n",
    "        val_path_alt = os.path.join(project_root, \"data/valid/images\")\n",
    "        test_path_alt = os.path.join(project_root, \"data/test/images\")\n",
    "        \n",
    "        print(\"\\nValidating absolute paths:\")\n",
    "        print(f\"Configured path: {data_cfg['path']}\")\n",
    "        \n",
    "        def check_path(path, name):\n",
    "            print(f\"- Checking {name} path: {path}\")\n",
    "            if os.path.exists(path):\n",
    "                images = len(list(Path(path).glob('*.jpg'))) + len(list(Path(path).glob('*.png')))\n",
    "                print(f\"  ✓ Path exists! Found {images} images.\")\n",
    "                # Check for corresponding labels\n",
    "                label_path = path.replace('images', 'labels')\n",
    "                if os.path.exists(label_path):\n",
    "                    labels = len(list(Path(label_path).glob('*.txt')))\n",
    "                    print(f\"  ✓ Found {labels} labels.\")\n",
    "                    if labels < images:\n",
    "                        print(f\"  ⚠ WARNING: {images-labels} images may be missing labels!\")\n",
    "                else:\n",
    "                    print(f\"  ✗ WARNING: Label directory {label_path} does not exist!\")\n",
    "                return images > 0\n",
    "            else:\n",
    "                print(f\"  ✗ ERROR: Directory does not exist!\")\n",
    "                return False\n",
    "        \n",
    "        # Check primary paths\n",
    "        print(\"\\nPrimary configurations:\")\n",
    "        train_ok = check_path(train_path.replace('\\\\', '/'), \"Train\")\n",
    "        val_ok = check_path(val_path.replace('\\\\', '/'), \"Validation\")\n",
    "        test_ok = check_path(test_path.replace('\\\\', '/'), \"Test\")\n",
    "        \n",
    "        # If paths are missing, check alternative paths\n",
    "        if not (train_ok and val_ok and test_ok):\n",
    "            print(\"\\nChecking alternative paths:\")\n",
    "            check_path(train_path_alt.replace('\\\\', '/'), \"Alt Train\")\n",
    "            check_path(val_path_alt.replace('\\\\', '/'), \"Alt Validation\")\n",
    "            check_path(test_path_alt.replace('\\\\', '/'), \"Alt Test\")\n",
    "        \n",
    "        # Check paths that YOLOv8 might be trying to use (for debugging)\n",
    "        datasets_path = os.path.join(project_root, \"datasets\")\n",
    "        if os.path.exists(datasets_path):\n",
    "            print(f\"\\nWARNING: A 'datasets' directory exists at {datasets_path}\")\n",
    "            print(\"This might be causing path resolution conflicts. YOLOv8 might be looking here instead of your data directory.\")\n",
    "        \n",
    "        # Check YOLOv8 settings\n",
    "        settings_path = os.path.expandvars(r\"%APPDATA%\\Ultralytics\\settings.json\")\n",
    "        if os.path.exists(settings_path):\n",
    "            print(f\"\\nYOLOv8 settings found at: {settings_path}\")\n",
    "            try:\n",
    "                import json\n",
    "                with open(settings_path, 'r') as f:\n",
    "                    settings = json.load(f)\n",
    "                if 'datasets_dir' in settings:\n",
    "                    print(f\"YOLOv8 datasets_dir: {settings['datasets_dir']}\")\n",
    "            except:\n",
    "                print(\"Could not read YOLOv8 settings file.\")\n",
    "        \n",
    "        return data_cfg\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR processing data.yaml: {e}\")\n",
    "        return None\n",
    "\n",
    "# Run validation and fix\n",
    "data_cfg = validate_and_fix_data_yaml(DATASET_YAML)\n",
    "\n",
    "def verify_dataset(yaml_path):\n",
    "    if not os.path.exists(yaml_path):\n",
    "        print(f\"ERROR: Dataset YAML file not found: {yaml_path}\")\n",
    "        return False\n",
    "    try:\n",
    "        with open(yaml_path, 'r') as f:\n",
    "            data_cfg = yaml.safe_load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Failed to load YAML: {e}\")\n",
    "        return False\n",
    "    if 'path' not in data_cfg:\n",
    "        print(\"ERROR: 'path' key missing in YAML file.\")\n",
    "        return False\n",
    "    base_path = Path(data_cfg['path'])\n",
    "    print(f\"Base path: {base_path}\")\n",
    "    all_ok = True\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        if split in data_cfg:\n",
    "            split_path = base_path / data_cfg[split]\n",
    "            print(f\"{split.capitalize()} path: {split_path}\")\n",
    "            if not split_path.exists():\n",
    "                print(f\"WARNING: {split} path does not exist: {split_path}\")\n",
    "                try:\n",
    "                    os.makedirs(split_path, exist_ok=True)\n",
    "                    print(f\"Created directory: {split_path}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"ERROR: Could not create directory {split_path}: {e}\")\n",
    "                    all_ok = False\n",
    "            else:\n",
    "                img_count = len(list(split_path.glob('*.jpg'))) + len(list(split_path.glob('*.png')))\n",
    "                print(f\"  Found {img_count} images in {split} folder\")\n",
    "                if img_count == 0:\n",
    "                    print(f\"WARNING: No images found in {split_path}\")\n",
    "    return all_ok\n",
    "\n",
    "# Device selection\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "    print(f\"CUDA available: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"No GPU available, using CPU instead.\")\n",
    "\n",
    "print(\"Verifying dataset paths...\")\n",
    "dataset_ok = verify_dataset(DATASET_YAML)\n",
    "if not dataset_ok:\n",
    "    print(\"Dataset verification failed. Please check your dataset structure and YAML file.\")\n",
    "else:\n",
    "    # Load YOLOv8 detection model\n",
    "    try:\n",
    "        model = YOLO('yolov8n.pt')\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Could not load YOLOv8 model: {e}\")\n",
    "        model = None\n",
    "    if model is not None:\n",
    "        try:\n",
    "            print(\"Starting training...\")\n",
    "            \n",
    "            # Create a copy of data.yaml with absolute paths to prevent path resolution issues\n",
    "            import shutil\n",
    "            temp_yaml = 'temp_data.yaml'\n",
    "            shutil.copy(DATASET_YAML, temp_yaml)\n",
    "            \n",
    "            # Update the temporary YAML with absolute paths\n",
    "            with open(temp_yaml, 'r') as f:\n",
    "                temp_data = yaml.safe_load(f)\n",
    "            \n",
    "            # Ensure path is absolute with forward slashes\n",
    "            project_root = os.path.abspath(os.path.dirname(DATASET_YAML))\n",
    "            temp_data['path'] = project_root.replace('\\\\', '/')\n",
    "            \n",
    "            with open(temp_yaml, 'w') as f:\n",
    "                yaml.dump(temp_data, f, default_flow_style=False)\n",
    "            \n",
    "            print(f\"Temporary data.yaml created with absolute paths:\")\n",
    "            print(f\"- path: {temp_data['path']}\")\n",
    "            print(f\"- train: {temp_data['train']}\")\n",
    "            print(f\"- val: {temp_data['val']}\")\n",
    "            \n",
    "            # Display actual paths that will be used\n",
    "            train_path = os.path.join(temp_data['path'], temp_data['train'])\n",
    "            val_path = os.path.join(temp_data['path'], temp_data['val'])\n",
    "            print(f\"Full train path: {train_path}\")\n",
    "            print(f\"Full val path: {val_path}\")\n",
    "            \n",
    "            #TODO: 1. change epoch to atleast 50 - 100,\n",
    "            #TODO: 2. change batch size to 16 or 32 (MAX)\n",
    "            #TODO: 3. change workers to 0-8 (MAX)\n",
    "            # you might face problems with various parameters, try focusing on finding the most optimal\n",
    "            # number of workers first.\n",
    "            results = model.train(\n",
    "                data=temp_yaml,  # Use the temporary YAML with absolute paths\n",
    "                epochs=1,             # Increased epochs for better learning\n",
    "                imgsz=640,             # Input image size\n",
    "                batch=8,               # Smaller batch size to prevent memory issues\n",
    "                workers=0,             # Set workers to 0 to avoid DataLoader multiprocessing issues\n",
    "                mosaic=1.0,            # Mosaic augmentation\n",
    "                scale=0.5,             # Scale augmentation\n",
    "                perspective=0.0,       # No perspective augmentation for small objects\n",
    "                flipud=0.5,            # Flip up-down augmentation\n",
    "                fliplr=0.5,            # Flip left-right augmentation\n",
    "                hsv_h=0.015,           # HSV hue augmentation (reduced for consistency)\n",
    "                hsv_s=0.7,             # HSV saturation augmentation\n",
    "                hsv_v=0.4,             # HSV value augmentation\n",
    "                patience=50,           # Early stopping patience\n",
    "                device='cpu',          # Force CPU to avoid CUDA/DataLoader issues\n",
    "                project='runs/detect', # Project directory\n",
    "                name='train',          # Run name\n",
    "                exist_ok=True,         # Overwrite existing directory\n",
    "                cache=False,           # Disable cache to prevent potential issues\n",
    "                amp=False,             # Disable mixed precision to avoid potential issues\n",
    "                single_cls=True,       # Force single class detection since we only have microplastics\n",
    "                rect=True              # Rectangular training reduces batch_size by 2, but makes training more stable\n",
    "            )\n",
    "            \n",
    "            # Clean up temporary file\n",
    "            try:\n",
    "                os.remove(temp_yaml)\n",
    "                print(f\"Temporary data file {temp_yaml} removed\")\n",
    "            except:\n",
    "                pass\n",
    "                \n",
    "            print(\"Training completed successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR during training: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3783eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 Training Visualization\n",
    "from IPython.display import display, Image\n",
    "from pathlib import Path\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Function to display training progress during or after training\n",
    "def show_training_plots():\n",
    "    results_path = Path('runs/detect/train')  # Changed from segment to detect\n",
    "    \n",
    "    # Check if the directory exists first\n",
    "    if not results_path.exists():\n",
    "        print(f\"Warning: Results directory not found at {results_path}\")\n",
    "        return\n",
    "    \n",
    "    # Results plots\n",
    "    plots = {\n",
    "        'Training Loss': results_path / 'results.png',\n",
    "        'Validation Confusion Matrix': results_path / 'val_confusion_matrix_normalized.png',\n",
    "        'PR Curve': results_path / 'PR_curve.png'\n",
    "    }\n",
    "    \n",
    "    found_plots = False\n",
    "    for title, plot_path in plots.items():\n",
    "        if plot_path.exists():\n",
    "            found_plots = True\n",
    "            print(f\"\\n{title}:\")\n",
    "            try:\n",
    "                display(Image(str(plot_path)))\n",
    "            except Exception as e:\n",
    "                print(f\"Error displaying {title}: {str(e)}\")\n",
    "        else:\n",
    "            print(f\"\\n{title} plot not found at {plot_path}\")\n",
    "    \n",
    "    if not found_plots:\n",
    "        print(\"No training plots found. Training may not have completed successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3707ac65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- LABEL SANITY CHECK ---\n",
      "Checking 5 random label files in data\\train\\labels...\n",
      "\n",
      "File: b-43-_jpg.rf.12e49b63335756b011829335dd63d188.txt\n",
      "  class: 0, x: 0.59765625, y: 0.21015625, w: 0.0328125, h: 0.0296875\n",
      "  class: 0, x: 0.83125, y: 0.2875, w: 0.05, h: 0.059375\n",
      "  class: 0, x: 0.02265625, y: 0.290625, w: 0.0453125, h: 0.071875\n",
      "  class: 0, x: 0.18515625, y: 0.290625, w: 0.0421875, h: 0.04375\n",
      "  class: 0, x: 0.91484375, y: 0.340625, w: 0.0671875, h: 0.06875\n",
      "  class: 0, x: 0.38359375, y: 0.5171875, w: 0.0671875, h: 0.053125\n",
      "  class: 0, x: 0.421875, y: 0.80234375, w: 0.0625, h: 0.0484375\n",
      "  class: 0, x: 0.203125, y: 0.8390625, w: 0.05, h: 0.05625\n",
      "  class: 0, x: 0.60703125, y: 0.86953125, w: 0.1140625, h: 0.0578125\n",
      "  class: 0, x: 0.18984375, y: 0.953125, w: 0.0703125, h: 0.090625\n",
      "\n",
      "File: a-59-_jpg.rf.20eab7733ef1c795a2ebb015b4110ee3.txt\n",
      "  class: 0, x: 0.58984375, y: 0.33125, w: 0.1859375, h: 0.184375\n",
      "  class: 0, x: 0.61484375, y: 0.609375, w: 0.1890625, h: 0.190625\n",
      "  class: 0, x: 0.10625, y: 0.63671875, w: 0.171875, h: 0.1734375\n",
      "  class: 0, x: 0.959375, y: 0.875, w: 0.08125, h: 0.215625\n",
      "\n",
      "File: f-142-_jpg.rf.ba572403f03b47fde4e338755362287e.txt\n",
      "  class: 0, x: 0.90859375, y: 0.92890625, w: 0.1828125, h: 0.1171875\n",
      "  class: 0, x: 0.06640625, y: 0.96015625, w: 0.1328125, h: 0.0796875\n",
      "  class: 0, x: 0.18125, y: 0.98203125, w: 0.0875, h: 0.0359375\n",
      "\n",
      "File: 155_jpg.rf.fa16eb81111effe21694a42ef8ceb7b5.txt\n",
      "  class: 0, x: 0.5046875, y: 0.053125, w: 0.06875, h: 0.065625\n",
      "  class: 0, x: 0.375, y: 0.0890625, w: 0.071875, h: 0.071875\n",
      "  class: 0, x: 0.45625, y: 0.11875, w: 0.059375, h: 0.03125\n",
      "  class: 0, x: 0.50625, y: 0.1203125, w: 0.05, h: 0.0625\n",
      "  class: 0, x: 0.7375, y: 0.134375, w: 0.078125, h: 0.078125\n",
      "  class: 0, x: 0.6609375, y: 0.1515625, w: 0.053125, h: 0.053125\n",
      "  class: 0, x: 0.32734375, y: 0.25859375, w: 0.0578125, h: 0.0578125\n",
      "  class: 0, x: 0.646875, y: 0.30625, w: 0.065625, h: 0.0625\n",
      "  class: 0, x: 0.69375, y: 0.30703125, w: 0.046875, h: 0.0453125\n",
      "  class: 0, x: 0.5421875, y: 0.33984375, w: 0.053125, h: 0.0546875\n",
      "  class: 0, x: 0.4328125, y: 0.3703125, w: 0.059375, h: 0.059375\n",
      "  class: 0, x: 0.634375, y: 0.55546875, w: 0.05, h: 0.0515625\n",
      "  class: 0, x: 0.796875, y: 0.559375, w: 0.05, h: 0.06875\n",
      "  class: 0, x: 0.2359375, y: 0.8359375, w: 0.115625, h: 0.08125\n",
      "  class: 0, x: 0.3765625, y: 0.975, w: 0.065625, h: 0.05\n",
      "  class: 0, x: 0.128125, y: 0.98203125, w: 0.046875, h: 0.0359375\n",
      "\n",
      "File: 82_jpg.rf.714a240939586ecc3c44826ad3a2753a.txt\n",
      "  class: 0, x: 0.0078125, y: 0.5, w: 0.015625, h: 0.04375\n",
      "  class: 0, x: 0.01796875, y: 0.42265625, w: 0.0359375, h: 0.0390625\n",
      "  class: 0, x: 0.0421875, y: 0.1390625, w: 0.059375, h: 0.0875\n",
      "  class: 0, x: 0.08515625, y: 0.46484375, w: 0.0421875, h: 0.0421875\n",
      "  class: 0, x: 0.28359375, y: 0.13671875, w: 0.0484375, h: 0.0484375\n",
      "  class: 0, x: 0.3203125, y: 0.71484375, w: 0.040625, h: 0.0421875\n",
      "  class: 0, x: 0.3453125, y: 0.59375, w: 0.040625, h: 0.040625\n",
      "  class: 0, x: 0.43828125, y: 0.63671875, w: 0.0578125, h: 0.0578125\n",
      "  class: 0, x: 0.46953125, y: 0.4953125, w: 0.0515625, h: 0.053125\n",
      "  class: 0, x: 0.4921875, y: 0.5703125, w: 0.040625, h: 0.025\n",
      "  class: 0, x: 0.5171875, y: 0.1515625, w: 0.06875, h: 0.06875\n",
      "  class: 0, x: 0.5609375, y: 0.184375, w: 0.04375, h: 0.046875\n",
      "  class: 0, x: 0.62734375, y: 0.7625, w: 0.0390625, h: 0.040625\n",
      "  class: 0, x: 0.62890625, y: 0.2546875, w: 0.0390625, h: 0.040625\n",
      "  class: 0, x: 0.65859375, y: 0.1078125, w: 0.0390625, h: 0.040625\n",
      "  class: 0, x: 0.7421875, y: 0.128125, w: 0.071875, h: 0.075\n",
      "  class: 0, x: 0.83984375, y: 0.37890625, w: 0.0609375, h: 0.0609375\n",
      "  class: 0, x: 0.8421875, y: 0.60546875, w: 0.046875, h: 0.0453125\n",
      "  class: 0, x: 0.98203125, y: 0.6796875, w: 0.0359375, h: 0.059375\n",
      "Checking 5 random label files in data\\valid\\labels...\n",
      "\n",
      "File: 37_jpg.rf.860f0affcebc0e1d92b1b8cf2b5fb4be.txt\n",
      "  class: 0, x: 0.46171875, y: 0.63828125, w: 0.0484375, h: 0.0484375\n",
      "  class: 0, x: 0.36875, y: 0.7484375, w: 0.040625, h: 0.0375\n",
      "  class: 0, x: 0.4359375, y: 0.8265625, w: 0.05625, h: 0.040625\n",
      "  class: 0, x: 0.76640625, y: 0.4890625, w: 0.0609375, h: 0.0625\n",
      "  class: 0, x: 0.55390625, y: 0.38203125, w: 0.0234375, h: 0.0234375\n",
      "  class: 0, x: 0.52578125, y: 0.31484375, w: 0.0359375, h: 0.0359375\n",
      "  class: 0, x: 0.5609375, y: 0.196875, w: 0.03125, h: 0.03125\n",
      "  class: 0, x: 0.459375, y: 0.1671875, w: 0.046875, h: 0.04375\n",
      "  class: 0, x: 0.24921875, y: 0.23359375, w: 0.0484375, h: 0.0484375\n",
      "  class: 0, x: 0.0265625, y: 0.01171875, w: 0.034375, h: 0.0234375\n",
      "  class: 0, x: 0.209375, y: 0.1359375, w: 0.053125, h: 0.071875\n",
      "  class: 0, x: 0.8890625, y: 0.85703125, w: 0.071875, h: 0.0390625\n",
      "  class: 0, x: 0.83671875, y: 0.79765625, w: 0.0515625, h: 0.0515625\n",
      "  class: 0, x: 0.9203125, y: 0.5078125, w: 0.028125, h: 0.0375\n",
      "  class: 0, x: 0.91796875, y: 0.69296875, w: 0.0296875, h: 0.0296875\n",
      "  class: 0, x: 0.7921875, y: 0.59375, w: 0.034375, h: 0.034375\n",
      "\n",
      "File: b-96-_jpg.rf.e24696c0ff7ae74b881538a4b6e40d6b.txt\n",
      "  class: 0, x: 0.18828125, y: 0.7609375, w: 0.0390625, h: 0.0375\n",
      "  class: 0, x: 0.3046875, y: 0.540625, w: 0.03125, h: 0.03125\n",
      "  class: 0, x: 0.428125, y: 0.5953125, w: 0.03125, h: 0.028125\n",
      "  class: 0, x: 0.47421875, y: 0.49296875, w: 0.0328125, h: 0.0328125\n",
      "  class: 0, x: 0.54375, y: 0.7359375, w: 0.053125, h: 0.05625\n",
      "  class: 0, x: 0.54921875, y: 0.56171875, w: 0.0421875, h: 0.0421875\n",
      "  class: 0, x: 0.55, y: 0.9796875, w: 0.06875, h: 0.040625\n",
      "  class: 0, x: 0.55234375, y: 0.03125, w: 0.0515625, h: 0.05\n",
      "  class: 0, x: 0.58359375, y: 0.65703125, w: 0.0390625, h: 0.0296875\n",
      "  class: 0, x: 0.5890625, y: 0.3640625, w: 0.046875, h: 0.046875\n",
      "  class: 0, x: 0.64765625, y: 0.5796875, w: 0.0546875, h: 0.03125\n",
      "  class: 0, x: 0.728125, y: 0.37578125, w: 0.03125, h: 0.0296875\n",
      "  class: 0, x: 0.74140625, y: 0.49375, w: 0.0265625, h: 0.025\n",
      "  class: 0, x: 0.76953125, y: 0.7578125, w: 0.0390625, h: 0.04375\n",
      "  class: 0, x: 0.89375, y: 0.1515625, w: 0.040625, h: 0.040625\n",
      "  class: 0, x: 0.9109375, y: 0.07578125, w: 0.034375, h: 0.0328125\n",
      "\n",
      "File: 180_jpg.rf.66561fbb8d98afb643b1d3c6e0411af4.txt\n",
      "  class: 0, x: 0.89140625, y: 0.20546875, w: 0.0421875, h: 0.0421875\n",
      "  class: 0, x: 0.20859375, y: 0.3546875, w: 0.0578125, h: 0.05625\n",
      "  class: 0, x: 0.9171875, y: 0.4796875, w: 0.046875, h: 0.046875\n",
      "  class: 0, x: 0.790625, y: 0.51484375, w: 0.046875, h: 0.0484375\n",
      "  class: 0, x: 0.32109375, y: 0.6484375, w: 0.0640625, h: 0.059375\n",
      "  class: 0, x: 0.76328125, y: 0.73046875, w: 0.1265625, h: 0.1265625\n",
      "  class: 0, x: 0.56015625, y: 0.73671875, w: 0.0703125, h: 0.0703125\n",
      "\n",
      "File: 54_jpg.rf.c4ba9baafad495a84f0d3eac8f9c493b.txt\n",
      "  class: 0, x: 0.1828125, y: 0.575, w: 0.078125, h: 0.078125\n",
      "  class: 0, x: 0.2703125, y: 0.609375, w: 0.059375, h: 0.046875\n",
      "  class: 0, x: 0.259375, y: 0.7, w: 0.059375, h: 0.059375\n",
      "  class: 0, x: 0.5125, y: 0.4625, w: 0.0375, h: 0.03125\n",
      "  class: 0, x: 0.0296875, y: 0.18515625, w: 0.053125, h: 0.0515625\n",
      "  class: 0, x: 0.6609375, y: 0.61640625, w: 0.05, h: 0.0515625\n",
      "  class: 0, x: 0.67734375, y: 0.590625, w: 0.0421875, h: 0.05625\n",
      "  class: 0, x: 0.75546875, y: 0.66171875, w: 0.0546875, h: 0.0546875\n",
      "  class: 0, x: 0.865625, y: 0.8265625, w: 0.084375, h: 0.05625\n",
      "  class: 0, x: 0.16953125, y: 0.84765625, w: 0.0515625, h: 0.0546875\n",
      "  class: 0, x: 0.2828125, y: 0.99296875, w: 0.025, h: 0.0140625\n",
      "  class: 0, x: 0.09375, y: 0.0546875, w: 0.04375, h: 0.04375\n",
      "  class: 0, x: 0.61484375, y: 0.196875, w: 0.0328125, h: 0.03125\n",
      "  class: 0, x: 0.34453125, y: 0.0796875, w: 0.0421875, h: 0.028125\n",
      "\n",
      "File: c-12-_jpg.rf.ae56ca4373c88863756aaabe463f060b.txt\n",
      "  class: 0, x: 0.50625, y: 0.090625, w: 0.040625, h: 0.046875\n",
      "  class: 0, x: 0.47265625, y: 0.0921875, w: 0.0296875, h: 0.028125\n",
      "  class: 0, x: 0.8296875, y: 0.103125, w: 0.04375, h: 0.03125\n",
      "  class: 0, x: 0.90703125, y: 0.196875, w: 0.0546875, h: 0.05625\n",
      "  class: 0, x: 0.87109375, y: 0.484375, w: 0.1109375, h: 0.0875\n",
      "  class: 0, x: 0.41328125, y: 0.5796875, w: 0.0453125, h: 0.0625\n",
      "  class: 0, x: 0.6375, y: 0.64296875, w: 0.065625, h: 0.0765625\n",
      "  class: 0, x: 0.02265625, y: 0.64921875, w: 0.0453125, h: 0.0984375\n",
      "Checking 5 random label files in data\\test\\labels...\n",
      "\n",
      "File: 9_jpg.rf.cb5870ee3e63c92a98f8363e00b76d1c.txt\n",
      "  class: 0, x: 0.4359375, y: 0.0078125, w: 0.06875, h: 0.015625\n",
      "  class: 0, x: 0.83203125, y: 0.03125, w: 0.0546875, h: 0.053125\n",
      "  class: 0, x: 0.42890625, y: 0.053125, w: 0.0515625, h: 0.05625\n",
      "  class: 0, x: 0.49296875, y: 0.0875, w: 0.0484375, h: 0.05\n",
      "  class: 0, x: 0.58359375, y: 0.15546875, w: 0.0484375, h: 0.0484375\n",
      "  class: 0, x: 0.40703125, y: 0.2859375, w: 0.0515625, h: 0.05\n",
      "  class: 0, x: 0.28984375, y: 0.34296875, w: 0.0421875, h: 0.0421875\n",
      "  class: 0, x: 0.6390625, y: 0.3625, w: 0.0375, h: 0.0375\n",
      "  class: 0, x: 0.584375, y: 0.37734375, w: 0.0375, h: 0.0609375\n",
      "  class: 0, x: 0.93046875, y: 0.50546875, w: 0.0359375, h: 0.0359375\n",
      "  class: 0, x: 0.28203125, y: 0.53203125, w: 0.0421875, h: 0.0421875\n",
      "  class: 0, x: 0.115625, y: 0.6, w: 0.090625, h: 0.084375\n",
      "  class: 0, x: 0.2234375, y: 0.60078125, w: 0.05625, h: 0.0578125\n",
      "  class: 0, x: 0.1984375, y: 0.6515625, w: 0.04375, h: 0.04375\n",
      "  class: 0, x: 0.903125, y: 0.73125, w: 0.08125, h: 0.084375\n",
      "  class: 0, x: 0.70078125, y: 0.7453125, w: 0.0578125, h: 0.053125\n",
      "  class: 0, x: 0.76796875, y: 0.7515625, w: 0.0453125, h: 0.046875\n",
      "  class: 0, x: 0.25, y: 0.90234375, w: 0.040625, h: 0.0390625\n",
      "  class: 0, x: 0.06953125, y: 0.9375, w: 0.0421875, h: 0.0375\n",
      "  class: 0, x: 0.80703125, y: 0.9453125, w: 0.0484375, h: 0.065625\n",
      "  class: 0, x: 0.18125, y: 0.9484375, w: 0.071875, h: 0.065625\n",
      "  class: 0, x: 0.23203125, y: 0.98515625, w: 0.0421875, h: 0.0296875\n",
      "\n",
      "File: a-31-_jpg.rf.2e0398884f26d5cb18f198e1318cdd46.txt\n",
      "  class: 0, x: 0.1875, y: 0.3875, w: 0.153125, h: 0.18125\n",
      "  class: 0, x: 0.43125, y: 0.078125, w: 0.171875, h: 0.15625\n",
      "  class: 0, x: 0.434375, y: 0.47265625, w: 0.196875, h: 0.1953125\n",
      "  class: 0, x: 0.8484375, y: 0.23203125, w: 0.178125, h: 0.2109375\n",
      "  class: 0, x: 0.928125, y: 0.8421875, w: 0.14375, h: 0.165625\n",
      "\n",
      "File: a-14-_jpg.rf.8c7ad22a25a274cc11765639c7dc1f08.txt\n",
      "  class: 0, x: 0.93828125, y: 0.0546875, w: 0.1234375, h: 0.109375\n",
      "  class: 0, x: 0.54609375, y: 0.228125, w: 0.1640625, h: 0.159375\n",
      "  class: 0, x: 0.47734375, y: 0.5484375, w: 0.2609375, h: 0.2625\n",
      "\n",
      "File: a-13-_jpg.rf.06f7a05d5903900df3e49765dc5e9ed8.txt\n",
      "  class: 0, x: 0.73671875, y: 0.3296875, w: 0.2328125, h: 0.234375\n",
      "  class: 0, x: 0.675, y: 0.553125, w: 0.1625, h: 0.15625\n",
      "  class: 0, x: 0.58125, y: 0.359375, w: 0.1375, h: 0.23125\n",
      "  class: 0, x: 0.31484375, y: 0.7, w: 0.2328125, h: 0.26875\n",
      "\n",
      "File: a-65-_jpg.rf.68b9f99be43624199772f037c339e5f1.txt\n",
      "  class: 0, x: 0.61796875, y: 0.41328125, w: 0.1609375, h: 0.1671875\n",
      "  class: 0, x: 0.85078125, y: 0.64453125, w: 0.1734375, h: 0.1890625\n",
      "  class: 0, x: 0.3515625, y: 0.534375, w: 0.171875, h: 0.196875\n",
      "  class: 0, x: 0.39296875, y: 0.09453125, w: 0.1765625, h: 0.1890625\n",
      "--- END LABEL CHECK ---\n",
      "\n",
      "  class: 0, x: 0.4359375, y: 0.0078125, w: 0.06875, h: 0.015625\n",
      "  class: 0, x: 0.83203125, y: 0.03125, w: 0.0546875, h: 0.053125\n",
      "  class: 0, x: 0.42890625, y: 0.053125, w: 0.0515625, h: 0.05625\n",
      "  class: 0, x: 0.49296875, y: 0.0875, w: 0.0484375, h: 0.05\n",
      "  class: 0, x: 0.58359375, y: 0.15546875, w: 0.0484375, h: 0.0484375\n",
      "  class: 0, x: 0.40703125, y: 0.2859375, w: 0.0515625, h: 0.05\n",
      "  class: 0, x: 0.28984375, y: 0.34296875, w: 0.0421875, h: 0.0421875\n",
      "  class: 0, x: 0.6390625, y: 0.3625, w: 0.0375, h: 0.0375\n",
      "  class: 0, x: 0.584375, y: 0.37734375, w: 0.0375, h: 0.0609375\n",
      "  class: 0, x: 0.93046875, y: 0.50546875, w: 0.0359375, h: 0.0359375\n",
      "  class: 0, x: 0.28203125, y: 0.53203125, w: 0.0421875, h: 0.0421875\n",
      "  class: 0, x: 0.115625, y: 0.6, w: 0.090625, h: 0.084375\n",
      "  class: 0, x: 0.2234375, y: 0.60078125, w: 0.05625, h: 0.0578125\n",
      "  class: 0, x: 0.1984375, y: 0.6515625, w: 0.04375, h: 0.04375\n",
      "  class: 0, x: 0.903125, y: 0.73125, w: 0.08125, h: 0.084375\n",
      "  class: 0, x: 0.70078125, y: 0.7453125, w: 0.0578125, h: 0.053125\n",
      "  class: 0, x: 0.76796875, y: 0.7515625, w: 0.0453125, h: 0.046875\n",
      "  class: 0, x: 0.25, y: 0.90234375, w: 0.040625, h: 0.0390625\n",
      "  class: 0, x: 0.06953125, y: 0.9375, w: 0.0421875, h: 0.0375\n",
      "  class: 0, x: 0.80703125, y: 0.9453125, w: 0.0484375, h: 0.065625\n",
      "  class: 0, x: 0.18125, y: 0.9484375, w: 0.071875, h: 0.065625\n",
      "  class: 0, x: 0.23203125, y: 0.98515625, w: 0.0421875, h: 0.0296875\n",
      "\n",
      "File: a-31-_jpg.rf.2e0398884f26d5cb18f198e1318cdd46.txt\n",
      "  class: 0, x: 0.1875, y: 0.3875, w: 0.153125, h: 0.18125\n",
      "  class: 0, x: 0.43125, y: 0.078125, w: 0.171875, h: 0.15625\n",
      "  class: 0, x: 0.434375, y: 0.47265625, w: 0.196875, h: 0.1953125\n",
      "  class: 0, x: 0.8484375, y: 0.23203125, w: 0.178125, h: 0.2109375\n",
      "  class: 0, x: 0.928125, y: 0.8421875, w: 0.14375, h: 0.165625\n",
      "\n",
      "File: a-14-_jpg.rf.8c7ad22a25a274cc11765639c7dc1f08.txt\n",
      "  class: 0, x: 0.93828125, y: 0.0546875, w: 0.1234375, h: 0.109375\n",
      "  class: 0, x: 0.54609375, y: 0.228125, w: 0.1640625, h: 0.159375\n",
      "  class: 0, x: 0.47734375, y: 0.5484375, w: 0.2609375, h: 0.2625\n",
      "\n",
      "File: a-13-_jpg.rf.06f7a05d5903900df3e49765dc5e9ed8.txt\n",
      "  class: 0, x: 0.73671875, y: 0.3296875, w: 0.2328125, h: 0.234375\n",
      "  class: 0, x: 0.675, y: 0.553125, w: 0.1625, h: 0.15625\n",
      "  class: 0, x: 0.58125, y: 0.359375, w: 0.1375, h: 0.23125\n",
      "  class: 0, x: 0.31484375, y: 0.7, w: 0.2328125, h: 0.26875\n",
      "\n",
      "File: a-65-_jpg.rf.68b9f99be43624199772f037c339e5f1.txt\n",
      "  class: 0, x: 0.61796875, y: 0.41328125, w: 0.1609375, h: 0.1671875\n",
      "  class: 0, x: 0.85078125, y: 0.64453125, w: 0.1734375, h: 0.1890625\n",
      "  class: 0, x: 0.3515625, y: 0.534375, w: 0.171875, h: 0.196875\n",
      "  class: 0, x: 0.39296875, y: 0.09453125, w: 0.1765625, h: 0.1890625\n",
      "--- END LABEL CHECK ---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3.2 Label Sanity Check\n",
    "import random\n",
    "from glob import glob\n",
    "\n",
    "def check_labels(label_dir, num_samples=5):\n",
    "    label_files = glob(os.path.join(label_dir, '*.txt'))\n",
    "    if not label_files:\n",
    "        print(f\"No label files found in {label_dir}\")\n",
    "        return\n",
    "    print(f\"Checking {min(num_samples, len(label_files))} random label files in {label_dir}...\")\n",
    "    for lf in random.sample(label_files, min(num_samples, len(label_files))):\n",
    "        print(f\"\\nFile: {os.path.basename(lf)}\")\n",
    "        with open(lf, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            if not lines:\n",
    "                print(\"  WARNING: Empty label file!\")\n",
    "            for line in lines:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) != 5:\n",
    "                    print(f\"  WARNING: Malformed line: {line.strip()}\")\n",
    "                else:\n",
    "                    cls_idx, x, y, w, h = parts\n",
    "                    print(f\"  class: {cls_idx}, x: {x}, y: {y}, w: {w}, h: {h}\")\n",
    "                    try:\n",
    "                        assert 0 <= float(x) <= 1\n",
    "                        assert 0 <= float(y) <= 1\n",
    "                        assert 0 <= float(w) <= 1\n",
    "                        assert 0 <= float(h) <= 1\n",
    "                    except:\n",
    "                        print(f\"  WARNING: Coordinates out of range: {line.strip()}\")\n",
    "\n",
    "# Check a few label files from train, val, and test\n",
    "print(\"\\n--- LABEL SANITY CHECK ---\")\n",
    "for split in ['train', 'valid', 'test']:\n",
    "    label_dir = os.path.join('data', split, 'labels')\n",
    "    check_labels(label_dir)\n",
    "print(\"--- END LABEL CHECK ---\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d1f96fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model on 453 test images...\n",
      "\n",
      "Test set evaluation (confidence threshold: 0.25):\n",
      "  Accuracy:  1.0000\n",
      "  Precision: 1.0000\n",
      "  Recall:    1.0000\n",
      "  F1-score:  1.0000\n",
      "  Average IoU: 0.5565\n",
      "  Average confidence: 0.6340\n",
      "  Min confidence: 0.2501\n",
      "  Max confidence: 0.9954\n",
      "\n",
      "Test set evaluation (confidence threshold: 0.25):\n",
      "  Accuracy:  1.0000\n",
      "  Precision: 1.0000\n",
      "  Recall:    1.0000\n",
      "  F1-score:  1.0000\n",
      "  Average IoU: 0.5565\n",
      "  Average confidence: 0.6340\n",
      "  Min confidence: 0.2501\n",
      "  Max confidence: 0.9954\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Showing 3 detection examples:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# Define IoU calculation function first to avoid \"not defined\" error\n",
    "def calculate_iou(box1, box2):\n",
    "    \"\"\"\n",
    "    Calculate IoU of two normalized bounding boxes in format [x_center, y_center, width, height]\n",
    "    \"\"\"\n",
    "    # Convert from [x_center, y_center, width, height] to [x1, y1, x2, y2]\n",
    "    box1_x1 = box1[0] - box1[2] / 2\n",
    "    box1_y1 = box1[1] - box1[3] / 2\n",
    "    box1_x2 = box1[0] + box1[2] / 2\n",
    "    box1_y2 = box1[1] + box1[3] / 2\n",
    "    \n",
    "    box2_x1 = box2[0] - box2[2] / 2\n",
    "    box2_y1 = box2[1] - box2[3] / 2\n",
    "    box2_x2 = box2[0] + box2[2] / 2\n",
    "    box2_y2 = box2[1] + box2[3] / 2\n",
    "    \n",
    "    # Calculate intersection area\n",
    "    x_left = max(box1_x1, box2_x1)\n",
    "    y_top = max(box1_y1, box2_y1)\n",
    "    x_right = min(box1_x2, box2_x2)\n",
    "    y_bottom = min(box1_y2, box2_y2)\n",
    "    \n",
    "    if x_right < x_left or y_bottom < y_top:\n",
    "        return 0.0\n",
    "    \n",
    "    intersection_area = (x_right - x_left) * (y_bottom - y_top)\n",
    "    \n",
    "    # Calculate union area\n",
    "    box1_area = (box1_x2 - box1_x1) * (box1_y2 - box1_y1)\n",
    "    box2_area = (box2_x2 - box2_x1) * (box2_y2 - box2_y1)\n",
    "    union_area = box1_area + box2_area - intersection_area\n",
    "    \n",
    "    if union_area == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    return intersection_area / union_area\n",
    "\n",
    "# 4. Evaluate on Test Set (with Confusion Matrix)\n",
    "\n",
    "# Ensure model is loaded from previous training\n",
    "if 'model' not in locals() or model is None:\n",
    "    print(\"Model not loaded. Please run the training cell first.\")\n",
    "else:\n",
    "    # Run inference on test images\n",
    "    test_images_dir = 'data/test/images'\n",
    "    test_images = list(Path(test_images_dir).glob('*.jpg')) + list(Path(test_images_dir).glob('*.png'))\n",
    "    if not test_images:\n",
    "        print(f\"No test images found in {test_images_dir}\")\n",
    "    else:\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        all_iou = []\n",
    "        confidence_scores = []\n",
    "        detection_examples = []\n",
    "        \n",
    "        # Set a confidence threshold for predictions\n",
    "        conf_threshold = 0.25\n",
    "        \n",
    "        print(f\"Evaluating model on {len(test_images)} test images...\")\n",
    "        \n",
    "        # Process in batches to be more efficient\n",
    "        batch_size = 4\n",
    "        for i in range(0, len(test_images), batch_size):\n",
    "            batch = test_images[i:i+batch_size]\n",
    "            batch_paths = [str(p) for p in batch]\n",
    "            \n",
    "            # Run batch prediction with low confidence threshold to get all potential predictions\n",
    "            results = model(batch_paths, conf=0.1, iou=0.5, verbose=False)\n",
    "            \n",
    "            for idx, (img_path, result) in enumerate(zip(batch, results)):\n",
    "                img_name = os.path.basename(img_path)\n",
    "                \n",
    "                # Get ground truth labels\n",
    "                label_path = Path(str(img_path).replace('\\\\images\\\\', '\\\\labels\\\\').replace('/images/', '/labels/').rsplit('.', 1)[0] + '.txt')\n",
    "                gt_boxes = []\n",
    "                if label_path.exists():\n",
    "                    with open(label_path, 'r') as f:\n",
    "                        for line in f:\n",
    "                            parts = line.strip().split()\n",
    "                            if len(parts) == 5:\n",
    "                                cls_id, x, y, w, h = map(float, parts)\n",
    "                                gt_boxes.append({\n",
    "                                    'class': int(cls_id),\n",
    "                                    'bbox': [x, y, w, h]  # Normalized coordinates\n",
    "                                })\n",
    "                \n",
    "                # Get model predictions\n",
    "                pred_boxes = []\n",
    "                if hasattr(result, 'boxes') and result.boxes is not None:\n",
    "                    boxes = result.boxes\n",
    "                    for box_idx in range(len(boxes.cls)):\n",
    "                        conf = float(boxes.conf[box_idx])\n",
    "                        if conf >= conf_threshold:  # Filter by confidence\n",
    "                            cls_id = int(boxes.cls[box_idx])\n",
    "                            xyxy = boxes.xyxy[box_idx].cpu().numpy()  # Get box in xyxy format\n",
    "                            \n",
    "                            # Convert xyxy to normalized xywh for comparison with ground truth\n",
    "                            # This assumes result.orig_shape contains [height, width]\n",
    "                            img_h, img_w = result.orig_shape\n",
    "                            x_center = (xyxy[0] + xyxy[2]) / 2 / img_w\n",
    "                            y_center = (xyxy[1] + xyxy[3]) / 2 / img_h\n",
    "                            width = (xyxy[2] - xyxy[0]) / img_w\n",
    "                            height = (xyxy[3] - xyxy[1]) / img_h\n",
    "                            \n",
    "                            pred_boxes.append({\n",
    "                                'class': cls_id,\n",
    "                                'conf': conf,\n",
    "                                'bbox': [x_center, y_center, width, height]  # Normalized coordinates\n",
    "                            })\n",
    "                            confidence_scores.append(conf)\n",
    "                \n",
    "                # Record true positives and false positives for global metrics\n",
    "                has_gt = len(gt_boxes) > 0\n",
    "                has_pred = len(pred_boxes) > 0\n",
    "                \n",
    "                y_true.append(1 if has_gt else 0)\n",
    "                y_pred.append(1 if has_pred else 0)\n",
    "                \n",
    "                # Store interesting examples for visualization (mismatches or high-confidence detections)\n",
    "                if (has_gt and not has_pred) or (has_pred and not has_gt) or (has_pred and has_gt and pred_boxes[0]['conf'] > 0.8):\n",
    "                    detection_examples.append({\n",
    "                        'img_path': str(img_path),\n",
    "                        'gt_boxes': gt_boxes,\n",
    "                        'pred_boxes': pred_boxes,\n",
    "                        'type': 'FN' if (has_gt and not has_pred) else 'FP' if (has_pred and not has_gt) else 'TP'\n",
    "                    })\n",
    "                \n",
    "                # Calculate IoU for each ground truth box with best matching prediction\n",
    "                if has_gt and has_pred:\n",
    "                    for gt_box in gt_boxes:\n",
    "                        best_iou = 0\n",
    "                        # Calculate IoU with each prediction\n",
    "                        for pred_box in pred_boxes:\n",
    "                            iou = calculate_iou(gt_box['bbox'], pred_box['bbox'])\n",
    "                            if iou > best_iou:\n",
    "                                best_iou = iou\n",
    "                        all_iou.append(best_iou)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "        prec = precision_score(y_true, y_pred, zero_division=0)\n",
    "        rec = recall_score(y_true, y_pred, zero_division=0)\n",
    "        f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "        \n",
    "        print(f\"\\nTest set evaluation (confidence threshold: {conf_threshold}):\")\n",
    "        print(f\"  Accuracy:  {acc:.4f}\")\n",
    "        print(f\"  Precision: {prec:.4f}\")\n",
    "        print(f\"  Recall:    {rec:.4f}\")\n",
    "        print(f\"  F1-score:  {f1:.4f}\")\n",
    "        \n",
    "        if all_iou:\n",
    "            print(f\"  Average IoU: {sum(all_iou)/len(all_iou):.4f}\")\n",
    "        \n",
    "        if confidence_scores:\n",
    "            print(f\"  Average confidence: {sum(confidence_scores)/len(confidence_scores):.4f}\")\n",
    "            print(f\"  Min confidence: {min(confidence_scores):.4f}\")\n",
    "            print(f\"  Max confidence: {max(confidence_scores):.4f}\")\n",
    "        \n",
    "        # Confusion matrix\n",
    "        cm = confusion_matrix(y_true, y_pred, labels=[0,1])\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"No Microplastic\", \"Microplastic\"])\n",
    "        disp.plot(cmap=plt.cm.Blues)\n",
    "        plt.title(\"Confusion Matrix: Microplastic Detection\")\n",
    "        plt.show()\n",
    "        \n",
    "        # Display prediction confidence distribution if available\n",
    "        if confidence_scores:\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.hist(confidence_scores, bins=20, alpha=0.7)\n",
    "            plt.title(\"Distribution of Prediction Confidence Scores\")\n",
    "            plt.xlabel(\"Confidence\")\n",
    "            plt.ylabel(\"Count\")\n",
    "            plt.axvline(x=conf_threshold, color='r', linestyle='--', label=f'Threshold: {conf_threshold}')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "        \n",
    "        # Visualize a few example detections\n",
    "        if detection_examples:\n",
    "            print(f\"\\nShowing {min(3, len(detection_examples))} detection examples:\")\n",
    "            for i, example in enumerate(detection_examples[:3]):\n",
    "                img = plt.imread(example['img_path'])\n",
    "                plt.figure(figsize=(10, 8))\n",
    "                plt.imshow(img)\n",
    "                plt.title(f\"Example {i+1}: {example['type']} - {'Ground Truth' if example['gt_boxes'] else 'No Ground Truth'} | {'Predicted' if example['pred_boxes'] else 'No Prediction'}\")\n",
    "                \n",
    "                # Draw ground truth boxes in green\n",
    "                for box in example['gt_boxes']:\n",
    "                    x, y, w, h = box['bbox']\n",
    "                    img_h, img_w = img.shape[:2]\n",
    "                    rect = plt.Rectangle(\n",
    "                        ((x - w/2) * img_w, (y - h/2) * img_h),\n",
    "                        w * img_w, h * img_h,\n",
    "                        linewidth=2, edgecolor='g', facecolor='none',\n",
    "                        label='Ground Truth'\n",
    "                    )\n",
    "                    plt.gca().add_patch(rect)\n",
    "                \n",
    "                # Draw prediction boxes in red\n",
    "                for box in example['pred_boxes']:\n",
    "                    x, y, w, h = box['bbox']\n",
    "                    conf = box.get('conf', 0)\n",
    "                    img_h, img_w = img.shape[:2]\n",
    "                    rect = plt.Rectangle(\n",
    "                        ((x - w/2) * img_w, (y - h/2) * img_h),\n",
    "                        w * img_w, h * img_h,\n",
    "                        linewidth=2, edgecolor='r', facecolor='none',\n",
    "                        label=f'Prediction (conf: {conf:.2f})'\n",
    "                    )\n",
    "                    plt.gca().add_patch(rect)\n",
    "                    plt.annotate(f'{conf:.2f}', ((x - w/2) * img_w, (y - h/2) * img_h - 5), \n",
    "                                 color='r', fontsize=12, weight='bold')\n",
    "                \n",
    "                plt.legend()\n",
    "                plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f7b26b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "- You can adjust the number of epochs, image size, or model variant (e.g., yolov8m.pt) as needed.\n",
    "- Run the notebook cells to train and evaluate your model.\n",
    "\n",
    "## Understanding the Results\n",
    "\n",
    "### Metrics Explanation\n",
    "- **mAP (mean Average Precision)**: The primary metric for object detection performance\n",
    "- **Precision**: How accurate the positive detections are\n",
    "- **Recall**: The ability of the model to find all microplastics in the image\n",
    "- **IoU (Intersection over Union)**: Measures how well the predicted bounding boxes overlap with the ground truth\n",
    "\n",
    "### Model Improvements\n",
    "\n",
    "To improve model performance:\n",
    "\n",
    "1. **Try larger models**: Replace `yolov8n.pt` with:\n",
    "   - `yolov8s.pt` (small) \n",
    "   - `yolov8m.pt` (medium)\n",
    "   - `yolov8l.pt` (large)\n",
    "   - `yolov8x.pt` (extra large)\n",
    "\n",
    "2. **Data augmentation**: Add more augmentations to prevent overfitting:\n",
    "   ```python\n",
    "   model.train(\n",
    "       # Other parameters\n",
    "       augment=True,\n",
    "       mixup=0.1,\n",
    "       copy_paste=0.1\n",
    "   )\n",
    "   ```\n",
    "\n",
    "3. **Optimization**: Try different optimizers:\n",
    "   ```python\n",
    "   model.train(\n",
    "       # Other parameters\n",
    "       optimizer=\"AdamW\",\n",
    "       lr0=0.001\n",
    "   )\n",
    "   ```\n",
    "\n",
    "4. **Export model**: Save the model for deployment:\n",
    "   ```python\n",
    "   model.export(format=\"onnx\") # or \"torchscript\", \"openvino\", etc.\n",
    "   ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bc9571",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56a41d2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 453 images in data/test/images\n",
      "Visualizing 3 random samples...\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 ground truth boxes and 10 detections.\n",
      "\n",
      "Detection Details:\n",
      "  Box 1: Class 0 (microplastic), Confidence: 0.8816\n",
      "  Box 2: Class 0 (microplastic), Confidence: 0.8204\n",
      "  Box 3: Class 0 (microplastic), Confidence: 0.7243\n",
      "  Box 4: Class 0 (microplastic), Confidence: 0.4579\n",
      "  Box 5: Class 0 (microplastic), Confidence: 0.4028\n",
      "  Box 6: Class 0 (microplastic), Confidence: 0.3163\n",
      "  Box 7: Class 0 (microplastic), Confidence: 0.2672\n",
      "  Box 8: Class 0 (microplastic), Confidence: 0.1938\n",
      "  Box 9: Class 0 (microplastic), Confidence: 0.1909\n",
      "  Box 10: Class 0 (microplastic), Confidence: 0.1191\n",
      "\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 13 ground truth boxes and 23 detections.\n",
      "\n",
      "Detection Details:\n",
      "  Box 1: Class 0 (microplastic), Confidence: 0.9194\n",
      "  Box 2: Class 0 (microplastic), Confidence: 0.8698\n",
      "  Box 3: Class 0 (microplastic), Confidence: 0.7011\n",
      "  Box 4: Class 0 (microplastic), Confidence: 0.6959\n",
      "  Box 5: Class 0 (microplastic), Confidence: 0.6897\n",
      "  Box 6: Class 0 (microplastic), Confidence: 0.5368\n",
      "  Box 7: Class 0 (microplastic), Confidence: 0.5021\n",
      "  Box 8: Class 0 (microplastic), Confidence: 0.4199\n",
      "  Box 9: Class 0 (microplastic), Confidence: 0.4135\n",
      "  Box 10: Class 0 (microplastic), Confidence: 0.3224\n",
      "  Box 11: Class 0 (microplastic), Confidence: 0.3097\n",
      "  Box 12: Class 0 (microplastic), Confidence: 0.2987\n",
      "  Box 13: Class 0 (microplastic), Confidence: 0.2771\n",
      "  Box 14: Class 0 (microplastic), Confidence: 0.2423\n",
      "  Box 15: Class 0 (microplastic), Confidence: 0.2188\n",
      "  Box 16: Class 0 (microplastic), Confidence: 0.2118\n",
      "  Box 17: Class 0 (microplastic), Confidence: 0.2108\n",
      "  Box 18: Class 0 (microplastic), Confidence: 0.1899\n",
      "  Box 19: Class 0 (microplastic), Confidence: 0.1694\n",
      "  Box 20: Class 0 (microplastic), Confidence: 0.1586\n",
      "  Box 21: Class 0 (microplastic), Confidence: 0.1549\n",
      "  Box 22: Class 0 (microplastic), Confidence: 0.1398\n",
      "  Box 23: Class 0 (microplastic), Confidence: 0.1354\n",
      "\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 ground truth boxes and 8 detections.\n",
      "\n",
      "Detection Details:\n",
      "  Box 1: Class 0 (microplastic), Confidence: 0.9690\n",
      "  Box 2: Class 0 (microplastic), Confidence: 0.9634\n",
      "  Box 3: Class 0 (microplastic), Confidence: 0.8262\n",
      "  Box 4: Class 0 (microplastic), Confidence: 0.8018\n",
      "  Box 5: Class 0 (microplastic), Confidence: 0.4220\n",
      "  Box 6: Class 0 (microplastic), Confidence: 0.2429\n",
      "  Box 7: Class 0 (microplastic), Confidence: 0.1225\n",
      "  Box 8: Class 0 (microplastic), Confidence: 0.1224\n",
      "\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 5. Visualize Model Predictions\n",
    "def visualize_predictions(model, image_path, confidence=0.25):\n",
    "    \"\"\"Visualize model predictions on a single image with detailed metrics\"\"\"\n",
    "    if not os.path.exists(image_path):\n",
    "        print(f\"Image not found: {image_path}\")\n",
    "        return\n",
    "    \n",
    "    # Run inference\n",
    "    results = model(image_path, conf=confidence)\n",
    "    result = results[0]\n",
    "    \n",
    "    # Get image\n",
    "    img = plt.imread(image_path)\n",
    "    \n",
    "    # Plot the image\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.imshow(img)\n",
    "    plt.title(f\"YOLOv8 Detection: {os.path.basename(image_path)}\")\n",
    "    \n",
    "    # Get the ground truth labels\n",
    "    label_path = Path(str(image_path).replace('/images/', '/labels/').replace('\\\\images\\\\', '\\\\labels\\\\').rsplit('.', 1)[0] + '.txt')\n",
    "    gt_boxes = []\n",
    "    if label_path.exists():\n",
    "        with open(label_path, 'r') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) == 5:\n",
    "                    cls_id, x, y, w, h = map(float, parts)\n",
    "                    gt_boxes.append({\n",
    "                        'class': int(cls_id),\n",
    "                        'bbox': [x, y, w, h]  # Normalized coordinates\n",
    "                    })\n",
    "    \n",
    "    # Draw ground truth boxes in green\n",
    "    for box in gt_boxes:\n",
    "        x, y, w, h = box['bbox']\n",
    "        img_h, img_w = img.shape[:2]\n",
    "        rect = plt.Rectangle(\n",
    "            ((x - w/2) * img_w, (y - h/2) * img_h),\n",
    "            w * img_w, h * img_h,\n",
    "            linewidth=2, edgecolor='g', facecolor='none',\n",
    "            label='Ground Truth'\n",
    "        )\n",
    "        plt.gca().add_patch(rect)\n",
    "    \n",
    "    # Draw detection boxes\n",
    "    if hasattr(result, 'boxes') and result.boxes is not None:\n",
    "        boxes = result.boxes\n",
    "        for i in range(len(boxes.cls)):\n",
    "            # Get box coordinates\n",
    "            box = boxes.xyxy[i].cpu().numpy()\n",
    "            x1, y1, x2, y2 = box\n",
    "            conf = boxes.conf[i].item()\n",
    "            cls_id = int(boxes.cls[i].item())\n",
    "            \n",
    "            # Draw rectangle\n",
    "            rect = plt.Rectangle(\n",
    "                (x1, y1),\n",
    "                x2 - x1, y2 - y1,\n",
    "                linewidth=2, edgecolor='r', facecolor='none',\n",
    "                label=f'Prediction (conf: {conf:.2f})'\n",
    "            )\n",
    "            plt.gca().add_patch(rect)\n",
    "            \n",
    "            # Add confidence score\n",
    "            plt.text(\n",
    "                x1, y1 - 5,\n",
    "                f\"{conf:.2f}\",\n",
    "                color='white', fontsize=10, bbox=dict(facecolor='red', alpha=0.5)\n",
    "            )\n",
    "    \n",
    "    # Only add unique legend entries\n",
    "    handles, labels = plt.gca().get_legend_handles_labels()\n",
    "    by_label = dict(zip(labels, handles))\n",
    "    plt.legend(by_label.values(), by_label.keys(), loc='upper right')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Print detection details\n",
    "    print(f\"Found {len(gt_boxes)} ground truth boxes and {0 if not hasattr(result, 'boxes') else len(result.boxes)} detections.\")\n",
    "    \n",
    "    if hasattr(result, 'boxes') and result.boxes is not None and len(result.boxes) > 0:\n",
    "        print(\"\\nDetection Details:\")\n",
    "        for i in range(len(result.boxes)):\n",
    "            conf = result.boxes.conf[i].item()\n",
    "            cls_id = int(result.boxes.cls[i].item())\n",
    "            print(f\"  Box {i+1}: Class {cls_id} (microplastic), Confidence: {conf:.4f}\")\n",
    "\n",
    "# Function to visualize random test images\n",
    "def visualize_random_samples(model, num_samples=3, data_path=\"data/test/images\", confidence=0.25):\n",
    "    \"\"\"Visualize predictions on random samples from the dataset\"\"\"\n",
    "    image_files = list(Path(data_path).glob('*.jpg')) + list(Path(data_path).glob('*.png'))\n",
    "    if not image_files:\n",
    "        print(f\"No images found in {data_path}\")\n",
    "        return\n",
    "        \n",
    "    print(f\"Found {len(image_files)} images in {data_path}\")\n",
    "    print(f\"Visualizing {min(num_samples, len(image_files))} random samples...\\n\")\n",
    "    \n",
    "    # Select random samples\n",
    "    samples = random.sample(image_files, min(num_samples, len(image_files)))\n",
    "    \n",
    "    # Visualize each sample\n",
    "    for img_path in samples:\n",
    "        visualize_predictions(model, str(img_path), confidence=confidence)\n",
    "        print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "\n",
    "# Try on a few random test images\n",
    "if 'model' in locals() and model is not None:\n",
    "    try:\n",
    "        # Set a lower confidence threshold to see more potential detections\n",
    "        visualize_random_samples(model, num_samples=3, confidence=0.1)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during visualization: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
