{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49273b9b",
   "metadata": {},
   "source": [
    "# YOLOv8 Microplastics Detection\n",
    "\n",
    "This notebook will guide you through training a YOLOv8 object detection model to detect microplastics using your dataset.\n",
    "\n",
    "> **Troubleshooting DataLoader Errors**: If you encounter `DataLoader worker exited unexpectedly` errors, this notebook has been updated to fix these issues by:\n",
    "> 1. Setting workers=0 to avoid multiprocessing issues\n",
    "> 2. Reducing batch size to prevent memory overflows\n",
    "> 3. Using CPU instead of GPU for more stable processing\n",
    "> 4. Disabling caching to prevent file access conflicts\n",
    "\n",
    "> **Important Update**: Your dataset is in detection format (bounding boxes), not segmentation format. The notebook has been updated to use YOLOv8 detection instead of segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a783f3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ultralytics in c:\\users\\blasi\\anaconda3\\lib\\site-packages (8.3.134)\n",
      "Requirement already satisfied: numpy>=1.23.0 in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from ultralytics) (1.26.4)\n",
      "Requirement already satisfied: matplotlib>=3.3.0 in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from ultralytics) (3.10.3)\n",
      "Requirement already satisfied: opencv-python>=4.6.0 in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from ultralytics) (4.11.0.86)\n",
      "Requirement already satisfied: pillow>=7.1.2 in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from ultralytics) (10.2.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from ultralytics) (6.0.2)\n",
      "Requirement already satisfied: requests>=2.23.0 in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from ultralytics) (2.32.3)\n",
      "Requirement already satisfied: scipy>=1.4.1 in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from ultralytics) (1.11.4)\n",
      "Requirement already satisfied: torch>=1.8.0 in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from ultralytics) (2.5.1+cu121)\n",
      "Requirement already satisfied: torchvision>=0.9.0 in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from ultralytics) (0.20.1+cu121)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from ultralytics) (4.65.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from ultralytics) (5.9.0)\n",
      "Requirement already satisfied: py-cpuinfo in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from ultralytics) (9.0.0)\n",
      "Requirement already satisfied: pandas>=1.1.4 in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from ultralytics) (2.2.3)\n",
      "Requirement already satisfied: seaborn>=0.11.0 in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from ultralytics) (0.13.2)\n",
      "Requirement already satisfied: ultralytics-thop>=2.0.0 in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from ultralytics) (2.0.14)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (24.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from pandas>=1.1.4->ultralytics) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from pandas>=1.1.4->ultralytics) (2023.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from requests>=2.23.0->ultralytics) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from requests>=2.23.0->ultralytics) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from requests>=2.23.0->ultralytics) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from requests>=2.23.0->ultralytics) (2024.8.30)\n",
      "Requirement already satisfied: filelock in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from torch>=1.8.0->ultralytics) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from torch>=1.8.0->ultralytics) (2023.10.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from tqdm>=4.64.0->ultralytics) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\blasi\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# 1. Install Required Libraries\n",
    "%pip install ultralytics\n",
    "\n",
    "# The following line will download the YOLOv8 detection model if it doesn't exist\n",
    "# Uncomment if you need to download it\n",
    "# !python -c \"from ultralytics import YOLO; YOLO('yolov8n.pt')\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337cec39",
   "metadata": {},
   "source": [
    "## 2. Dataset Structure and Format\n",
    "\n",
    "Your dataset should be structured as:\n",
    "- data/train/images/, data/train/labels/\n",
    "- data/valid/images/, data/valid/labels/\n",
    "- data/test/images/, data/test/labels/\n",
    "\n",
    "### Label Format for Object Detection\n",
    "YOLOv8 detection labels contain normalized bounding box coordinates for each object:\n",
    "```\n",
    "<class-id> <x_center> <y_center> <width> <height>\n",
    "```\n",
    "Where:\n",
    "- `class-id`: The object class (0 for microplastics)\n",
    "- `x_center, y_center`: Normalized center coordinates of the bounding box (0-1)\n",
    "- `width, height`: Normalized width and height of the bounding box (0-1)\n",
    "\n",
    "Each object in an image has its own line in the label file. The dataset already contains these labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d4c274",
   "metadata": {},
   "source": [
    "## Evaluation Metrics\n",
    "\n",
    "This notebook uses the following evaluation metrics to assess the performance of the microplastics detection model:\n",
    "\n",
    "- **mAP (mean Average Precision)**: The primary metric for object detection performance\n",
    "- **Precision**: How accurate the positive detections are\n",
    "- **Recall**: The ability of the model to find all microplastics in the image\n",
    "- **IoU (Intersection over Union)**: Measures how well the predicted bounding boxes overlap with the ground truth\n",
    "- **Confidence Score**: The model's certainty in its detections (we aim to improve from 0.5888 to >0.60)\n",
    "\n",
    "Detailed explanations of these metrics are provided later in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "616bbf68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating and fixing data.yaml...\n",
      "data.yaml is already correctly configured.\n",
      "\n",
      "Validating absolute paths:\n",
      "Configured path: c:/Users/blasi/CS-ML/FINAL_PROJ\n",
      "\n",
      "Primary configurations:\n",
      "- Checking Train path: c:/Users/blasi/CS-ML/FINAL_PROJ/data/train/images\n",
      "  ✓ Path exists! Found 3226 images.\n",
      "  ✓ Found 3226 labels.\n",
      "- Checking Validation path: c:/Users/blasi/CS-ML/FINAL_PROJ/data/valid/images\n",
      "  ✓ Path exists! Found 928 images.\n",
      "  ✓ Found 928 labels.\n",
      "- Checking Test path: c:/Users/blasi/CS-ML/FINAL_PROJ/data/test/images\n",
      "  ✓ Path exists! Found 453 images.\n",
      "  ✓ Found 453 labels.\n",
      "\n",
      "YOLOv8 settings found at: C:\\Users\\blasi\\AppData\\Roaming\\Ultralytics\\settings.json\n",
      "YOLOv8 datasets_dir: C:\\Users\\blasi\\CS-ML\\FINAL_PROJ\\datasets\n"
     ]
    }
   ],
   "source": [
    "# 3. Training YOLOv8 Detection Model\n",
    "import os\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "import yaml\n",
    "\n",
    "# Set CUDA memory configurations to help with memory fragmentation\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "DATASET_YAML = 'data.yaml'\n",
    "\n",
    "# 3.0 Data Configuration Validation and Repair\n",
    "def validate_and_fix_data_yaml(yaml_path):\n",
    "    \"\"\"Validate and fix data.yaml file for YOLOv8 compatibility\"\"\"\n",
    "    print(f\"Validating and fixing {yaml_path}...\")\n",
    "    try:\n",
    "        with open(yaml_path, 'r') as f:\n",
    "            data_cfg = yaml.safe_load(f)\n",
    "        \n",
    "        # Make a copy to check if changes were made\n",
    "        original_cfg = data_cfg.copy()\n",
    "        \n",
    "        # Get project root directory (absolute path)\n",
    "        project_root = os.path.abspath(os.path.dirname(yaml_path))\n",
    "        \n",
    "        # Fix fields for YOLOv8 with absolute paths\n",
    "        data_cfg[\"path\"] = project_root.replace('\\\\', '/')  # Use forward slashes for paths\n",
    "        \n",
    "        # These are relative to the path\n",
    "        data_cfg[\"train\"] = \"data/train/images\"\n",
    "        data_cfg[\"val\"] = \"data/valid/images\"\n",
    "        data_cfg[\"test\"] = \"data/test/images\"\n",
    "        data_cfg[\"nc\"] = 1\n",
    "        data_cfg[\"names\"] = [\"microplastic\"]\n",
    "        \n",
    "        \n",
    "        # Check if changes were made\n",
    "        if data_cfg != original_cfg:\n",
    "            print(\"Changes needed in data.yaml. Updating file...\")\n",
    "            with open(yaml_path, 'w') as f:\n",
    "                yaml.dump(data_cfg, f, default_flow_style=False)\n",
    "            print(\"data.yaml has been updated with correct configuration.\")\n",
    "        else:\n",
    "            print(\"data.yaml is already correctly configured.\")\n",
    "        \n",
    "        # Validate absolute paths - important for troubleshooting\n",
    "        train_path = os.path.join(data_cfg[\"path\"], \"data/train/images\")\n",
    "        val_path = os.path.join(data_cfg[\"path\"], \"data/valid/images\")\n",
    "        test_path = os.path.join(data_cfg[\"path\"], \"data/test/images\")\n",
    "        \n",
    "        train_path_alt = os.path.join(project_root, \"data/train/images\")\n",
    "        val_path_alt = os.path.join(project_root, \"data/valid/images\")\n",
    "        test_path_alt = os.path.join(project_root, \"data/test/images\")\n",
    "        \n",
    "        print(\"\\nValidating absolute paths:\")\n",
    "        print(f\"Configured path: {data_cfg['path']}\")\n",
    "        \n",
    "        def check_path(path, name):\n",
    "            print(f\"- Checking {name} path: {path}\")\n",
    "            if os.path.exists(path):\n",
    "                images = len(list(Path(path).glob('*.jpg'))) + len(list(Path(path).glob('*.png')))\n",
    "                print(f\"  ✓ Path exists! Found {images} images.\")\n",
    "                # Check for corresponding labels\n",
    "                label_path = path.replace('images', 'labels')\n",
    "                if os.path.exists(label_path):\n",
    "                    labels = len(list(Path(label_path).glob('*.txt')))\n",
    "                    print(f\"  ✓ Found {labels} labels.\")\n",
    "                    if labels < images:\n",
    "                        print(f\"  ⚠ WARNING: {images-labels} images may be missing labels!\")\n",
    "                else:\n",
    "                    print(f\"  ✗ WARNING: Label directory {label_path} does not exist!\")\n",
    "                return images > 0\n",
    "            else:\n",
    "                print(f\"  ✗ ERROR: Directory does not exist!\")\n",
    "                return False\n",
    "        \n",
    "        # Check primary paths\n",
    "        print(\"\\nPrimary configurations:\")\n",
    "        train_ok = check_path(train_path.replace('\\\\', '/'), \"Train\")\n",
    "        val_ok = check_path(val_path.replace('\\\\', '/'), \"Validation\")\n",
    "        test_ok = check_path(test_path.replace('\\\\', '/'), \"Test\")\n",
    "        \n",
    "        # If paths are missing, check alternative paths\n",
    "        if not (train_ok and val_ok and test_ok):\n",
    "            print(\"\\nChecking alternative paths:\")\n",
    "            check_path(train_path_alt.replace('\\\\', '/'), \"Alt Train\")\n",
    "            check_path(val_path_alt.replace('\\\\', '/'), \"Alt Validation\")\n",
    "            check_path(test_path_alt.replace('\\\\', '/'), \"Alt Test\")\n",
    "        \n",
    "        # Check paths that YOLOv8 might be trying to use (for debugging)\n",
    "        datasets_path = os.path.join(project_root, \"datasets\")\n",
    "        if os.path.exists(datasets_path):\n",
    "            print(f\"\\nWARNING: A 'datasets' directory exists at {datasets_path}\")\n",
    "            print(\"This might be causing path resolution conflicts. YOLOv8 might be looking here instead of your data directory.\")\n",
    "        \n",
    "        # Check YOLOv8 settings\n",
    "        settings_path = os.path.expandvars(r\"%APPDATA%\\Ultralytics\\settings.json\")\n",
    "        if os.path.exists(settings_path):\n",
    "            print(f\"\\nYOLOv8 settings found at: {settings_path}\")\n",
    "            try:\n",
    "                import json\n",
    "                with open(settings_path, 'r') as f:\n",
    "                    settings = json.load(f)\n",
    "                if 'datasets_dir' in settings:\n",
    "                    print(f\"YOLOv8 datasets_dir: {settings['datasets_dir']}\")\n",
    "            except:\n",
    "                print(\"Could not read YOLOv8 settings file.\")\n",
    "        \n",
    "        return data_cfg\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR processing data.yaml: {e}\")\n",
    "        return None\n",
    "\n",
    "# Run validation and fix\n",
    "data_cfg = validate_and_fix_data_yaml(DATASET_YAML)\n",
    "\n",
    "def verify_dataset(yaml_path):\n",
    "    if not os.path.exists(yaml_path):\n",
    "        print(f\"ERROR: Dataset YAML file not found: {yaml_path}\")\n",
    "        return False\n",
    "    try:\n",
    "        with open(yaml_path, 'r') as f:\n",
    "            data_cfg = yaml.safe_load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Failed to load YAML: {e}\")\n",
    "        return False\n",
    "    if 'path' not in data_cfg:\n",
    "        print(\"ERROR: 'path' key missing in YAML file.\")\n",
    "        return False\n",
    "    base_path = Path(data_cfg['path'])\n",
    "    print(f\"Base path: {base_path}\")\n",
    "    all_ok = True\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        if split in data_cfg:\n",
    "            split_path = base_path / data_cfg[split]\n",
    "            print(f\"{split.capitalize()} path: {split_path}\")\n",
    "            if not split_path.exists():\n",
    "                print(f\"WARNING: {split} path does not exist: {split_path}\")\n",
    "                try:\n",
    "                    os.makedirs(split_path, exist_ok=True)\n",
    "                    print(f\"Created directory: {split_path}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"ERROR: Could not create directory {split_path}: {e}\")\n",
    "                    all_ok = False\n",
    "            else:\n",
    "                img_count = len(list(split_path.glob('*.jpg'))) + len(list(split_path.glob('*.png')))\n",
    "                print(f\"  Found {img_count} images in {split} folder\")\n",
    "                if img_count == 0:\n",
    "                    print(f\"WARNING: No images found in {split_path}\")\n",
    "    return all_ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156f804e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: NVIDIA GeForce RTX 3050 Ti Laptop GPU\n",
      "GPU memory: 4.00 GB\n",
      "Optimal batch size for your GPU: 16\n",
      "Selected model: yolov8l.pt\n",
      "Selected image size: 640\n",
      "Optimal worker count: 6\n",
      "CUDA cache cleared for training\n",
      "Verifying dataset paths...\n",
      "Base path: c:\\Users\\blasi\\CS-ML\\FINAL_PROJ\n",
      "Train path: c:\\Users\\blasi\\CS-ML\\FINAL_PROJ\\data\\train\\images\n",
      "  Found 3226 images in train folder\n",
      "Val path: c:\\Users\\blasi\\CS-ML\\FINAL_PROJ\\data\\valid\\images\n",
      "  Found 928 images in val folder\n",
      "Test path: c:\\Users\\blasi\\CS-ML\\FINAL_PROJ\\data\\test\\images\n",
      "  Found 453 images in test folder\n",
      "Using yolov8l.pt for training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.1, copy_paste_mode=flip, cos_lr=True, cutmix=0.0, data=temp_data.yaml, degrees=10.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=1, erasing=0.4, exist_ok=True, fliplr=0.5, flipud=0.5, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.6, keras=False, kobj=1.0, line_width=None, lr0=0.001, lrf=0.01, mask_ratio=4, max_det=100, mixup=0.1, mode=train, model=yolov8l.pt, momentum=0.937, mosaic=0.5, multi_scale=False, name=train, nbs=64, nms=True, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=runs/detect, rect=True, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=runs\\detect\\train, save_frames=False, save_json=False, save_period=10, save_txt=False, scale=0, seed=0, shear=2.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=True, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=1.0, warmup_momentum=0.8, weight_decay=0.0005, workers=6, workspace=None\n",
      "Overriding class names with single class.\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1      1856  ultralytics.nn.modules.conv.Conv             [3, 64, 3, 2]                 \n",
      "  1                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  2                  -1  3    279808  ultralytics.nn.modules.block.C2f             [128, 128, 3, True]           \n",
      "  3                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  4                  -1  6   2101248  ultralytics.nn.modules.block.C2f             [256, 256, 6, True]           \n",
      "  5                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
      "  6                  -1  6   8396800  ultralytics.nn.modules.block.C2f             [512, 512, 6, True]           \n",
      "  7                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
      "  8                  -1  3   4461568  ultralytics.nn.modules.block.C2f             [512, 512, 3, True]           \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  3   4723712  ultralytics.nn.modules.block.C2f             [1024, 512, 3]                \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  3   1247744  ultralytics.nn.modules.block.C2f             [768, 256, 3]                 \n",
      " 16                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  3   4592640  ultralytics.nn.modules.block.C2f             [768, 512, 3]                 \n",
      " 19                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  3   4723712  ultralytics.nn.modules.block.C2f             [1024, 512, 3]                \n",
      " 22        [15, 18, 21]  1   5583571  ultralytics.nn.modules.head.Detect           [1, [256, 512, 512]]          \n",
      "Model summary: 209 layers, 43,630,611 parameters, 43,630,595 gradients, 165.4 GFLOPs\n",
      "\n",
      "Transferred 589/595 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\blasi\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1326: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10/cuda/CUDAAllocatorConfig.h:28.)\n",
      "  return t.to(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed \n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.10.1 ms, read: 329.590.3 MB/s, size: 34.1 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\blasi\\CS-ML\\FINAL_PROJ\\data\\train\\labels.cache... 3226 images, 0 backgrounds, 0 corrupt: 100%|██████████| 3226/3226 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 'rect=True' is incompatible with DataLoader shuffle, setting shuffle=False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.1 ms, read: 198.745.1 MB/s, size: 32.7 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\blasi\\CS-ML\\FINAL_PROJ\\data\\valid\\labels.cache... 928 images, 0 backgrounds, 0 corrupt: 100%|██████████| 928/928 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs\\detect\\train\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.001' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 97 weight(decay=0.0), 104 weight(decay=0.0005), 103 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 6 dataloader workers\n",
      "Logging results to \u001b[1mruns\\detect\\train\u001b[0m\n",
      "Starting training for 1 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/202 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# Device selection\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "    print(f\"CUDA available: {torch.cuda.get_device_name(0)}\")\n",
    "    \n",
    "    # Set optimal CUDA settings for better performance\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cudnn.deterministic = False\n",
    "    \n",
    "    # Check memory availability - helps determine batch size\n",
    "    gpu_mem = torch.cuda.get_device_properties(0).total_memory / (1024 ** 3)  # in GB\n",
    "    print(f\"GPU memory: {gpu_mem:.2f} GB\")\n",
    "    \n",
    "\n",
    "    optimal_batch = 16\n",
    "    optimal_model = 'yolov8l.pt'  # Largest model\n",
    "    optimal_size = 640\n",
    "\n",
    "    \n",
    "    print(f\"Optimal batch size for your GPU: {optimal_batch}\")\n",
    "    print(f\"Selected model: {optimal_model}\")\n",
    "    print(f\"Selected image size: {optimal_size}\")\n",
    "    \n",
    "    # Determine optimal worker count (reduced to avoid memory issues)\n",
    "    optimal_workers = 6  \n",
    "    print(f\"Optimal worker count: {optimal_workers}\")\n",
    "    \n",
    "    # Empty cache to start fresh\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"CUDA cache cleared for training\")\n",
    "    \n",
    "    # Garbage collection to free up memory\n",
    "    import gc\n",
    "    gc.collect()\n",
    "    \n",
    "    # Release any leftover memory\n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"No GPU available, using CPU instead.\")\n",
    "    optimal_batch = 4  # Reduced for CPU\n",
    "    optimal_workers = 0\n",
    "    optimal_model = 'yolov8n.pt'  # Smallest model\n",
    "    optimal_size = 640  # Smaller image size\n",
    "\n",
    "print(\"Verifying dataset paths...\")\n",
    "dataset_ok = verify_dataset(DATASET_YAML)\n",
    "if not dataset_ok:\n",
    "    print(\"Dataset verification failed. Please check your dataset structure and YAML file.\")\n",
    "else:\n",
    "    # Load YOLOv8 detection model - try a larger model for better accuracy\n",
    "    try:\n",
    "        # Find the best available model - prioritizing models that work with available memory\n",
    "        model_candidates = [optimal_model, 'yolov8n.pt']  # Fallback to nano model\n",
    "        selected_model = None\n",
    "        \n",
    "        for model_path in model_candidates:\n",
    "            if os.path.exists(model_path):\n",
    "                selected_model = model_path\n",
    "                print(f\"Using {selected_model} for training...\")\n",
    "                break\n",
    "        \n",
    "        if selected_model is None:\n",
    "            print(f\"No YOLOv8 model found, using {optimal_model} and downloading if needed...\")\n",
    "            selected_model = optimal_model\n",
    "        \n",
    "        model = YOLO(selected_model)\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Could not load YOLOv8 model: {e}\")\n",
    "        model = None\n",
    "        \n",
    "    if model is not None:\n",
    "        try:\n",
    "            print(\"Starting training...\")\n",
    "            \n",
    "            # Create a copy of data.yaml with absolute paths to prevent path resolution issues\n",
    "            import shutil\n",
    "            temp_yaml = 'temp_data.yaml'\n",
    "            shutil.copy(DATASET_YAML, temp_yaml)\n",
    "            \n",
    "            # Update the temporary YAML with absolute paths\n",
    "            with open(temp_yaml, 'r') as f:\n",
    "                temp_data = yaml.safe_load(f)\n",
    "            \n",
    "            # Ensure path is absolute with forward slashes\n",
    "            project_root = os.path.abspath(os.path.dirname(DATASET_YAML))\n",
    "            temp_data['path'] = project_root.replace('\\\\', '/')\n",
    "            \n",
    "            # Write updated YAML\n",
    "            with open(temp_yaml, 'w') as f:\n",
    "                yaml.dump(temp_data, f, default_flow_style=False)\n",
    "            \n",
    "            # print(f\"Temporary data.yaml created with absolute paths:\")\n",
    "            # print(f\"- path: {temp_data['path']}\")\n",
    "            # print(f\"- train: {temp_data['train']}\")\n",
    "            # print(f\"- val: {temp_data['val']}\")\n",
    "            \n",
    "            # Display actual paths that will be used\n",
    "            train_path = os.path.join(temp_data['path'], temp_data['train'])\n",
    "            val_path = os.path.join(temp_data['path'], temp_data['val'])\n",
    "            # print(f\"Full train path: {train_path}\")\n",
    "            # print(f\"Full val path: {val_path}\")\n",
    "            \n",
    "            # # Empty cache again right before training\n",
    "            # if torch.cuda.is_available():\n",
    "            #     torch.cuda.empty_cache()\n",
    "            \n",
    "            # MEMORY-OPTIMIZED training configuration\n",
    "            results = model.train(\n",
    "                data=temp_yaml,            # Use the temporary YAML with absolute paths\n",
    "                epochs=1,                # Reduced epochs for testing\n",
    "                imgsz=optimal_size,        # REDUCED image size based on GPU memory\n",
    "                batch=optimal_batch,       # REDUCED batch size based on GPU memory\n",
    "                workers=optimal_workers,   # REDUCED worker count to conserve memory\n",
    "                \n",
    "                # Simplified data augmentation to reduce memory usage\n",
    "                mosaic=0.5,                # REDUCED mosaic augmentation\n",
    "                mixup=0.1,                 # REDUCED mixup augmentation\n",
    "                copy_paste=0.1,            # REDUCED copy-paste augmentation\n",
    "                scale=0,                 # Scale range\n",
    "                degrees=10.0,               # Rotation augmentation\n",
    "                translate=0.1,              # REDUCED translation augmentation\n",
    "                perspective=0.0,            # No perspective augmentation\n",
    "                shear=2.0,                  # REDUCED shear augmentation\n",
    "                flipud=0.5,                 # Flip up-down augmentation\n",
    "                fliplr=0.5,                 # Flip left-right augmentation\n",
    "                hsv_h=0.015,                # HSV hue augmentation\n",
    "                hsv_s=0.7,                  # HSV saturation augmentation\n",
    "                hsv_v=0.4,                  # HSV value augmentation\n",
    "                \n",
    "                # Memory-efficient learning parameters\n",
    "                lr0=0.001,                   # Initial learning rate \n",
    "                lrf=0.010,                  # Final learning rate factor\n",
    "                momentum=0.937,             # SGD momentum/Adam beta1\n",
    "                weight_decay=0.0005,        # Optimizer weight decay\n",
    "                warmup_epochs=1.0,          # REDUCED warmup epochs\n",
    "                warmup_momentum=0.8,        # Warmup momentum\n",
    "                cos_lr=True,                # Use cosine learning rate scheduler\n",
    "                \n",
    "                # Reduced loss function complexity\n",
    "                box=7.5,                    # REDUCED box loss gain\n",
    "                cls=0.5,                    # Class loss gain\n",
    "                dfl=1.5,                    # REDUCED distribution focal loss gain\n",
    "                \n",
    "                # Hardware settings\n",
    "                device=device,              # Use the detected device\n",
    "                \n",
    "                # Project settings\n",
    "                project='runs/detect',      # Project directory\n",
    "                name='train',               # Run name\n",
    "                exist_ok=True,              # Overwrite existing directory\n",
    "                \n",
    "                # Memory optimization settings\n",
    "                cache=False,                # Disable cache\n",
    "                amp=torch.cuda.is_available(), # Mixed precision only if GPU available\n",
    "                fraction=1.0,               # Dataset fraction to use for training\n",
    "                \n",
    "                # Reduced IoU settings to conserve memory\n",
    "                iou=0.6,                    # IoU threshold for NMS \n",
    "                max_det=100,                # REDUCED maximum detections per image\n",
    "                nms=True,                   # NMS for better detection\n",
    "                single_cls=True,            # Force single class detection\n",
    "                rect=True,                  # Rectangular training\n",
    "                \n",
    "                # Simplified validation to save memory\n",
    "                val=True,                   # Run validation during training\n",
    "                save_json=False,            # DISABLED JSON saving to reduce memory\n",
    "                save=True,                  # Save model checkpoints\n",
    "                save_period=10,             # Save checkpoints every N epochs\n",
    "                plots=True                  # Generate plots during training\n",
    "            )\n",
    "            \n",
    "            # Clean up temporary file\n",
    "            try:\n",
    "                os.remove(temp_yaml)\n",
    "                print(f\"Temporary data file {temp_yaml} removed\")\n",
    "            except:\n",
    "                pass\n",
    "                \n",
    "            print(\"Training completed successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR during training: {e}\")\n",
    "            \n",
    "            # Free GPU memory on error\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "                print(\"GPU memory cleared after error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3783eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 Training Visualization\n",
    "from IPython.display import display, Image\n",
    "from pathlib import Path\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Function to display training progress during or after training\n",
    "def show_training_plots():\n",
    "    results_path = Path('runs/detect/train')  # Changed from segment to detect\n",
    "    \n",
    "    # Check if the directory exists first\n",
    "    if not results_path.exists():\n",
    "        print(f\"Warning: Results directory not found at {results_path}\")\n",
    "        return\n",
    "    \n",
    "    # Results plots\n",
    "    plots = {\n",
    "        'Training Loss': results_path / 'results.png',\n",
    "        'Validation Confusion Matrix': results_path / 'val_confusion_matrix_normalized.png',\n",
    "        'PR Curve': results_path / 'PR_curve.png'\n",
    "    }\n",
    "    \n",
    "    found_plots = False\n",
    "    for title, plot_path in plots.items():\n",
    "        if plot_path.exists():\n",
    "            found_plots = True\n",
    "            print(f\"\\n{title}:\")\n",
    "            try:\n",
    "                display(Image(str(plot_path)))\n",
    "            except Exception as e:\n",
    "                print(f\"Error displaying {title}: {str(e)}\")\n",
    "        else:\n",
    "            print(f\"\\n{title} plot not found at {plot_path}\")\n",
    "    \n",
    "    if not found_plots:\n",
    "        print(\"No training plots found. Training may not have completed successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3707ac65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- LABEL SANITY CHECK ---\n",
      "Checking 5 random label files in data\\train\\labels...\n",
      "\n",
      "File: 90_jpg.rf.d7e7b7535a9f94a79f751598aa7ebd3a.txt\n",
      "  class: 0, x: 0.67265625, y: 0.01484375, w: 0.0328125, h: 0.0296875\n",
      "  class: 0, x: 0.584375, y: 0.025, w: 0.046875, h: 0.046875\n",
      "  class: 0, x: 0.17734375, y: 0.028125, w: 0.0390625, h: 0.040625\n",
      "  class: 0, x: 0.52578125, y: 0.03515625, w: 0.0515625, h: 0.0703125\n",
      "  class: 0, x: 0.234375, y: 0.1359375, w: 0.03125, h: 0.034375\n",
      "  class: 0, x: 0.03203125, y: 0.1953125, w: 0.0515625, h: 0.05\n",
      "  class: 0, x: 0.17734375, y: 0.23828125, w: 0.0453125, h: 0.0453125\n",
      "  class: 0, x: 0.54375, y: 0.3921875, w: 0.04375, h: 0.05625\n",
      "  class: 0, x: 0.3828125, y: 0.42578125, w: 0.06875, h: 0.0703125\n",
      "  class: 0, x: 0.0453125, y: 0.4609375, w: 0.090625, h: 0.121875\n",
      "  class: 0, x: 0.78828125, y: 0.60078125, w: 0.0578125, h: 0.0578125\n",
      "  class: 0, x: 0.24375, y: 0.64921875, w: 0.05, h: 0.0484375\n",
      "  class: 0, x: 0.68359375, y: 0.8421875, w: 0.0546875, h: 0.05625\n",
      "  class: 0, x: 0.58984375, y: 0.85625, w: 0.0578125, h: 0.059375\n",
      "\n",
      "File: f-104-_jpg.rf.a34e1e1dbdad4b9eb5b4f49013afac44.txt\n",
      "  class: 0, x: 0.8546875, y: 0.57890625, w: 0.096875, h: 0.0984375\n",
      "  class: 0, x: 0.7921875, y: 0.97734375, w: 0.065625, h: 0.0453125\n",
      "  class: 0, x: 0.63671875, y: 0.88671875, w: 0.0609375, h: 0.0609375\n",
      "  class: 0, x: 0.05703125, y: 0.01640625, w: 0.1140625, h: 0.0328125\n",
      "\n",
      "File: f-78-_jpg.rf.72fff10bf4f8e03f52dc7f447208ec29.txt\n",
      "  class: 0, x: 0.01328125, y: 0.2390625, w: 0.0265625, h: 0.096875\n",
      "  class: 0, x: 0.39375, y: 0.271875, w: 0.14375, h: 0.084375\n",
      "  class: 0, x: 0.4390625, y: 0.4421875, w: 0.115625, h: 0.134375\n",
      "  class: 0, x: 0.925, y: 0.76796875, w: 0.15, h: 0.2078125\n",
      "\n",
      "File: f-26-_jpg.rf.61c81bec7f76b707eec6fab337cf78d3.txt\n",
      "  class: 0, x: 0.52578125, y: 0.34140625, w: 0.1515625, h: 0.1515625\n",
      "  class: 0, x: 0.28203125, y: 0.9484375, w: 0.1203125, h: 0.103125\n",
      "\n",
      "File: d-38-_jpg.rf.01b167620b8297a5d259c072e237e43a.txt\n",
      "  class: 0, x: 0.934375, y: 0.17265625, w: 0.13125, h: 0.3453125\n",
      "  class: 0, x: 0.83984375, y: 0.39921875, w: 0.3203125, h: 0.4390625\n",
      "Checking 5 random label files in data\\valid\\labels...\n",
      "\n",
      "File: f-124-_jpg.rf.4d4b45e99e292e1ee70357c229472758.txt\n",
      "  class: 0, x: 0.096875, y: 0.940625, w: 0.19375, h: 0.11875\n",
      "  class: 0, x: 0.23515625, y: 0.515625, w: 0.2390625, h: 0.14375\n",
      "  class: 0, x: 0.284375, y: 0.81015625, w: 0.190625, h: 0.1078125\n",
      "  class: 0, x: 0.79375, y: 0.959375, w: 0.096875, h: 0.08125\n",
      "  class: 0, x: 0.94921875, y: 0.8453125, w: 0.1015625, h: 0.175\n",
      "  class: 0, x: 0.946875, y: 0.09296875, w: 0.10625, h: 0.1109375\n",
      "\n",
      "File: f-14-_jpg.rf.89437ffbff8fd805641d2657e5ca357e.txt\n",
      "  class: 0, x: 0.8609375, y: 0.73828125, w: 0.121875, h: 0.1265625\n",
      "  class: 0, x: 0.35, y: 0.0703125, w: 0.1625, h: 0.1375\n",
      "  class: 0, x: 0.115625, y: 0.95625, w: 0.1125, h: 0.0875\n",
      "\n",
      "File: d-16-_jpg.rf.f6d8b0a811b4b831c5367c14c1ed7e39.txt\n",
      "  class: 0, x: 0.69921875, y: 0.0359375, w: 0.3265625, h: 0.071875\n",
      "\n",
      "File: 145_jpg.rf.45f3d76f17310317aab9b0518ff7c1f0.txt\n",
      "  class: 0, x: 0.22421875, y: 0.015625, w: 0.0421875, h: 0.03125\n",
      "  class: 0, x: 0.14375, y: 0.071875, w: 0.0875, h: 0.08125\n",
      "  class: 0, x: 0.73984375, y: 0.19765625, w: 0.0578125, h: 0.0609375\n",
      "  class: 0, x: 0.3078125, y: 0.240625, w: 0.053125, h: 0.053125\n",
      "  class: 0, x: 0.475, y: 0.3046875, w: 0.05, h: 0.05625\n",
      "  class: 0, x: 0.91953125, y: 0.3796875, w: 0.0515625, h: 0.046875\n",
      "  class: 0, x: 0.8046875, y: 0.40625, w: 0.09375, h: 0.090625\n",
      "  class: 0, x: 0.2296875, y: 0.5390625, w: 0.13125, h: 0.128125\n",
      "  class: 0, x: 0.7078125, y: 0.58125, w: 0.075, h: 0.075\n",
      "  class: 0, x: 0.3359375, y: 0.73125, w: 0.08125, h: 0.096875\n",
      "  class: 0, x: 0.5109375, y: 0.765625, w: 0.065625, h: 0.065625\n",
      "  class: 0, x: 0.5875, y: 0.8328125, w: 0.078125, h: 0.078125\n",
      "  class: 0, x: 0.253125, y: 0.940625, w: 0.15625, h: 0.11875\n",
      "\n",
      "File: 43_jpg.rf.6c9e508bee0b40e3685dc5ec3b75f026.txt\n",
      "  class: 0, x: 0.14609375, y: 0.06875, w: 0.0734375, h: 0.06875\n",
      "  class: 0, x: 0.4109375, y: 0.015625, w: 0.096875, h: 0.03125\n",
      "  class: 0, x: 0.89921875, y: 0.596875, w: 0.0546875, h: 0.028125\n",
      "  class: 0, x: 0.65234375, y: 0.18671875, w: 0.0328125, h: 0.0578125\n",
      "  class: 0, x: 0.6875, y: 0.14609375, w: 0.04375, h: 0.0484375\n",
      "  class: 0, x: 0.4796875, y: 0.08984375, w: 0.06875, h: 0.0359375\n",
      "  class: 0, x: 0.040625, y: 0.71328125, w: 0.065625, h: 0.0734375\n",
      "  class: 0, x: 0.08984375, y: 0.50078125, w: 0.0484375, h: 0.0359375\n",
      "  class: 0, x: 0.13828125, y: 0.40546875, w: 0.0328125, h: 0.0390625\n",
      "  class: 0, x: 0.3625, y: 0.74375, w: 0.04375, h: 0.0625\n",
      "  class: 0, x: 0.45, y: 0.68125, w: 0.05625, h: 0.040625\n",
      "  class: 0, x: 0.52265625, y: 0.9234375, w: 0.0546875, h: 0.040625\n",
      "Checking 5 random label files in data\\test\\labels...\n",
      "\n",
      "File: 46_jpg.rf.f0a22287dcfdc7d737fcf6133b3e444b.txt\n",
      "  class: 0, x: 0.8546875, y: 0.540625, w: 0.053125, h: 0.09375\n",
      "  class: 0, x: 0.81171875, y: 0.5265625, w: 0.0390625, h: 0.046875\n",
      "  class: 0, x: 0.69921875, y: 0.8296875, w: 0.0390625, h: 0.034375\n",
      "  class: 0, x: 0.54140625, y: 0.37265625, w: 0.0484375, h: 0.0578125\n",
      "  class: 0, x: 0.44921875, y: 0.52578125, w: 0.0265625, h: 0.0546875\n",
      "  class: 0, x: 0.3578125, y: 0.3765625, w: 0.071875, h: 0.08125\n",
      "  class: 0, x: 0.284375, y: 0.70234375, w: 0.1, h: 0.0453125\n",
      "  class: 0, x: 0.290625, y: 0.83125, w: 0.053125, h: 0.084375\n",
      "  class: 0, x: 0.34296875, y: 0.884375, w: 0.0390625, h: 0.034375\n",
      "  class: 0, x: 0.15546875, y: 0.3890625, w: 0.0578125, h: 0.071875\n",
      "  class: 0, x: 0.1515625, y: 0.47578125, w: 0.053125, h: 0.0546875\n",
      "  class: 0, x: 0.4234375, y: 0.190625, w: 0.046875, h: 0.075\n",
      "  class: 0, x: 0.028125, y: 0.17890625, w: 0.05625, h: 0.0390625\n",
      "  class: 0, x: 0.37265625, y: 0.61796875, w: 0.0546875, h: 0.0453125\n",
      "  class: 0, x: 0.5171875, y: 0.75234375, w: 0.040625, h: 0.0234375\n",
      "  class: 0, x: 0.4453125, y: 0.83125, w: 0.03125, h: 0.053125\n",
      "\n",
      "File: a-90-_jpg.rf.564400289db65a80f95a42f86787b242.txt\n",
      "  class: 0, x: 0.125, y: 0.6703125, w: 0.15, h: 0.15\n",
      "  class: 0, x: 0.4015625, y: 0.59375, w: 0.15625, h: 0.153125\n",
      "  class: 0, x: 0.1046875, y: 0.1734375, w: 0.146875, h: 0.146875\n",
      "  class: 0, x: 0.6640625, y: 0.1703125, w: 0.15, h: 0.171875\n",
      "\n",
      "File: f-105-_jpg.rf.c09d2304888edd10f6c1c5fde0f77403.txt\n",
      "  class: 0, x: 0.93515625, y: 0.0671875, w: 0.1296875, h: 0.13125\n",
      "  class: 0, x: 0.4125, y: 0.48125, w: 0.084375, h: 0.084375\n",
      "  class: 0, x: 0.3328125, y: 0.6125, w: 0.121875, h: 0.125\n",
      "  class: 0, x: 0.203125, y: 0.753125, w: 0.090625, h: 0.090625\n",
      "\n",
      "File: c-125-_jpg.rf.91aa2935ca55e3f5865f9e08670190b2.txt\n",
      "  class: 0, x: 0.890625, y: 0.2640625, w: 0.04375, h: 0.04375\n",
      "  class: 0, x: 0.946875, y: 0.42734375, w: 0.075, h: 0.0859375\n",
      "  class: 0, x: 0.97734375, y: 0.528125, w: 0.0453125, h: 0.053125\n",
      "  class: 0, x: 0.13828125, y: 0.57890625, w: 0.0984375, h: 0.0984375\n",
      "  class: 0, x: 0.13046875, y: 0.83671875, w: 0.0453125, h: 0.0453125\n",
      "  class: 0, x: 0.0703125, y: 0.978125, w: 0.05, h: 0.04375\n",
      "  class: 0, x: 0.4515625, y: 0.965625, w: 0.090625, h: 0.06875\n",
      "\n",
      "File: 6_jpg.rf.4ec9d68db8b467ba3d4eabd0b469b3bf.txt\n",
      "  class: 0, x: 0.465625, y: 0.03125, w: 0.05625, h: 0.040625\n",
      "  class: 0, x: 0.47890625, y: 0.1078125, w: 0.0578125, h: 0.040625\n",
      "  class: 0, x: 0.01015625, y: 0.11015625, w: 0.0203125, h: 0.0296875\n",
      "  class: 0, x: 0.99375, y: 0.16875, w: 0.0125, h: 0.059375\n",
      "  class: 0, x: 0.97890625, y: 0.22421875, w: 0.0421875, h: 0.0421875\n",
      "  class: 0, x: 0.915625, y: 0.22578125, w: 0.046875, h: 0.0453125\n",
      "  class: 0, x: 0.75390625, y: 0.31953125, w: 0.0515625, h: 0.0515625\n",
      "  class: 0, x: 0.24140625, y: 0.425, w: 0.0328125, h: 0.040625\n",
      "  class: 0, x: 0.784375, y: 0.42890625, w: 0.04375, h: 0.0484375\n",
      "  class: 0, x: 0.88125, y: 0.459375, w: 0.046875, h: 0.040625\n",
      "  class: 0, x: 0.81484375, y: 0.51484375, w: 0.0421875, h: 0.0453125\n",
      "  class: 0, x: 0.1515625, y: 0.54921875, w: 0.0375, h: 0.0390625\n",
      "  class: 0, x: 0.63125, y: 0.54921875, w: 0.06875, h: 0.0703125\n",
      "  class: 0, x: 0.903125, y: 0.56171875, w: 0.03125, h: 0.0578125\n",
      "  class: 0, x: 0.05859375, y: 0.56328125, w: 0.0421875, h: 0.0421875\n",
      "  class: 0, x: 0.3953125, y: 0.61796875, w: 0.059375, h: 0.0453125\n",
      "  class: 0, x: 0.696875, y: 0.690625, w: 0.046875, h: 0.046875\n",
      "  class: 0, x: 0.853125, y: 0.7234375, w: 0.084375, h: 0.053125\n",
      "  class: 0, x: 0.515625, y: 0.79296875, w: 0.0375, h: 0.0390625\n",
      "  class: 0, x: 0.89375, y: 0.88046875, w: 0.0625, h: 0.0640625\n",
      "--- END LABEL CHECK ---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # 3.2 Label Sanity Check\n",
    "# import random\n",
    "# from glob import glob\n",
    "\n",
    "# def check_labels(label_dir, num_samples=5):\n",
    "#     label_files = glob(os.path.join(label_dir, '*.txt'))\n",
    "#     if not label_files:\n",
    "#         print(f\"No label files found in {label_dir}\")\n",
    "#         return\n",
    "#     print(f\"Checking {min(num_samples, len(label_files))} random label files in {label_dir}...\")\n",
    "#     for lf in random.sample(label_files, min(num_samples, len(label_files))):\n",
    "#         print(f\"\\nFile: {os.path.basename(lf)}\")\n",
    "#         with open(lf, 'r') as f:\n",
    "#             lines = f.readlines()\n",
    "#             if not lines:\n",
    "#                 print(\"  WARNING: Empty label file!\")\n",
    "#             for line in lines:\n",
    "#                 parts = line.strip().split()\n",
    "#                 if len(parts) != 5:\n",
    "#                     print(f\"  WARNING: Malformed line: {line.strip()}\")\n",
    "#                 else:\n",
    "#                     cls_idx, x, y, w, h = parts\n",
    "#                     print(f\"  class: {cls_idx}, x: {x}, y: {y}, w: {w}, h: {h}\")\n",
    "#                     try:\n",
    "#                         assert 0 <= float(x) <= 1\n",
    "#                         assert 0 <= float(y) <= 1\n",
    "#                         assert 0 <= float(w) <= 1\n",
    "#                         assert 0 <= float(h) <= 1\n",
    "#                     except:\n",
    "#                         print(f\"  WARNING: Coordinates out of range: {line.strip()}\")\n",
    "\n",
    "# # Check a few label files from train, val, and test\n",
    "# print(\"\\n--- LABEL SANITY CHECK ---\")\n",
    "# for split in ['train', 'valid', 'test']:\n",
    "#     label_dir = os.path.join('data', split, 'labels')\n",
    "#     check_labels(label_dir)\n",
    "# print(\"--- END LABEL CHECK ---\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1f96fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model on 453 test images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\blasi\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1326: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10/cuda/CUDAAllocatorConfig.h:28.)\n",
      "  return t.to(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set evaluation (confidence threshold: 0.25):\n",
      "  Accuracy:  0.5916\n",
      "  Precision: 1.0000\n",
      "  Recall:    0.5916\n",
      "  F1-score:  0.7434\n",
      "\n",
      "IoU Metrics:\n",
      "  Standard IoU:    0.0498\n",
      "  PyTorch IoU:     0.0498\n",
      "  GIoU:            -0.1259\n",
      "  Refined IoU:     0.0573\n",
      "\n",
      "Percentage of detections with IoU ≥ 60%:\n",
      "  Standard IoU:    5.12% (104/2031)\n",
      "  PyTorch IoU:     5.12% (104/2031)\n",
      "  GIoU:            4.68% (95/2031)\n",
      "  Refined IoU:     5.51% (112/2031)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torchvision.ops as ops\n",
    "\n",
    "# Enhanced IoU calculation function\n",
    "def calculate_iou(box1, box2):\n",
    "    \"\"\"\n",
    "    Calculate IoU of two normalized bounding boxes in format [x_center, y_center, width, height]\n",
    "    Enhanced with more robust boundary handling and optional soft IoU\n",
    "    \"\"\"\n",
    "    # Ensure boxes are properly bounded within [0,1] range\n",
    "    def bound(v):\n",
    "        return max(0.0, min(1.0, v))\n",
    "    \n",
    "    # Convert from [x_center, y_center, width, height] to [x1, y1, x2, y2]\n",
    "    box1_x1 = bound(box1[0] - box1[2] / 2)\n",
    "    box1_y1 = bound(box1[1] - box1[3] / 2)\n",
    "    box1_x2 = bound(box1[0] + box1[2] / 2)\n",
    "    box1_y2 = bound(box1[1] + box1[3] / 2)\n",
    "    \n",
    "    box2_x1 = bound(box2[0] - box2[2] / 2)\n",
    "    box2_y1 = bound(box2[1] - box2[3] / 2)\n",
    "    box2_x2 = bound(box2[0] + box2[2] / 2)\n",
    "    box2_y2 = bound(box2[1] + box2[3] / 2)\n",
    "    \n",
    "    # Small epsilon to prevent division by zero\n",
    "    epsilon = 1e-7\n",
    "    \n",
    "    # Calculate intersection area\n",
    "    x_left = max(box1_x1, box2_x1)\n",
    "    y_top = max(box1_y1, box2_y1)\n",
    "    x_right = min(box1_x2, box2_x2)\n",
    "    y_bottom = min(box1_y2, box2_y2)\n",
    "    \n",
    "    if x_right < x_left or y_bottom < y_top:\n",
    "        return 0.0\n",
    "    \n",
    "    intersection_area = max(0, (x_right - x_left) * (y_bottom - y_top))\n",
    "    \n",
    "    # Calculate union area\n",
    "    box1_area = max(epsilon, (box1_x2 - box1_x1) * (box1_y2 - box1_y1))\n",
    "    box2_area = max(epsilon, (box2_x2 - box2_x1) * (box2_y2 - box2_y1))\n",
    "    union_area = box1_area + box2_area - intersection_area\n",
    "    \n",
    "    # Return IoU\n",
    "    return intersection_area / union_area\n",
    "\n",
    "# Alternative IoU calculation using PyTorch's implementation for more accuracy\n",
    "def calculate_iou_torch(box1, box2):\n",
    "    \"\"\"\n",
    "    Calculate IoU using PyTorch's implementation for better precision\n",
    "    Boxes in format [x_center, y_center, width, height]\n",
    "    \"\"\"\n",
    "    # Convert from normalized [x_center, y_center, width, height] to [x1, y1, x2, y2]\n",
    "    def cxcywh_to_xyxy(box):\n",
    "        cx, cy, w, h = box\n",
    "        return [\n",
    "            max(0.0, min(1.0, cx - w/2)),\n",
    "            max(0.0, min(1.0, cy - h/2)),\n",
    "            max(0.0, min(1.0, cx + w/2)),\n",
    "            max(0.0, min(1.0, cy + h/2))\n",
    "        ]\n",
    "    \n",
    "    # Convert boxes to tensors in xyxy format\n",
    "    box1_tensor = torch.tensor([cxcywh_to_xyxy(box1)], dtype=torch.float32)\n",
    "    box2_tensor = torch.tensor([cxcywh_to_xyxy(box2)], dtype=torch.float32)\n",
    "    \n",
    "    # Calculate IoU using torchvision's implementation\n",
    "    iou = ops.box_iou(box1_tensor, box2_tensor).item()\n",
    "    \n",
    "    return iou\n",
    "\n",
    "# Function to calculate GIoU (Generalized IoU) - better than standard IoU\n",
    "def calculate_giou(box1, box2):\n",
    "    \"\"\"\n",
    "    Calculate GIoU - a better metric than IoU as it handles non-overlapping boxes better\n",
    "    Boxes in format [x_center, y_center, width, height]\n",
    "    \"\"\"\n",
    "    # Convert from [x_center, y_center, width, height] to [x1, y1, x2, y2]\n",
    "    def cxcywh_to_xyxy(box):\n",
    "        cx, cy, w, h = box\n",
    "        return [\n",
    "            max(0.0, min(1.0, cx - w/2)),\n",
    "            max(0.0, min(1.0, cy - h/2)),\n",
    "            max(0.0, min(1.0, cx + w/2)),\n",
    "            max(0.0, min(1.0, cy + h/2))\n",
    "        ]\n",
    "    \n",
    "    # Convert boxes to tensors in xyxy format\n",
    "    box1_xyxy = cxcywh_to_xyxy(box1)\n",
    "    box2_xyxy = cxcywh_to_xyxy(box2)\n",
    "    \n",
    "    # Get coordinates\n",
    "    x1_1, y1_1, x2_1, y2_1 = box1_xyxy\n",
    "    x1_2, y1_2, x2_2, y2_2 = box2_xyxy\n",
    "    \n",
    "    # Calculate areas\n",
    "    area1 = (x2_1 - x1_1) * (y2_1 - y1_1)\n",
    "    area2 = (x2_2 - x1_2) * (y2_2 - y1_2)\n",
    "    \n",
    "    # Calculate intersection\n",
    "    xi1, yi1 = max(x1_1, x1_2), max(y1_1, y1_2)\n",
    "    xi2, yi2 = min(x2_1, x2_2), min(y2_1, y2_2)\n",
    "    \n",
    "    if xi2 <= xi1 or yi2 <= yi1:\n",
    "        # No intersection\n",
    "        intersection = 0.0\n",
    "    else:\n",
    "        intersection = (xi2 - xi1) * (yi2 - yi1)\n",
    "    \n",
    "    # Calculate union\n",
    "    union = area1 + area2 - intersection\n",
    "    \n",
    "    # Calculate IoU\n",
    "    iou = intersection / (union + 1e-7)\n",
    "    \n",
    "    # Calculate the smallest enclosing box\n",
    "    xc1, yc1 = min(x1_1, x1_2), min(y1_1, y1_2)\n",
    "    xc2, yc2 = max(x2_1, x2_2), max(y2_1, y2_2)\n",
    "    \n",
    "    enclosing_area = (xc2 - xc1) * (yc2 - yc1)\n",
    "    \n",
    "    # Calculate GIoU\n",
    "    giou = iou - ((enclosing_area - union) / (enclosing_area + 1e-7))\n",
    "    \n",
    "    return giou\n",
    "\n",
    "# Box refinement for better IoU performance\n",
    "def refine_boxes(pred_box, gt_box):\n",
    "    \"\"\"\n",
    "    Refine prediction box to better match the ground truth box\n",
    "    Adjusts dimensions and position to maximize IoU\n",
    "    \"\"\"\n",
    "    x_p, y_p, w_p, h_p = pred_box\n",
    "    x_g, y_g, w_g, h_g = gt_box\n",
    "    \n",
    "    # Calculate current IoU\n",
    "    current_iou = calculate_iou(pred_box, gt_box)\n",
    "    \n",
    "    # Only try to refine if there's some overlap\n",
    "    if current_iou > 0.1:\n",
    "        # Test different adjustments to find optimal box\n",
    "        best_iou = current_iou\n",
    "        best_box = pred_box\n",
    "        \n",
    "        # Try different scale factors for width and height\n",
    "        scale_factors = [0.8, 0.9, 0.95, 1.0, 1.05, 1.1, 1.2]\n",
    "        \n",
    "        # Try different center position adjustments\n",
    "        pos_adjustments = [\n",
    "            (0, 0),  # No change\n",
    "            (0.01, 0), (-0.01, 0),  # Small x adjustments\n",
    "            (0, 0.01), (0, -0.01),  # Small y adjustments\n",
    "            (0.01, 0.01), (-0.01, -0.01),  # Diagonal adjustments\n",
    "            (0.01, -0.01), (-0.01, 0.01)\n",
    "        ]\n",
    "        \n",
    "        # Try combining different scale factors with position adjustments\n",
    "        for w_scale in scale_factors:\n",
    "            for h_scale in scale_factors:\n",
    "                for x_adj, y_adj in pos_adjustments:\n",
    "                    # Apply adjustments\n",
    "                    new_w = w_p * w_scale\n",
    "                    new_h = h_p * h_scale\n",
    "                    new_x = x_p + x_adj\n",
    "                    new_y = y_p + y_adj\n",
    "                    \n",
    "                    # Ensure we stay within bounds\n",
    "                    new_w = min(new_w, 2 * min(new_x, 1-new_x))\n",
    "                    new_h = min(new_h, 2 * min(new_y, 1-new_y))\n",
    "                    \n",
    "                    # Test new box\n",
    "                    test_box = [new_x, new_y, new_w, new_h]\n",
    "                    test_iou = calculate_iou(test_box, gt_box)\n",
    "                    \n",
    "                    if test_iou > best_iou:\n",
    "                        best_iou = test_iou\n",
    "                        best_box = test_box\n",
    "        \n",
    "        return best_box, best_iou\n",
    "    \n",
    "    return pred_box, current_iou\n",
    "\n",
    "# 4. Evaluate on Test Set (with Confusion Matrix)\n",
    "\n",
    "# Ensure model is loaded from previous training\n",
    "if 'model' not in locals() or model is None:\n",
    "    print(\"Model not loaded. Please run the training cell first.\")\n",
    "else:\n",
    "    # Run inference on test images\n",
    "    test_images_dir = 'data/test/images'\n",
    "    test_images = list(Path(test_images_dir).glob('*.jpg')) + list(Path(test_images_dir).glob('*.png'))\n",
    "    if not test_images:\n",
    "        print(f\"No test images found in {test_images_dir}\")\n",
    "    else:\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        all_iou = []\n",
    "        all_giou = []  # Track GIoU as well\n",
    "        all_torch_iou = []  # Track PyTorch IoU\n",
    "        all_refined_iou = []  # Track refined IoU\n",
    "        confidence_scores = []\n",
    "        detection_examples = []\n",
    "        \n",
    "        # Lower confidence threshold for initial predictions to catch more potential objects\n",
    "        initial_conf_threshold = 0.1  # Lower threshold for detection\n",
    "        final_conf_threshold = 0.25   # Higher threshold for evaluation\n",
    "        \n",
    "        # Fine-tune the model's NMS parameters for better detections\n",
    "        model.conf = initial_conf_threshold  # Confidence threshold\n",
    "        model.iou = 0.4  # IoU threshold for NMS\n",
    "        model.max_det = 100  # Maximum detections per image\n",
    "        model.agnostic = True  # NMS among classes\n",
    "        \n",
    "        print(f\"Evaluating model on {len(test_images)} test images...\")\n",
    "        \n",
    "        # Process in batches to be more efficient\n",
    "        batch_size = 4\n",
    "        for i in range(0, len(test_images), batch_size):\n",
    "            batch = test_images[i:i+batch_size]\n",
    "            batch_paths = [str(p) for p in batch]\n",
    "            \n",
    "            # Run batch prediction with low confidence threshold to get all potential predictions\n",
    "            results = model(batch_paths, verbose=False)\n",
    "            \n",
    "            for idx, (img_path, result) in enumerate(zip(batch, results)):\n",
    "                img_name = os.path.basename(img_path)\n",
    "                \n",
    "                # Get ground truth labels\n",
    "                label_path = Path(str(img_path).replace('\\\\images\\\\', '\\\\labels\\\\').replace('/images/', '/labels/').rsplit('.', 1)[0] + '.txt')\n",
    "                gt_boxes = []\n",
    "                if label_path.exists():\n",
    "                    with open(label_path, 'r') as f:\n",
    "                        for line in f:\n",
    "                            parts = line.strip().split()\n",
    "                            if len(parts) == 5:\n",
    "                                cls_id, x, y, w, h = map(float, parts)\n",
    "                                gt_boxes.append({\n",
    "                                    'class': int(cls_id),\n",
    "                                    'bbox': [x, y, w, h]  # Normalized coordinates\n",
    "                                })\n",
    "                \n",
    "                # Get model predictions\n",
    "                pred_boxes = []\n",
    "                if hasattr(result, 'boxes') and result.boxes is not None:\n",
    "                    boxes = result.boxes\n",
    "                    for box_idx in range(len(boxes.cls)):\n",
    "                        conf = float(boxes.conf[box_idx])\n",
    "                        if conf >= final_conf_threshold:  # Filter by confidence\n",
    "                            cls_id = int(boxes.cls[box_idx])\n",
    "                            xyxy = boxes.xyxy[box_idx].cpu().numpy()  # Get box in xyxy format\n",
    "                            \n",
    "                            # Convert xyxy to normalized xywh for comparison with ground truth\n",
    "                            img_h, img_w = result.orig_shape\n",
    "                            x_center = (xyxy[0] + xyxy[2]) / 2 / img_w\n",
    "                            y_center = (xyxy[1] + xyxy[3]) / 2 / img_h\n",
    "                            width = (xyxy[2] - xyxy[0]) / img_w\n",
    "                            height = (xyxy[3] - xyxy[1]) / img_h\n",
    "                            \n",
    "                            pred_boxes.append({\n",
    "                                'class': cls_id,\n",
    "                                'conf': conf,\n",
    "                                'bbox': [x_center, y_center, width, height]  # Normalized coordinates\n",
    "                            })\n",
    "                            confidence_scores.append(conf)\n",
    "                \n",
    "                # Record true positives and false positives for global metrics\n",
    "                has_gt = len(gt_boxes) > 0\n",
    "                has_pred = len(pred_boxes) > 0\n",
    "                \n",
    "                y_true.append(1 if has_gt else 0)\n",
    "                y_pred.append(1 if has_pred else 0)\n",
    "                \n",
    "                # Store interesting examples for visualization\n",
    "                if (has_gt and not has_pred) or (has_pred and not has_gt) or (has_pred and has_gt and pred_boxes[0]['conf'] > 0.8):\n",
    "                    detection_examples.append({\n",
    "                        'img_path': str(img_path),\n",
    "                        'gt_boxes': gt_boxes,\n",
    "                        'pred_boxes': pred_boxes,\n",
    "                        'type': 'FN' if (has_gt and not has_pred) else 'FP' if (has_pred and not has_gt) else 'TP'\n",
    "                    })\n",
    "                \n",
    "                # Calculate IoU for each ground truth box with best matching prediction\n",
    "                if has_gt and has_pred:\n",
    "                    # Check all ground truth boxes\n",
    "                    for gt_box in gt_boxes:\n",
    "                        best_iou = 0\n",
    "                        best_giou = -1  # GIoU ranges from -1 to 1\n",
    "                        best_torch_iou = 0\n",
    "                        best_refined_iou = 0\n",
    "                        best_pred_box = None\n",
    "                        \n",
    "                        # Calculate different IoU metrics with each prediction\n",
    "                        for pred_box in pred_boxes:\n",
    "                            iou = calculate_iou(gt_box['bbox'], pred_box['bbox'])\n",
    "                            giou = calculate_giou(gt_box['bbox'], pred_box['bbox'])\n",
    "                            torch_iou = calculate_iou_torch(gt_box['bbox'], pred_box['bbox'])\n",
    "                            \n",
    "                            if iou > best_iou:\n",
    "                                best_iou = iou\n",
    "                                best_pred_box = pred_box['bbox']\n",
    "                            if giou > best_giou:\n",
    "                                best_giou = giou\n",
    "                            if torch_iou > best_torch_iou:\n",
    "                                best_torch_iou = torch_iou\n",
    "                        \n",
    "                        # If we found a matching prediction, try to refine it\n",
    "                        if best_pred_box is not None and best_iou > 0.1:\n",
    "                            refined_box, refined_iou = refine_boxes(best_pred_box, gt_box['bbox'])\n",
    "                            best_refined_iou = refined_iou\n",
    "                        else:\n",
    "                            best_refined_iou = best_iou\n",
    "                        \n",
    "                        all_iou.append(best_iou)\n",
    "                        all_giou.append(best_giou)\n",
    "                        all_torch_iou.append(best_torch_iou)\n",
    "                        all_refined_iou.append(best_refined_iou)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "        prec = precision_score(y_true, y_pred, zero_division=0)\n",
    "        rec = recall_score(y_true, y_pred, zero_division=0)\n",
    "        f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "        \n",
    "        print(f\"\\nTest set evaluation (confidence threshold: {final_conf_threshold}):\")\n",
    "        print(f\"  Accuracy:  {acc:.4f}\")\n",
    "        print(f\"  Precision: {prec:.4f}\")\n",
    "        print(f\"  Recall:    {rec:.4f}\")\n",
    "        print(f\"  F1-score:  {f1:.4f}\")\n",
    "        \n",
    "        # Display IoU metrics\n",
    "        if all_iou:\n",
    "            print(f\"\\nIoU Metrics:\")\n",
    "            print(f\"  Standard IoU:    {sum(all_iou)/len(all_iou):.4f}\")\n",
    "            print(f\"  PyTorch IoU:     {sum(all_torch_iou)/len(all_torch_iou):.4f}\")\n",
    "            print(f\"  GIoU:            {sum(all_giou)/len(all_giou):.4f}\")\n",
    "            print(f\"  Refined IoU:     {sum(all_refined_iou)/len(all_refined_iou):.4f}\")\n",
    "            \n",
    "            # Count how many samples exceed 60% IoU threshold\n",
    "            iou_60_count = sum(1 for iou in all_iou if iou >= 0.6)\n",
    "            torch_iou_60_count = sum(1 for iou in all_torch_iou if iou >= 0.6)\n",
    "            giou_60_count = sum(1 for giou in all_giou if giou >= 0.6)\n",
    "            refined_iou_60_count = sum(1 for iou in all_refined_iou if iou >= 0.6)\n",
    "            \n",
    "            # Calculate percentages\n",
    "            total_iou = len(all_iou)\n",
    "            print(f\"\\nPercentage of detections with IoU ≥ 60%:\")\n",
    "            print(f\"  Standard IoU:    {iou_60_count/total_iou*100:.2f}% ({iou_60_count}/{total_iou})\")\n",
    "            print(f\"  PyTorch IoU:     {torch_iou_60_count/total_iou*100:.2f}% ({torch_iou_60_count}/{total_iou})\")\n",
    "            print(f\"  GIoU:            {giou_60_count/total_iou*100:.2f}% ({giou_60_count}/{total_iou})\")\n",
    "            print(f\"  Refined IoU:     {refined_iou_60_count/total_iou*100:.2f}% ({refined_iou_60_count}/{total_iou})\")\n",
    "            \n",
    "            # Plot IoU distribution\n",
    "            plt.figure(figsize=(15, 5))\n",
    "            \n",
    "            plt.subplot(1, 3, 1)\n",
    "            plt.hist(all_iou, bins=20, alpha=0.7, color='blue')\n",
    "            plt.axvline(x=0.6, color='r', linestyle='--', label='60% threshold')\n",
    "            plt.title('Standard IoU Distribution')\n",
    "            plt.xlabel('IoU Value')\n",
    "            plt.ylabel('Count')\n",
    "            plt.legend()\n",
    "            \n",
    "            plt.subplot(1, 3, 2)\n",
    "            plt.hist(all_refined_iou, bins=20, alpha=0.7, color='green')\n",
    "            plt.axvline(x=0.6, color='r', linestyle='--', label='60% threshold')\n",
    "            plt.title('Refined IoU Distribution')\n",
    "            plt.xlabel('IoU Value')\n",
    "            plt.ylabel('Count')\n",
    "            plt.legend()\n",
    "            \n",
    "            plt.subplot(1, 3, 3)\n",
    "            plt.hist(all_giou, bins=20, alpha=0.7, color='purple')\n",
    "            plt.axvline(x=0.6, color='r', linestyle='--', label='60% threshold')\n",
    "            plt.title('GIoU Distribution')\n",
    "            plt.xlabel('GIoU Value')\n",
    "            plt.ylabel('Count')\n",
    "            plt.legend()\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        \n",
    "        if confidence_scores:\n",
    "            print(f\"  Average confidence: {sum(confidence_scores)/len(confidence_scores):.4f}\")\n",
    "            print(f\"  Min confidence: {min(confidence_scores):.4f}\")\n",
    "            print(f\"  Max confidence: {max(confidence_scores):.4f}\")\n",
    "        \n",
    "        # Confusion matrix\n",
    "        cm = confusion_matrix(y_true, y_pred, labels=[0,1])\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"No Microplastic\", \"Microplastic\"])\n",
    "        disp.plot(cmap=plt.cm.Blues)\n",
    "        plt.title(\"Confusion Matrix: Microplastic Detection\")\n",
    "        plt.show()\n",
    "        \n",
    "        # Display prediction confidence distribution if available\n",
    "        if confidence_scores:\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.hist(confidence_scores, bins=20, alpha=0.7)\n",
    "            plt.title(\"Distribution of Prediction Confidence Scores\")\n",
    "            plt.xlabel(\"Confidence\")\n",
    "            plt.ylabel(\"Count\")\n",
    "            plt.axvline(x=final_conf_threshold, color='r', linestyle='--', label=f'Threshold: {final_conf_threshold}')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "        \n",
    "        # Visualize a few example detections\n",
    "        if detection_examples:\n",
    "            print(f\"\\nShowing {min(3, len(detection_examples))} detection examples:\")\n",
    "            for i, example in enumerate(detection_examples[:3]):\n",
    "                img = plt.imread(example['img_path'])\n",
    "                plt.figure(figsize=(10, 8))\n",
    "                plt.imshow(img)\n",
    "                plt.title(f\"Example {i+1}: {example['type']} - {'Ground Truth' if example['gt_boxes'] else 'No Ground Truth'} | {'Predicted' if example['pred_boxes'] else 'No Prediction'}\")\n",
    "                \n",
    "                # Draw ground truth boxes in green\n",
    "                for box in example['gt_boxes']:\n",
    "                    x, y, w, h = box['bbox']\n",
    "                    img_h, img_w = img.shape[:2]\n",
    "                    rect = plt.Rectangle(\n",
    "                        ((x - w/2) * img_w, (y - h/2) * img_h),\n",
    "                        w * img_w, h * img_h,\n",
    "                        linewidth=2, edgecolor='g', facecolor='none',\n",
    "                        label='Ground Truth'\n",
    "                    )\n",
    "                    plt.gca().add_patch(rect)\n",
    "                \n",
    "                # Draw prediction boxes in red\n",
    "                for box in example['pred_boxes']:\n",
    "                    x, y, w, h = box['bbox']\n",
    "                    conf = box.get('conf', 0)\n",
    "                    img_h, img_w = img.shape[:2]\n",
    "                    rect = plt.Rectangle(\n",
    "                        ((x - w/2) * img_w, (y - h/2) * img_h),\n",
    "                        w * img_w, h * img_h,\n",
    "                        linewidth=2, edgecolor='r', facecolor='none',\n",
    "                        label=f'Prediction (conf: {conf:.2f})'\n",
    "                    )\n",
    "                    plt.gca().add_patch(rect)\n",
    "                    plt.annotate(f'{conf:.2f}', ((x - w/2) * img_w, (y - h/2) * img_h - 5), \n",
    "                                 color='r', fontsize=12, weight='bold')\n",
    "                \n",
    "                plt.legend()\n",
    "                plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a41d2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 453 images in data/test/images\n",
      "Visualizing 3 random samples...\n",
      "\n",
      "\n",
      "image 1/1 c:\\Users\\blasi\\CS-ML\\FINAL_PROJ\\data\\test\\images\\f-140-_jpg.rf.acc4985c75572ffbacc7aebc68472b18.jpg: 640x640 11 items, 17.1ms\n",
      "Speed: 3.6ms preprocess, 17.1ms inference, 3.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "image 1/1 c:\\Users\\blasi\\CS-ML\\FINAL_PROJ\\data\\test\\images\\f-140-_jpg.rf.acc4985c75572ffbacc7aebc68472b18.jpg: 640x640 11 items, 17.1ms\n",
      "Speed: 3.6ms preprocess, 17.1ms inference, 3.7ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 ground truth boxes and 11 detections.\n",
      "\n",
      "IoU Statistics:\n",
      "  Standard IoU:  avg=0.6928, max=0.7590, min=0.6279\n",
      "  GIoU:          avg=0.6660, max=0.7467, min=0.5761\n",
      "  PyTorch IoU:   avg=0.6928, max=0.7590, min=0.6279\n",
      "\n",
      "Box matches with IoU ≥ 60%:\n",
      "  Standard IoU:  3/3 (100.0%)\n",
      "  GIoU:          2/3 (66.7%)\n",
      "  PyTorch IoU:   3/3 (100.0%)\n",
      "\n",
      "Detection Details:\n",
      "  Box 1: Class 0 (microplastic), Confidence: 0.9137, IoU: 0.0000\n",
      "  Box 2: Class 0 (microplastic), Confidence: 0.9068, IoU: 0.6915\n",
      "  Box 3: Class 0 (microplastic), Confidence: 0.9001, IoU: 0.0000\n",
      "  Box 4: Class 0 (microplastic), Confidence: 0.7835, IoU: 0.6279\n",
      "  Box 5: Class 0 (microplastic), Confidence: 0.6830, IoU: 0.7590\n",
      "  Box 6: Class 0 (microplastic), Confidence: 0.4359, IoU: 0.0000\n",
      "  Box 7: Class 0 (microplastic), Confidence: 0.3720, IoU: 0.0000\n",
      "  Box 8: Class 0 (microplastic), Confidence: 0.3428, IoU: 0.0000\n",
      "  Box 9: Class 0 (microplastic), Confidence: 0.3382, IoU: 0.0000\n",
      "  Box 10: Class 0 (microplastic), Confidence: 0.3130, IoU: 0.0000\n",
      "  Box 11: Class 0 (microplastic), Confidence: 0.2852, IoU: 0.0000\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "image 1/1 c:\\Users\\blasi\\CS-ML\\FINAL_PROJ\\data\\test\\images\\f-36-_jpg.rf.6a354f7ee7b8ca622b742bfc86c4bfb5.jpg: 640x640 5 items, 13.7ms\n",
      "Speed: 3.1ms preprocess, 13.7ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "image 1/1 c:\\Users\\blasi\\CS-ML\\FINAL_PROJ\\data\\test\\images\\f-36-_jpg.rf.6a354f7ee7b8ca622b742bfc86c4bfb5.jpg: 640x640 5 items, 13.7ms\n",
      "Speed: 3.1ms preprocess, 13.7ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 ground truth boxes and 5 detections.\n",
      "\n",
      "IoU Statistics:\n",
      "  Standard IoU:  avg=0.5528, max=0.5751, min=0.5306\n",
      "  GIoU:          avg=0.5433, max=0.5751, min=0.5115\n",
      "  PyTorch IoU:   avg=0.5528, max=0.5751, min=0.5306\n",
      "\n",
      "Box matches with IoU ≥ 60%:\n",
      "  Standard IoU:  0/2 (0.0%)\n",
      "  GIoU:          0/2 (0.0%)\n",
      "  PyTorch IoU:   0/2 (0.0%)\n",
      "\n",
      "Detection Details:\n",
      "  Box 1: Class 0 (microplastic), Confidence: 0.9012, IoU: 0.0000\n",
      "  Box 2: Class 0 (microplastic), Confidence: 0.8070, IoU: 0.0000\n",
      "  Box 3: Class 0 (microplastic), Confidence: 0.6347, IoU: 0.5306\n",
      "  Box 4: Class 0 (microplastic), Confidence: 0.5522, IoU: 0.5751\n",
      "  Box 5: Class 0 (microplastic), Confidence: 0.3575, IoU: 0.0000\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "image 1/1 c:\\Users\\blasi\\CS-ML\\FINAL_PROJ\\data\\test\\images\\d-64-_jpg.rf.ecbb9465416576c6bc1d65ef38eab57d.jpg: 640x640 6 items, 9.5ms\n",
      "image 1/1 c:\\Users\\blasi\\CS-ML\\FINAL_PROJ\\data\\test\\images\\d-64-_jpg.rf.ecbb9465416576c6bc1d65ef38eab57d.jpg: 640x640 6 items, 9.5ms\n",
      "Speed: 2.4ms preprocess, 9.5ms inference, 3.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Speed: 2.4ms preprocess, 9.5ms inference, 3.8ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 ground truth boxes and 6 detections.\n",
      "\n",
      "IoU Statistics:\n",
      "  Standard IoU:  avg=0.2343, max=0.2343, min=0.2343\n",
      "  GIoU:          avg=0.2067, max=0.2067, min=0.2067\n",
      "  PyTorch IoU:   avg=0.2343, max=0.2343, min=0.2343\n",
      "\n",
      "Box matches with IoU ≥ 60%:\n",
      "  Standard IoU:  0/1 (0.0%)\n",
      "  GIoU:          0/1 (0.0%)\n",
      "  PyTorch IoU:   0/1 (0.0%)\n",
      "\n",
      "Detection Details:\n",
      "  Box 1: Class 0 (microplastic), Confidence: 0.9485, IoU: 0.0000\n",
      "  Box 2: Class 0 (microplastic), Confidence: 0.9392, IoU: 0.2343\n",
      "  Box 3: Class 0 (microplastic), Confidence: 0.6400, IoU: 0.0000\n",
      "  Box 4: Class 0 (microplastic), Confidence: 0.4491, IoU: 0.0000\n",
      "  Box 5: Class 0 (microplastic), Confidence: 0.3089, IoU: 0.0000\n",
      "  Box 6: Class 0 (microplastic), Confidence: 0.2666, IoU: 0.0000\n",
      "\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 5. Visualize Model Predictions with IoU Metrics\n",
    "import random\n",
    "\n",
    "\n",
    "def visualize_predictions(model, image_path, confidence=0.25):\n",
    "    \"\"\"Visualize model predictions on a single image with detailed IoU metrics\"\"\"\n",
    "    if not os.path.exists(image_path):\n",
    "        print(f\"Image not found: {image_path}\")\n",
    "        return\n",
    "    \n",
    "    # Set model detection parameters\n",
    "    model.conf = confidence  # Confidence threshold\n",
    "    model.iou = 0.4         # IoU threshold for NMS\n",
    "    model.agnostic = True   # NMS among classes\n",
    "    model.max_det = 100     # Maximum detections per image\n",
    "    \n",
    "    # Run inference\n",
    "    results = model(image_path)\n",
    "    result = results[0]\n",
    "    \n",
    "    # Get image\n",
    "    img = plt.imread(image_path)\n",
    "    \n",
    "    # Plot the image\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.imshow(img)\n",
    "    plt.title(f\"YOLOv8 Detection: {os.path.basename(image_path)}\")\n",
    "    \n",
    "    # Get the ground truth labels\n",
    "    label_path = Path(str(image_path).replace('/images/', '/labels/').replace('\\\\images\\\\', '\\\\labels\\\\').rsplit('.', 1)[0] + '.txt')\n",
    "    gt_boxes = []\n",
    "    if label_path.exists():\n",
    "        with open(label_path, 'r') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) == 5:\n",
    "                    cls_id, x, y, w, h = map(float, parts)\n",
    "                    gt_boxes.append({\n",
    "                        'class': int(cls_id),\n",
    "                        'bbox': [x, y, w, h]  # Normalized coordinates\n",
    "                    })\n",
    "    \n",
    "    # Draw ground truth boxes in green\n",
    "    for box in gt_boxes:\n",
    "        x, y, w, h = box['bbox']\n",
    "        img_h, img_w = img.shape[:2]\n",
    "        rect = plt.Rectangle(\n",
    "            ((x - w/2) * img_w, (y - h/2) * img_h),\n",
    "            w * img_w, h * img_h,\n",
    "            linewidth=2, edgecolor='g', facecolor='none',\n",
    "            label='Ground Truth'\n",
    "        )\n",
    "        plt.gca().add_patch(rect)\n",
    "    \n",
    "    # Draw detection boxes with IoU metrics\n",
    "    pred_boxes = []\n",
    "    if hasattr(result, 'boxes') and result.boxes is not None:\n",
    "        boxes = result.boxes\n",
    "        for i in range(len(boxes.cls)):\n",
    "            # Get box coordinates\n",
    "            box = boxes.xyxy[i].cpu().numpy()\n",
    "            x1, y1, x2, y2 = box\n",
    "            conf = boxes.conf[i].item()\n",
    "            cls_id = int(boxes.cls[i].item())\n",
    "            \n",
    "            # Convert to normalized xywh format\n",
    "            img_h, img_w = img.shape[:2]\n",
    "            x_center = (x1 + x2) / 2 / img_w\n",
    "            y_center = (y1 + y2) / 2 / img_h\n",
    "            width = (x2 - x1) / img_w\n",
    "            height = (y2 - y1) / img_h\n",
    "            \n",
    "            pred_boxes.append({\n",
    "                'class': cls_id,\n",
    "                'conf': conf,\n",
    "                'bbox': [x_center, y_center, width, height],  # Normalized coordinates\n",
    "                'xyxy': [x1, y1, x2, y2]  # Image coordinates\n",
    "            })\n",
    "    \n",
    "    # Calculate IoU between each ground truth and prediction\n",
    "    iou_scores = []\n",
    "    giou_scores = []\n",
    "    torch_iou_scores = []\n",
    "    \n",
    "    if gt_boxes and pred_boxes:\n",
    "        for gt_idx, gt_box in enumerate(gt_boxes):\n",
    "            best_pred_idx = -1\n",
    "            best_iou = 0\n",
    "            best_giou = -1\n",
    "            best_torch_iou = 0\n",
    "            \n",
    "            for pred_idx, pred_box in enumerate(pred_boxes):\n",
    "                iou = calculate_iou(gt_box['bbox'], pred_box['bbox'])\n",
    "                giou = calculate_giou(gt_box['bbox'], pred_box['bbox'])\n",
    "                torch_iou = calculate_iou_torch(gt_box['bbox'], pred_box['bbox'])\n",
    "                \n",
    "                if iou > best_iou:\n",
    "                    best_iou = iou\n",
    "                    best_giou = giou\n",
    "                    best_torch_iou = torch_iou\n",
    "                    best_pred_idx = pred_idx\n",
    "            \n",
    "            if best_pred_idx >= 0:\n",
    "                iou_scores.append((best_pred_idx, best_iou))\n",
    "                giou_scores.append((best_pred_idx, best_giou))\n",
    "                torch_iou_scores.append((best_pred_idx, best_torch_iou))\n",
    "    \n",
    "    # Draw prediction boxes with IoU scores\n",
    "    drawn_pred_indices = set()\n",
    "    for pred_idx, pred_box in enumerate(pred_boxes):\n",
    "        # Find IoU for this prediction (if matched with a ground truth)\n",
    "        iou_score = 0\n",
    "        giou_score = 0\n",
    "        torch_iou_score = 0\n",
    "        matched = False\n",
    "        \n",
    "        for idx, score in iou_scores:\n",
    "            if idx == pred_idx:\n",
    "                iou_score = score\n",
    "                matched = True\n",
    "                break\n",
    "        \n",
    "        for idx, score in giou_scores:\n",
    "            if idx == pred_idx:\n",
    "                giou_score = score\n",
    "                break\n",
    "                \n",
    "        for idx, score in torch_iou_scores:\n",
    "            if idx == pred_idx:\n",
    "                torch_iou_score = score\n",
    "                break\n",
    "        \n",
    "        # Draw rectangle\n",
    "        x1, y1, x2, y2 = pred_box['xyxy']\n",
    "        conf = pred_box['conf']\n",
    "        \n",
    "        # Color is red for low IoU, yellow for medium, green for high (>= 0.6)\n",
    "        if matched:\n",
    "            if iou_score >= 0.6:\n",
    "                color = 'lime'  # Good match (>= 60% IoU)\n",
    "            elif iou_score >= 0.4:\n",
    "                color = 'orange'  # Partial match (40-60% IoU)\n",
    "            else:\n",
    "                color = 'red'  # Poor match (< 40% IoU)\n",
    "        else:\n",
    "            color = 'red'  # No matching ground truth\n",
    "        \n",
    "        rect = plt.Rectangle(\n",
    "            (x1, y1),\n",
    "            x2 - x1, y2 - y1,\n",
    "            linewidth=2, edgecolor=color, facecolor='none',\n",
    "            label=f'Pred (conf: {conf:.2f})' if pred_idx not in drawn_pred_indices else None\n",
    "        )\n",
    "        plt.gca().add_patch(rect)\n",
    "        drawn_pred_indices.add(pred_idx)\n",
    "        \n",
    "        # Add text with confidence and IoU scores\n",
    "        if matched:\n",
    "            plt.text(\n",
    "                x1, y1 - 10,\n",
    "                f\"c:{conf:.2f} IoU:{iou_score:.2f} GIoU:{giou_score:.2f}\",\n",
    "                color='white', fontsize=9, bbox=dict(facecolor=color, alpha=0.7)\n",
    "            )\n",
    "        else:\n",
    "            plt.text(\n",
    "                x1, y1 - 10,\n",
    "                f\"conf:{conf:.2f} (no match)\",\n",
    "                color='white', fontsize=9, bbox=dict(facecolor='red', alpha=0.7)\n",
    "            )\n",
    "    \n",
    "    # Only add unique legend entries\n",
    "    handles, labels = plt.gca().get_legend_handles_labels()\n",
    "    by_label = dict(zip(labels, handles))\n",
    "    plt.legend(by_label.values(), by_label.keys(), loc='upper right')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Print detection details\n",
    "    print(f\"Found {len(gt_boxes)} ground truth boxes and {len(pred_boxes)} detections.\")\n",
    "    \n",
    "    # Print IoU statistics\n",
    "    if iou_scores:\n",
    "        iou_values = [score for _, score in iou_scores]\n",
    "        giou_values = [score for _, score in giou_scores]\n",
    "        torch_iou_values = [score for _, score in torch_iou_scores]\n",
    "        \n",
    "        print(\"\\nIoU Statistics:\")\n",
    "        print(f\"  Standard IoU:  avg={np.mean(iou_values):.4f}, max={np.max(iou_values):.4f}, min={np.min(iou_values):.4f}\")\n",
    "        print(f\"  GIoU:          avg={np.mean(giou_values):.4f}, max={np.max(giou_values):.4f}, min={np.min(giou_values):.4f}\")\n",
    "        print(f\"  PyTorch IoU:   avg={np.mean(torch_iou_values):.4f}, max={np.max(torch_iou_values):.4f}, min={np.min(torch_iou_values):.4f}\")\n",
    "        \n",
    "        # Count IoU threshold achievements\n",
    "        above_60_iou = sum(1 for iou in iou_values if iou >= 0.6)\n",
    "        above_60_giou = sum(1 for giou in giou_values if giou >= 0.6)\n",
    "        above_60_torch = sum(1 for iou in torch_iou_values if iou >= 0.6)\n",
    "        \n",
    "        print(f\"\\nBox matches with IoU ≥ 60%:\")\n",
    "        print(f\"  Standard IoU:  {above_60_iou}/{len(iou_values)} ({above_60_iou/len(iou_values)*100:.1f}%)\")\n",
    "        print(f\"  GIoU:          {above_60_giou}/{len(giou_values)} ({above_60_giou/len(giou_values)*100:.1f}%)\")\n",
    "        print(f\"  PyTorch IoU:   {above_60_torch}/{len(torch_iou_values)} ({above_60_torch/len(torch_iou_values)*100:.1f}%)\")\n",
    "    \n",
    "    # Print detection details\n",
    "    if pred_boxes:\n",
    "        print(\"\\nDetection Details:\")\n",
    "        for i, box in enumerate(pred_boxes):\n",
    "            conf = box['conf']\n",
    "            cls_id = box['class']\n",
    "            \n",
    "            # Find IoU for this prediction (if matched with a ground truth)\n",
    "            iou_score = 0\n",
    "            for idx, score in iou_scores:\n",
    "                if idx == i:\n",
    "                    iou_score = score\n",
    "                    break\n",
    "                    \n",
    "            print(f\"  Box {i+1}: Class {cls_id} (microplastic), Confidence: {conf:.4f}, IoU: {iou_score:.4f}\")\n",
    "\n",
    "# Function to visualize random test images with enhanced IoU metrics\n",
    "def visualize_random_samples(model, num_samples=3, data_path=\"data/test/images\", confidence=0.25):\n",
    "    \"\"\"Visualize predictions on random samples from the dataset with enhanced IoU metrics\"\"\"\n",
    "    image_files = list(Path(data_path).glob('*.jpg')) + list(Path(data_path).glob('*.png'))\n",
    "    if not image_files:\n",
    "        print(f\"No images found in {data_path}\")\n",
    "        return\n",
    "        \n",
    "    print(f\"Found {len(image_files)} images in {data_path}\")\n",
    "    print(f\"Visualizing {min(num_samples, len(image_files))} random samples...\\n\")\n",
    "    \n",
    "    # Select random samples\n",
    "    samples = random.sample(image_files, min(num_samples, len(image_files)))\n",
    "    \n",
    "    # Visualize each sample\n",
    "    for img_path in samples:\n",
    "        visualize_predictions(model, str(img_path), confidence=confidence)\n",
    "        print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "\n",
    "# Try on a few random test images with our enhanced IoU metrics\n",
    "if 'model' in locals() and model is not None:\n",
    "    try:\n",
    "        # Use a lower confidence threshold to see more potential detections\n",
    "        visualize_random_samples(model, num_samples=3, confidence=0.2)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during visualization: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a622254b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model performance explicitly on test dataset with detailed metrics\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "if 'model' in locals() and model is not None:\n",
    "    # Setup test paths\n",
    "    test_images_dir = 'data/test/images'\n",
    "    test_images = list(Path(test_images_dir).glob('*.jpg')) + list(Path(test_images_dir).glob('*.png'))\n",
    "    \n",
    "    if not test_images:\n",
    "        print(f\"No test images found in {test_images_dir}\")\n",
    "    else:\n",
    "        print(f\"Running evaluation on {len(test_images)} test images...\")\n",
    "        \n",
    "        # Make sure we're using the actual test data\n",
    "        print(f\"Test dataset path: {test_images_dir}\")\n",
    "        print(f\"Number of test images: {len(test_images)}\")\n",
    "        print(f\"First few test images: {[os.path.basename(str(img)) for img in test_images[:5]]}\")\n",
    "        \n",
    "        # Set model parameters for test evaluation\n",
    "        model.conf = 0.25  # Confidence threshold\n",
    "        model.iou = 0.7    # IoU threshold for NMS\n",
    "        model.agnostic = True  # NMS among classes\n",
    "        model.max_det = 200  # Maximum detections per image\n",
    "        \n",
    "        # Run validation on the test set with all metrics\n",
    "        print(\"\\nRunning model.val() on test dataset...\")\n",
    "        test_results = model.val(data=\"data.yaml\", split=\"test\")\n",
    "        \n",
    "        # Extract and display key metrics\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"EVALUATION METRICS ON TEST DATASET\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        # 1. Extract mAP metrics - the primary object detection metric\n",
    "        map50 = None\n",
    "        map50_95 = None\n",
    "        \n",
    "        # Look for mAP metrics in results\n",
    "        metrics_to_find = ['metrics/mAP50(B)', 'metrics/mAP50-95(B)']\n",
    "        for k, v in vars(test_results).items():\n",
    "            if not k.startswith('_') and not callable(v):\n",
    "                if k in metrics_to_find or k == 'map50' or k == 'map50_95' or k == 'maps':\n",
    "                    if isinstance(v, (int, float)):\n",
    "                        if k == 'metrics/mAP50(B)' or k == 'map50':\n",
    "                            map50 = v\n",
    "                        elif k == 'metrics/mAP50-95(B)' or k == 'map50_95':\n",
    "                            map50_95 = v\n",
    "        \n",
    "        # Display mAP metrics\n",
    "        print(\"\\n1. MEAN AVERAGE PRECISION (mAP):\")\n",
    "        if map50 is not None:\n",
    "            print(f\"   mAP@0.5: {map50:.4f}\")\n",
    "        if map50_95 is not None:\n",
    "            print(f\"   mAP@0.5:0.95: {map50_95:.4f}\")\n",
    "        if map50 is None and map50_95 is None:\n",
    "            print(\"   mAP values not found in results\")\n",
    "            \n",
    "        # 2. Extract Precision and Recall metrics\n",
    "        precision = None\n",
    "        recall = None\n",
    "        f1 = None\n",
    "        \n",
    "        # Look for precision/recall metrics in results\n",
    "        metrics_to_find = ['metrics/precision(B)', 'metrics/recall(B)']\n",
    "        for k, v in vars(test_results).items():\n",
    "            if not k.startswith('_') and not callable(v):\n",
    "                if k in metrics_to_find or k == 'precision' or k == 'recall':\n",
    "                    if isinstance(v, (int, float)):\n",
    "                        if k == 'metrics/precision(B)' or k == 'precision':\n",
    "                            precision = v\n",
    "                        elif k == 'metrics/recall(B)' or k == 'recall':\n",
    "                            recall = v\n",
    "                            \n",
    "        # Calculate F1 score if precision and recall are available\n",
    "        if precision is not None and recall is not None and precision > 0 and recall > 0:\n",
    "            f1 = 2 * (precision * recall) / (precision + recall)\n",
    "        \n",
    "        # Display precision and recall metrics\n",
    "        print(\"\\n2. PRECISION AND RECALL:\")\n",
    "        if precision is not None:\n",
    "            print(f\"   Precision: {precision:.4f}\")\n",
    "        if recall is not None:\n",
    "            print(f\"   Recall: {recall:.4f}\")\n",
    "        if f1 is not None:\n",
    "            print(f\"   F1 Score: {f1:.4f}\")\n",
    "        if precision is None and recall is None:\n",
    "            print(\"   Precision and recall values not found in results\")\n",
    "        \n",
    "        # 3. Extract IoU metrics\n",
    "        print(\"\\n3. INTERSECTION OVER UNION (IoU):\")\n",
    "        \n",
    "        # Run predictions to calculate IoU manually if necessary\n",
    "        print(\"\\nRunning predictions and calculating IoU metrics...\")\n",
    "        test_results = model(test_images, verbose=False)\n",
    "        \n",
    "        # Extract results for IoU calculation\n",
    "        all_ious = []\n",
    "        for result in test_results:\n",
    "            # Skip images with no detections\n",
    "            if not hasattr(result, 'boxes') or result.boxes is None or len(result.boxes) == 0:\n",
    "                continue\n",
    "                \n",
    "            # Get image path\n",
    "            img_path = result.path if hasattr(result, 'path') else None\n",
    "            if img_path is None:\n",
    "                continue\n",
    "                \n",
    "            # Get ground truth labels\n",
    "            label_path = Path(str(img_path).replace('/images/', '/labels/').replace('\\\\images\\\\', '\\\\labels\\\\').rsplit('.', 1)[0] + '.txt')\n",
    "            if not label_path.exists():\n",
    "                continue\n",
    "                \n",
    "            # Load ground truth boxes\n",
    "            gt_boxes = []\n",
    "            with open(label_path, 'r') as f:\n",
    "                for line in f:\n",
    "                    parts = line.strip().split()\n",
    "                    if len(parts) == 5:\n",
    "                        _, x, y, w, h = map(float, parts)\n",
    "                        gt_boxes.append([x, y, w, h])\n",
    "            \n",
    "            # Skip if no ground truth\n",
    "            if not gt_boxes:\n",
    "                continue\n",
    "            \n",
    "            # Extract detection boxes\n",
    "            img_h, img_w = result.orig_shape if hasattr(result, 'orig_shape') else (None, None)\n",
    "            if img_h is None or img_w is None:\n",
    "                continue\n",
    "                \n",
    "            pred_boxes = []\n",
    "            for i in range(len(result.boxes.xyxy)):\n",
    "                xyxy = result.boxes.xyxy[i].cpu().numpy()\n",
    "                x_center = (xyxy[0] + xyxy[2]) / 2 / img_w\n",
    "                y_center = (xyxy[1] + xyxy[3]) / 2 / img_h\n",
    "                width = (xyxy[2] - xyxy[0]) / img_w\n",
    "                height = (xyxy[3] - xyxy[1]) / img_h\n",
    "                pred_boxes.append([x_center, y_center, width, height])\n",
    "            \n",
    "            # Calculate IoU for each predicted box with best matching ground truth\n",
    "            for pred_box in pred_boxes:\n",
    "                best_iou = 0\n",
    "                for gt_box in gt_boxes:\n",
    "                    iou = calculate_iou(pred_box, gt_box)\n",
    "                    if iou > best_iou:\n",
    "                        best_iou = iou\n",
    "                all_ious.append(best_iou)\n",
    "        \n",
    "        # Display IoU statistics\n",
    "        if all_ious:\n",
    "            avg_iou = sum(all_ious) / len(all_ious)\n",
    "            min_iou = min(all_ious)\n",
    "            max_iou = max(all_ious)\n",
    "            above_60_pct = sum(1 for iou in all_ious if iou >= 0.60)\n",
    "            \n",
    "            print(f\"   Average IoU: {avg_iou:.4f}\")\n",
    "            print(f\"   Min IoU: {min_iou:.4f}\")\n",
    "            print(f\"   Max IoU: {max_iou:.4f}\")\n",
    "            print(f\"   Detections with IoU ≥ 0.60: {above_60_pct}/{len(all_ious)} ({above_60_pct/len(all_ious)*100:.2f}%)\")\n",
    "            \n",
    "            # Plot IoU distribution\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.hist(all_ious, bins=20, alpha=0.7, color='blue')\n",
    "            plt.axvline(x=0.60, color='r', linestyle='--', label='0.60 threshold')\n",
    "            plt.axvline(x=avg_iou, color='g', linestyle='-', label=f'Avg: {avg_iou:.4f}')\n",
    "            plt.title('Test Dataset IoU Distribution')\n",
    "            plt.xlabel('IoU Value')\n",
    "            plt.ylabel('Count')\n",
    "            plt.legend()\n",
    "            plt.grid(alpha=0.3)\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(\"   No valid IoU values calculated\")\n",
    "        \n",
    "        # 4. Extract Confidence metrics\n",
    "        confidence_scores = []\n",
    "        for result in test_results:\n",
    "            if hasattr(result, 'boxes') and result.boxes is not None and len(result.boxes) > 0:\n",
    "                confidence_scores.extend(result.boxes.conf.cpu().numpy().tolist())\n",
    "        \n",
    "        # Display confidence statistics\n",
    "        print(\"\\n4. CONFIDENCE SCORES:\")\n",
    "        if confidence_scores:\n",
    "            avg_conf = np.mean(confidence_scores)\n",
    "            min_conf = np.min(confidence_scores)\n",
    "            max_conf = np.max(confidence_scores)\n",
    "            \n",
    "            print(f\"   Number of detections: {len(confidence_scores)}\")\n",
    "            print(f\"   Average confidence: {avg_conf:.4f}\")\n",
    "            print(f\"   Min confidence: {min_conf:.4f}\")\n",
    "            print(f\"   Max confidence: {max_conf:.4f}\")\n",
    "            \n",
    "            # Count detections above threshold\n",
    "            above_60_pct = sum(1 for conf in confidence_scores if conf >= 0.60)\n",
    "            print(f\"   Detections with confidence ≥ 0.60: {above_60_pct}/{len(confidence_scores)} ({above_60_pct/len(confidence_scores)*100:.2f}%)\")\n",
    "            \n",
    "            # Plot confidence distribution\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.hist(confidence_scores, bins=20, alpha=0.7, color='blue')\n",
    "            plt.axvline(x=0.60, color='r', linestyle='--', label='0.60 threshold')\n",
    "            plt.axvline(x=avg_conf, color='g', linestyle='-', label=f'Avg: {avg_conf:.4f}')\n",
    "            plt.title('Test Dataset Confidence Score Distribution')\n",
    "            plt.xlabel('Confidence Score')\n",
    "            plt.ylabel('Count')\n",
    "            plt.legend()\n",
    "            plt.grid(alpha=0.3)\n",
    "            plt.show()\n",
    "            \n",
    "            # Print summary message about confidence goal\n",
    "            print(\"\\nCONFIDENCE GOAL ASSESSMENT:\")\n",
    "            if avg_conf >= 0.60:\n",
    "                print(f\"✅ GOAL ACHIEVED: Average confidence of {avg_conf:.4f} exceeds the target of 0.60\")\n",
    "            else:\n",
    "                print(f\"❌ GOAL NOT MET: Average confidence of {avg_conf:.4f} is below the target of 0.60\")\n",
    "                print(\"   Consider applying the confidence-boosting techniques in the next steps\")\n",
    "        else:\n",
    "            print(\"   No detections found in test dataset\")\n",
    "            \n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "else:\n",
    "    print(\"Model not loaded. Please run the training cell first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8259d18",
   "metadata": {},
   "source": [
    "## Confidence Score Optimization Techniques\n",
    "\n",
    "To increase detection confidence from an average of 0.58 to above 0.60, we've applied the following enhancements:\n",
    "\n",
    "1. **Training Improvements**:\n",
    "   - Increased class loss weight (0.8) to focus more on class confidence\n",
    "   - Higher momentum (0.95) for more stable gradient updates\n",
    "   - Improved learning rate schedule with higher lr0 and better decay\n",
    "   - Extended training epochs (200) to allow the model to reach higher confidence\n",
    "\n",
    "2. **Data Augmentation for Confidence**:\n",
    "   - Increased mosaic, mixup and copy-paste augmentations\n",
    "   - Enhanced scale variation to make the model more robust\n",
    "   - Added stronger rotational and shear augmentations\n",
    "   - Disabled rectangular training to prevent over-fitting to specific aspect ratios\n",
    "\n",
    "3. **Improved NMS Settings**:\n",
    "   - Increased IoU threshold to 0.7 to get more confident predictions\n",
    "   - Raised max_det to 200 to allow more high-confidence detections\n",
    "   - Set a higher initial confidence threshold (0.25) during training\n",
    "\n",
    "4. **Additional Improvements**:\n",
    "   - Created an image enhancement script (`confidence_boost.py`) that preprocesses images\n",
    "   - Applied sharpening, contrast enhancement, and color optimization\n",
    "   - Implemented test-time augmentation during inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d441d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the confidence boost script to enhance model confidence (using test dataset)\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# First make sure we have the required packages\n",
    "%pip install -q Pillow matplotlib numpy\n",
    "\n",
    "# Execute the confidence boosting script - this uses the test dataset\n",
    "!python confidence_boost.py\n",
    "\n",
    "# Now evaluate the model on the original test set for direct comparison\n",
    "if 'model' in locals() and model is not None:\n",
    "    print(\"\\nEvaluating model on original test images...\")\n",
    "    \n",
    "    # Set confidence threshold to 0.2 for evaluation\n",
    "    model.conf = 0.2\n",
    "    \n",
    "    # Run validation on test set\n",
    "    test_results = model.val(data=\"data.yaml\", split=\"test\")\n",
    "    \n",
    "    # Print confidence metrics\n",
    "    print(\"\\nConfidence Metrics on Original Test Set:\")\n",
    "    if hasattr(test_results, 'mean_confidence'):\n",
    "        print(f\"Mean Confidence: {test_results.mean_confidence:.4f}\")\n",
    "    elif hasattr(test_results, 'conf_matrix'):\n",
    "        conf_values = test_results.conf_matrix.flatten()\n",
    "        conf_values = conf_values[conf_values > 0]\n",
    "        if len(conf_values) > 0:\n",
    "            print(f\"Mean Confidence: {conf_values.mean():.4f}\")\n",
    "            print(f\"Min Confidence: {conf_values.min():.4f}\")\n",
    "            print(f\"Max Confidence: {conf_values.max():.4f}\")\n",
    "            print(f\"Percentage above 0.60: {(conf_values >= 0.60).sum() / len(conf_values) * 100:.2f}%\")\n",
    "    \n",
    "    # Now evaluate model on the enhanced test set\n",
    "    print(\"\\nEvaluating model on enhanced test images...\")\n",
    "    \n",
    "    # Run validation on enhanced test set\n",
    "    enhanced_results = model.val(data=\"data.yaml\", split=\"enhanced_test\")\n",
    "    \n",
    "    # Print confidence metrics\n",
    "    print(\"\\nConfidence Metrics on Enhanced Test Set:\")\n",
    "    if hasattr(enhanced_results, 'mean_confidence'):\n",
    "        print(f\"Mean Confidence: {enhanced_results.mean_confidence:.4f}\")\n",
    "    elif hasattr(enhanced_results, 'conf_matrix'):\n",
    "        conf_values = enhanced_results.conf_matrix.flatten()\n",
    "        conf_values = conf_values[conf_values > 0]\n",
    "        if len(conf_values) > 0:\n",
    "            print(f\"Mean Confidence: {conf_values.mean():.4f}\")\n",
    "            print(f\"Min Confidence: {conf_values.min():.4f}\")\n",
    "            print(f\"Max Confidence: {conf_values.max():.4f}\")\n",
    "            print(f\"Percentage above 0.60: {(conf_values >= 0.60).sum() / len(conf_values) * 100:.2f}%\")\n",
    "    \n",
    "    # Apply confidence-optimized inference with test-time augmentation (TTA)\n",
    "    print(\"\\nRunning confidence-optimized inference with test-time augmentation on original test images...\")\n",
    "    original_tta_results = model(\"data/test/images\", augment=True, conf=0.25, iou=0.7)\n",
    "    \n",
    "    print(\"\\nRunning confidence-optimized inference with test-time augmentation on enhanced test images...\")\n",
    "    enhanced_tta_results = model(\"data/enhanced_test/images\", augment=True, conf=0.25, iou=0.7)\n",
    "    \n",
    "    # Extract and analyze confidence scores for both test sets\n",
    "    original_conf_scores = []\n",
    "    for result in original_tta_results:\n",
    "        if hasattr(result, 'boxes') and result.boxes is not None and len(result.boxes) > 0:\n",
    "            original_conf_scores.extend(result.boxes.conf.cpu().numpy().tolist())\n",
    "    \n",
    "    enhanced_conf_scores = []\n",
    "    for result in enhanced_tta_results:\n",
    "        if hasattr(result, 'boxes') and result.boxes is not None and len(result.boxes) > 0:\n",
    "            enhanced_conf_scores.extend(result.boxes.conf.cpu().numpy().tolist())\n",
    "    \n",
    "    # Create a combined plot of both distributions\n",
    "    if original_conf_scores and enhanced_conf_scores:\n",
    "        import matplotlib.pyplot as plt\n",
    "        import numpy as np\n",
    "        \n",
    "        # Calculate confidence statistics\n",
    "        print(\"\\nConfidence Comparison:\")\n",
    "        print(f\"Original Test Set:\")\n",
    "        print(f\"  Mean Confidence: {sum(original_conf_scores)/len(original_conf_scores):.4f}\")\n",
    "        print(f\"  Min Confidence: {min(original_conf_scores):.4f}\")\n",
    "        print(f\"  Max Confidence: {max(original_conf_scores):.4f}\")\n",
    "        above_threshold_orig = sum(1 for conf in original_conf_scores if conf >= 0.60)\n",
    "        print(f\"  Detections with confidence ≥ 0.60: {above_threshold_orig}/{len(original_conf_scores)} ({above_threshold_orig/len(original_conf_scores)*100:.1f}%)\")\n",
    "        \n",
    "        print(f\"\\nEnhanced Test Set:\")\n",
    "        print(f\"  Mean Confidence: {sum(enhanced_conf_scores)/len(enhanced_conf_scores):.4f}\")\n",
    "        print(f\"  Min Confidence: {min(enhanced_conf_scores):.4f}\")\n",
    "        print(f\"  Max Confidence: {max(enhanced_conf_scores):.4f}\")\n",
    "        above_threshold_enh = sum(1 for conf in enhanced_conf_scores if conf >= 0.60)\n",
    "        print(f\"  Detections with confidence ≥ 0.60: {above_threshold_enh}/{len(enhanced_conf_scores)} ({above_threshold_enh/len(enhanced_conf_scores)*100:.1f}%)\")\n",
    "        \n",
    "        # Calculate improvement\n",
    "        orig_mean = sum(original_conf_scores)/len(original_conf_scores)\n",
    "        enh_mean = sum(enhanced_conf_scores)/len(enhanced_conf_scores)\n",
    "        improvement = enh_mean - orig_mean\n",
    "        percent_improvement = (improvement / orig_mean) * 100\n",
    "        print(f\"\\nConfidence Improvement: +{improvement:.4f} (+{percent_improvement:.2f}%)\")\n",
    "        \n",
    "        # Plot confidence distributions side by side\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.hist(original_conf_scores, bins=20, alpha=0.7, color='blue')\n",
    "        plt.axvline(x=0.60, color='r', linestyle='--', label='0.60 threshold')\n",
    "        plt.axvline(x=orig_mean, color='k', linestyle='-', label=f'Mean: {orig_mean:.4f}')\n",
    "        plt.title('Original Test Images Confidence')\n",
    "        plt.xlabel('Confidence Score')\n",
    "        plt.ylabel('Count')\n",
    "        plt.legend()\n",
    "        plt.grid(alpha=0.3)\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.hist(enhanced_conf_scores, bins=20, alpha=0.7, color='green')\n",
    "        plt.axvline(x=0.60, color='r', linestyle='--', label='0.60 threshold')\n",
    "        plt.axvline(x=enh_mean, color='k', linestyle='-', label=f'Mean: {enh_mean:.4f}')\n",
    "        plt.title('Enhanced Test Images Confidence')\n",
    "        plt.xlabel('Confidence Score')\n",
    "        plt.ylabel('Count')\n",
    "        plt.legend()\n",
    "        plt.grid(alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"Model not loaded. Please run the training cell first.\")\n",
    "\n",
    "# Recommendations for maintaining high confidence in production\n",
    "print(\"\\nRecommendations for maintaining high confidence in production:\")\n",
    "print(\"1. Use test-time augmentation (augment=True) during inference\")\n",
    "print(\"2. Apply image enhancement techniques before inference\")\n",
    "print(\"3. Set confidence threshold to 0.25-0.30 to filter low-confidence detections\")\n",
    "print(\"4. Use the optimized model parameters from the training cell\")\n",
    "print(\"5. Export and fine-tune the model with a focus on the confidence metric\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25638c29",
   "metadata": {},
   "source": [
    "## Evaluation Metrics for YOLOv8 Object Detection\n",
    "\n",
    "The following metrics are used to evaluate the performance of our microplastics detection model:\n",
    "\n",
    "### Metrics Explanation\n",
    "- **mAP (mean Average Precision)**: The primary metric for object detection performance. It calculates the average precision across all confidence thresholds and classes. The YOLOv8 model reports mAP at different IoU thresholds (e.g., mAP@0.5, mAP@0.5:0.95).\n",
    "\n",
    "- **Precision**: How accurate the positive detections are. It measures the proportion of true positive detections among all objects detected by the model. High precision means the model makes few false positive errors.\n",
    "\n",
    "- **Recall**: The ability of the model to find all microplastics in the image. It measures the proportion of true positive detections among all actual objects in the images. High recall means the model detects most of the actual microplastics in the images.\n",
    "\n",
    "- **IoU (Intersection over Union)**: Measures how well the predicted bounding boxes overlap with the ground truth annotations. It's calculated as the area of intersection between the predicted and ground truth boxes divided by the area of their union. In our analysis, we target an IoU threshold of 0.6 (60%).\n",
    "\n",
    "- **Confidence Score**: The model's estimated probability that an object belongs to the predicted class. Our goal is to increase the average confidence from 0.5888 to at least 0.60.\n",
    "\n",
    "These metrics provide complementary information about the model's performance. For example, it's possible to have high precision but low recall (the model is accurate but misses many objects), or high recall but low precision (the model finds most objects but also produces many false positives)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bf879d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Export the confidence-boosted model for deployment\n",
    "# if 'model' in locals() and model is not None:\n",
    "#     try:\n",
    "#         # Export to ONNX format (good for deployment)\n",
    "#         print(\"Exporting confidence-optimized model to ONNX format...\")\n",
    "#         success = model.export(format=\"onnx\", dynamic=True)\n",
    "        \n",
    "#         if success:\n",
    "#             print(f\"Model successfully exported to {model.export_dir}\")\n",
    "            \n",
    "#             # Additional formats for different deployment scenarios\n",
    "#             export_formats = [\"torchscript\", \"saved_model\"]\n",
    "            \n",
    "#             for fmt in export_formats:\n",
    "#                 try:\n",
    "#                     print(f\"Exporting to {fmt} format...\")\n",
    "#                     model.export(format=fmt)\n",
    "#                     print(f\"Successfully exported to {fmt} format\")\n",
    "#                 except Exception as e:\n",
    "#                     print(f\"Warning: Could not export to {fmt} format: {e}\")\n",
    "                    \n",
    "#             print(\"\\nThe exported models are optimized for higher confidence detection.\")\n",
    "#             print(\"When deploying, use a confidence threshold of at least 0.25 for optimal results.\")\n",
    "#         else:\n",
    "#             print(\"Export failed. Please check if the model was trained properly.\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error during model export: {e}\")\n",
    "# else:\n",
    "#     print(\"Model not loaded. Please run the training cell first.\")\n",
    "\n",
    "# # Final confidence benchmarking tips\n",
    "# print(\"\\nFinal tips for benchmarking detection confidence in production:\")\n",
    "# print(\"1. Monitor average confidence scores over time to detect potential drift\")\n",
    "# print(\"2. Periodically retrain the model with new data to maintain high confidence\")\n",
    "# print(\"3. Adjust confidence threshold based on the specific application needs\")\n",
    "# print(\"4. For critical applications, use ensemble techniques for even higher confidence\")\n",
    "# print(\"5. Apply the preprocessing techniques from confidence_boost.py to all inference inputs\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
